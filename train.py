
from typing import Any, Dict, List
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from requests import Session
from sqlalchemy import select
import torch
from tqdm import tqdm
from utils.util import fetch_training_data_from_db, load_pretrained_vectors_from_db
from database import ProductInferenceInput, SessionLocal, get_db
from utils.dependencies import get_global_encoder, get_global_projector
from model import CoarseToFineItemTower, OptimizedItemTower, SimCSEModelWrapper, SimCSERecSysDataset
import torch.nn as nn
import torch.nn.functional as F
from pytorch_metric_learning import losses
from torch.utils.data import DataLoader, Dataset
import os
from model import SymmetricUserTower 
import torch.optim as optim



DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_router = APIRouter()
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)



# ------------------------------------------------------
# Item Tower Training Task
# ------------------------------------------------------

class TrainingItem(BaseModel):

    product_id: int
    feature_data: Dict[str, Any]

def collate_simcse(batch):
    from APIController.serving_controller import preprocess_batch_input
    """(View1, View2) ë¦¬ìŠ¤íŠ¸ -> Tensor ë³€í™˜"""
    view1_list = [item[0] for item in batch]
    view2_list = [item[1] for item in batch]
    
    t_std1, t_re1 = preprocess_batch_input(view1_list)
    t_std2, t_re2 = preprocess_batch_input(view2_list)
    
    return t_std1, t_re1, t_std2, t_re2


## ë©”ëª¨ë¦¬ ìµœì í™”: db_session.query(Model).all() ëŒ€ì‹  select(...).mappings().all()ì„ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”


def train_simcse_from_db(    
    encoder: nn.Module,       
    projector: nn.Module,
    batch_size: int = 4,
    epochs: int = 5,
    lr: float = 1e-4
):
    print("ğŸš€ Fetching data from DB...")
    
    # í˜¹ì‹œ ëª¨ë¥¼ taskbackgroundë–„ë¬¸ì— ì¼ë‹¨.
    db_session = SessionLocal()
    
    
    stmt = select(ProductInferenceInput.product_id, ProductInferenceInput.feature_data)
    result = db_session.execute(stmt).mappings().all()
    
    if not result:
        print("âŒ No data found.")
        return

    # [ìˆ˜ì • 2] Dictionary -> Pydantic ë³€í™˜
    products_list = []
    for row in result:
        # row['feature_data'] ì ‘ê·¼
        f_data = row['feature_data']
        p_input = TrainingItem(
            product_id=row['product_id'],
            feature_data=f_data
        )
        products_list.append(p_input)
        
    print(f"âœ… Loaded {len(products_list)} items.")
    
    # 3. ëª¨ë¸ ì„¤ì •
    model = SimCSEModelWrapper(encoder, projector).to(DEVICE)
    model.train() 
    
    # OptimizerëŠ” ë‘ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ í•™ìŠµí•´ì•¼ í•¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    
    # Loss Function (Contrastive Learning)
    loss_func = losses.NTXentLoss(temperature=0.07)
    
    dataset = SimCSERecSysDataset(products_list, dropout_prob=0.2)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True, 
        collate_fn=collate_simcse,
        drop_last=True
    )
    
    print("ğŸ”¥ Starting Training Loop...")
    
    # 5. Training Loop
    for epoch in range(epochs):
        total_loss = 0
        step = 0
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for t_std1, t_re1, t_std2, t_re2 in progress:
            t_std1, t_re1 = t_std1.to(DEVICE), t_re1.to(DEVICE)
            t_std2, t_re2 = t_std2.to(DEVICE), t_re2.to(DEVICE)
            
            optimizer.zero_grad()
            
            
            # Forward (Cross-Attention)
            emb1 = model(t_std1, t_re1)
            emb2 = model(t_std2, t_re2)
            
            # Contrastive Loss Calculation
            embeddings = torch.cat([emb1, emb2], dim=0)
            
            # Label generation
            # ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ 0~N ë¼ë²¨ì„ ë§Œë“¤ê³  ë‘ ë²ˆ ë°˜ë³µ
            batch_curr = emb1.size(0)
            labels = torch.arange(batch_curr).to(DEVICE)
            labels = torch.cat([labels, labels], dim=0)
            
            loss = loss_func(embeddings, labels)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            step += 1
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
        print(f"Epoch {epoch+1} Avg Loss: {total_loss/step:.4f}")
        
    print("Training Finished.")
    

    
    print("ğŸ’¾ Saving models...")
    torch.save(encoder.state_dict(), os.path.join(MODEL_DIR, "encoder_stage1.pth"))
    torch.save(projector.state_dict(), os.path.join(MODEL_DIR, "projector_stage2.pth"))
    
    # torch.save(model.state_dict(), "final_simcse_model.pth")    



#DBì— ìˆëŠ” Item load -> positives(dropout) item ì¦ê°• -> collateê°€ì„œ í”¼ì³ í† í¬ë‚˜ì´ì € í•˜ê³  í…ì„œí™”
#ì´í›„ í…ì„œ ì•„ì´í…œíƒ€ì›Œê°€ì„œ trnsf-> std, re cross att í•˜ê³  ì§„í–‰
#ProductItem(DB) -> ItemTower(1ì°¨ì•„ì´í…œí…ì„œ) -> opt tensor í•™ìŠµ (1ì°¨í•™ìŠµ)  
@train_router.post("/run")
def test_line(

    encoder_instance: CoarseToFineItemTower = Depends(get_global_encoder), 
    projector_instance: OptimizedItemTower = Depends(get_global_projector)
):
    
    train_simcse_from_db(
        encoder=encoder_instance,
        projector=projector_instance
    )
    
    return {"message": "SimCSE training task initiated and completed."}




# ------------------------------------------------------
# User Tower Training Task
# ------------------------------------------------------


class UserTowerTrainDataset(Dataset):
    def __init__(self, 
                 user_data_list: List[dict], 
                 product_id_map: Dict[int, int], 
                 max_seq_len: int = 50):
        """
        user_data_list: [
            {'history': [101, 102], 'target': 103, 'gender': 1, 'age': 2}, ...
        ]
        """
        self.data = user_data_list
        self.product_id_map = product_id_map # DB ID -> Model Index ë³€í™˜ê¸°
        self.max_seq_len = max_seq_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data[idx]
        
        # 1. History ID Mapping & Padding
        raw_history = row['history']
        mapped_history = [self.product_id_map.get(pid, 0) for pid in raw_history] # ì—†ìœ¼ë©´ 0(PAD)
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸° (Truncate or Pad)
        seq_len = len(mapped_history)
        if seq_len > self.max_seq_len:
            mapped_history = mapped_history[-self.max_seq_len:] # ìµœê·¼ ê²ƒë§Œ ìœ ì§€
        else:
            mapped_history = mapped_history + [0] * (self.max_seq_len - seq_len) # ë’¤ì— 0 ì±„ì›€

        # 2. Target Item Mapping
        target_db_id = row['target']
        target_idx = self.product_id_map.get(target_db_id, 0)
        
        # 3. Profile Data
        gender = row.get('gender', 0)
        age = row.get('age', 0)

        return {
            "history": torch.tensor(mapped_history, dtype=torch.long),
            "target_idx": torch.tensor(target_idx, dtype=torch.long), # ì •ë‹µ ì•„ì´í…œì˜ Model Index
            "gender": torch.tensor(gender, dtype=torch.long),
            "age": torch.tensor(age, dtype=torch.long)
        }


def train_user_tower_task(
    db_session: Session = Depends(get_db), 
    epochs: int = 5, 
    batch_size: int = 4, 
    lr: float = 1e-4,
    temperature: float = 0.075 # Loss dx ë‚®ìŒ : low , Loss div : High
):
    print("\nğŸš€ [Task Started] User Tower Training...")
    
    # 1. Pre-trained Vector ë¡œë“œ (Lookup Table ì¤€ë¹„)
    pretrained_matrix, product_id_map = load_pretrained_vectors_from_db(db_session)
    num_total_products = len(product_id_map)
    
    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    model = SymmetricUserTower(
        num_total_products=num_total_products,
        max_seq_len=50,
        input_dim=128
    )
    
    # â­ í•µì‹¬: í•™ìŠµëœ ì•„ì´í…œ ë²¡í„° ì£¼ì… ë° ë™ê²°
    model.load_pretrained_weights(pretrained_matrix, freeze=True)
    model.to(DEVICE)
    model.train() # í•™ìŠµ ëª¨ë“œ
    
    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (Dummy Logic - ì‹¤ì œë¡œëŠ” DB User Log í…Œì´ë¸”ì—ì„œ ì¿¼ë¦¬í•´ì•¼ í•¨)
    # TODO: ì‹¤ì œ DBì—ì„œ ìœ ì € ë¡œê·¸(UserInteraction)ë¥¼ ê¸ì–´ì˜¤ëŠ” ë¡œì§ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”
    print("ğŸ“Š Fetching user interaction data...")
    
    train_data = fetch_training_data_from_db(db_session, min_interactions=2)
    print(f" ë°ì´í„° ê°œìˆ˜ í™•ì¸: {len(train_data)}ê°œ")
    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨ (Safety Check)
    if len(train_data) < batch_size:
        print("âš ï¸ Warning: Not enough data to train. At least one batch needed.")
       
    
    
    dataset = UserTowerTrainDataset(train_data, product_id_map)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True
        )
    
    # 4. Optimizer & Loss
    optimizer = optim.AdamW(model.parameters(), lr=lr)
    
    # CrossEntropyLossë¥¼ ì‚¬ìš© (In-batch Negative ë°©ì‹)
    # ì •ë‹µ ë¼ë²¨ì€ í•­ìƒ ëŒ€ê°ì„ (0, 1, 2...)ì´ ë¨
    criterion = nn.CrossEntropyLoss()

    # 5. Training Loop
    print(f"ğŸ”¥ Start Training for {epochs} epochs (Temp={temperature})...")
    
    for epoch in range(epochs):
        total_loss = 0
        steps = 0
        
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in progress:
            history = batch['history'].to(DEVICE)     # (B, L)
            target_idx = batch['target_idx'].to(DEVICE) # (B,)
            gender = batch['gender'].to(DEVICE)
            age = batch['age'].to(DEVICE)
            
            profile_data = {'gender': gender, 'age': age}
            
            optimizer.zero_grad()
            
            # -----------------------------------------------------------
            # (A) User Vector ìƒì„± (B, 128)
            # -> ì´ë¯¸ ëª¨ë¸ ë‚´ë¶€ì—ì„œ F.normalize ë˜ì–´ì„œ ë‚˜ì˜´ (ê¸¸ì´=1)
            # -----------------------------------------------------------
            user_vectors = model(history, profile_data)
            
            # -----------------------------------------------------------
            # (B) Target Item Vector ì¡°íšŒ (B, 128)
            # -> DBì—ì„œ ì˜¨ ë²¡í„°ì´ë¯€ë¡œ ì´ë¯¸ ì •ê·œí™” ë˜ì–´ ìˆìŒ (ê¸¸ì´=1)
            # -----------------------------------------------------------
            target_item_vectors = model.item_embedding(target_idx)
            
            # -----------------------------------------------------------
            # (C) Similarity (Logits) Calculation & Scaling [í•µì‹¬!]
            # -----------------------------------------------------------
            # ë‚´ì (Dot Product) ìˆ˜í–‰ -> ì •ê·œí™”ëœ ë²¡í„°ë¼ë¦¬ì˜ ë‚´ì ì´ë¯€ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì„ (-1.0 ~ 1.0)
            # (B, 128) x (128, B) = (B, B) Matrix
            sim_matrix = torch.matmul(user_vectors, target_item_vectors.T)
            
            # [Temperature Scaling]
            # ê°’ì˜ ë²”ìœ„ë¥¼ -1~1ì—ì„œ -10~10 (temp=0.1 ê¸°ì¤€)ìœ¼ë¡œ ë»¥íŠ€ê¸°í•´ì¤Œ.
            # ê·¸ë˜ì•¼ Softmaxê°€ ë¾°ì¡±í•´ì§€ê³ (Sharpening), Gradientê°€ ì˜ íë¦„.
            logits = sim_matrix / temperature 
            
            # -----------------------------------------------------------
            # (D) Labeling (In-batch Negative)
            # -----------------------------------------------------------
            # ië²ˆì§¸ ìœ ì €ëŠ” ië²ˆì§¸ ì•„ì´í…œ(ëŒ€ê°ì„ )ì´ ì •ë‹µ.
            # ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ Negative Sampleë¡œ ê°„ì£¼.
            labels = torch.arange(batch_size).to(DEVICE)
            
            # -----------------------------------------------------------
            # (E) Loss & Update
            # -----------------------------------------------------------
            loss = criterion(logits, labels)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            steps += 1
            
            # ì§„í–‰ë°”ì— í˜„ì¬ Loss í‘œì‹œ
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
        avg_loss = total_loss / steps if steps > 0 else 0
        print(f"   Epoch {epoch+1} Summary | Avg Loss: {avg_loss:.4f}")

    # 5. Save Model
    save_path = os.path.join(MODEL_DIR, "user_tower_symmetric_final.pth")
    torch.save(model.state_dict(), save_path)
    print(f"âœ… Training Complete. Model saved to {save_path}")
    
    
    
# ------------------------------------------------------
# Ranker (DCN V2)Task
# ------------------------------------------------------

from torch.utils.data import Dataset, DataLoader

class RankingDataset(Dataset):
    def __init__(self, 
                 interaction_logs,       # [user_id, item_id, label, context] ë¦¬ìŠ¤íŠ¸
                 user_vector_store,      # {user_id: vector(128)} (Redis/Memory)
                 item_vector_store):     # {item_id: vector(128)} (DB/Memory)
        self.logs = interaction_logs
        self.u_store = user_vector_store
        self.i_store = item_vector_store
        
        # OOV(Out of Vocabulary) ë°©ì§€ìš© ëœë¤ ë²¡í„° (í˜¹ì€ í‰ê·  ë²¡í„°)
        self.default_vec = torch.zeros(128) 

    def __len__(self):
        return len(self.logs)

    def __getitem__(self, idx):
        uid, iid, label, context = self.logs[idx]
        
        # 1. Feature Fetching 
        u_vec = self.u_store.get(uid, self.default_vec)
        i_vec = self.i_store.get(iid, self.default_vec)
        
        # Tensor ë³€í™˜
        u_tensor = torch.tensor(u_vec, dtype=torch.float32)
        i_tensor = torch.tensor(i_vec, dtype=torch.float32)
        c_tensor = torch.tensor(context, dtype=torch.float32) # ì˜ˆ: ì‹œê°„ ì •ë³´ ë“±
        
        # Label (0 or 1)
        label_tensor = torch.tensor(label, dtype=torch.float32)
        
        return u_tensor, i_tensor, c_tensor, label_tensor

def train_ranking_model(
    ranking_model, 
    train_loader, 
    epochs=5, 
    lr=0.001
):
    # 1. Setup
    ranking_model.to(DEVICE)
    optimizer = torch.optim.AdamW(ranking_model.parameters(), lr=lr)
    criterion = nn.BCELoss() # Binary Cross Entropy (ì¶œë ¥ì´ Sigmoidì—¬ì•¼ í•¨)
    
    print("ğŸ”¥ Start Ranking Model Training...")
    
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        total_samples = 0
        
        for u_emb, i_emb, c_emb, labels in tqdm(train_loader):
            u_emb = u_emb.to(DEVICE)
            i_emb = i_emb.to(DEVICE)
            c_emb = c_emb.to(DEVICE)
            labels = labels.to(DEVICE).unsqueeze(1) # (B,) -> (B, 1) ì°¨ì› ë§ì¶¤
            
            # 2. Forward
            optimizer.zero_grad()
            
            # Ranking Modelì€ (0~1) ì‚¬ì´ì˜ í™•ë¥ ê°’ì„ ë±‰ìŒ
            preds = ranking_model(u_emb, i_emb, c_emb) 
            
            # 3. Loss Calculation
            loss = criterion(preds, labels)
            
            # 4. Backward
            loss.backward()
            optimizer.step()
            
            # 5. Accuracy Check (0.5 ê¸°ì¤€)
            total_loss += loss.item()
            predicted_labels = (preds > 0.5).float()
            correct += (predicted_labels == labels).sum().item()
            total_samples += labels.size(0)
            
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total_samples
        print(f"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Acc: {accuracy*100:.2f}%")
        
    print("âœ… Ranking Training Finished.")