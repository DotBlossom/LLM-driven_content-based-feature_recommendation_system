
from typing import Any, Dict, List
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from requests import Session
from sqlalchemy import select
import torch
from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup
from database import ProductInferenceInput, SessionLocal, get_db
from utils.dependencies import get_global_batch_size, get_global_encoder, get_global_projector
from model import CoarseToFineItemTower, FinalUserTower, OptimizedItemTower, SimCSEModelWrapper, SimCSERecSysDataset
import torch.nn as nn
import torch.nn.functional as F
from pytorch_metric_learning import losses
from torch.utils.data import DataLoader, Dataset
import os
#from model import SymmetricUserTower 
import torch.optim as optim



DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_router = APIRouter()
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)



# ------------------------------------------------------
# Item Tower Training Task
# ------------------------------------------------------

class TrainingItem(BaseModel):

    product_id: int
    feature_data: Dict[str, Any]

def collate_simcse(batch):
    from APIController.serving_controller import preprocess_batch_input
    """(View1, View2) ë¦¬ìŠ¤íŠ¸ -> Tensor ë³€í™˜"""
    view1_list = [item[0] for item in batch]
    view2_list = [item[1] for item in batch]
    
    t_std1, t_re1 = preprocess_batch_input(view1_list)
    t_std2, t_re2 = preprocess_batch_input(view2_list)
    
    return t_std1, t_re1, t_std2, t_re2


## ë©”ëª¨ë¦¬ ìµœì í™”: db_session.query(Model).all() ëŒ€ì‹  select(...).mappings().all()ì„ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”


def train_simcse_from_db(    
    encoder: nn.Module,       
    projector: nn.Module,
    batch_size: int = Depends(get_global_batch_size),
    epochs: int = 20,
    lr: float = 1e-4
):
    print("ğŸš€ Fetching data from DB...")
    
    # í˜¹ì‹œ ëª¨ë¥¼ taskbackgroundë–„ë¬¸ì— ì¼ë‹¨.
    db_session = SessionLocal()
    
    
    stmt = select(ProductInferenceInput.product_id, ProductInferenceInput.feature_data)
    result = db_session.execute(stmt).mappings().all()
    
    if not result:
        print("âŒ No data found.")
        return

    # [ìˆ˜ì • 2] Dictionary -> Pydantic ë³€í™˜
    products_list = []
    for row in result:
        # row['feature_data'] ì ‘ê·¼
        f_data = row['feature_data']
        p_input = TrainingItem(
            product_id=row['product_id'],
            feature_data=f_data
        )
        products_list.append(p_input)
        
    print(f"âœ… Loaded {len(products_list)} items.")
    
    # 3. ëª¨ë¸ ì„¤ì •
    model = SimCSEModelWrapper(encoder, projector).to(DEVICE)
    model.train() 
    
    # OptimizerëŠ” ë‘ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ í•™ìŠµí•´ì•¼ í•¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    

    
    # Loss Function (Contrastive Learning)
    loss_func = losses.NTXentLoss(temperature=0.07)
    
    dataset = SimCSERecSysDataset(products_list, dropout_prob=0.2)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True, 
        collate_fn=collate_simcse,
        drop_last=True
    )

    # [ì¶”ê°€] ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (Warmup 10%)
    total_steps = len(dataloader) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * 0.1),
        num_training_steps=total_steps
    )

    
    print("ğŸ”¥ Starting Training Loop...")
    
    # 5. Training Loop
    for epoch in range(epochs):
        total_loss = 0
        step = 0
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for t_std1, t_re1, t_std2, t_re2 in progress:

            t_std1, t_re1 = t_std1.to(DEVICE), t_re1.to(DEVICE)
            t_std2, t_re2 = t_std2.to(DEVICE), t_re2.to(DEVICE)
            
            optimizer.zero_grad()
            
            
            # Forward 
            emb1 = model(t_std1, t_re1)
            emb2 = model(t_std2, t_re2)
            
            # Contrastive Loss Calculation
            embeddings = torch.cat([emb1, emb2], dim=0)
            
            # Label generation
            # ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ 0~N ë¼ë²¨ì„ ë§Œë“¤ê³  ë‘ ë²ˆ ë°˜ë³µ
            batch_curr = emb1.size(0)
            labels = torch.arange(batch_curr).to(DEVICE)
            labels = torch.cat([labels, labels], dim=0)
            
            loss = loss_func(embeddings, labels)
            
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            step += 1
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
        if epochs % 10 == 0:
            print(f"Epoch {epoch+1} Avg Loss: {total_loss/step:.4f}")
        
    print("Training Finished.")
    

    
    print("ğŸ’¾ Saving models...")
    torch.save(encoder.state_dict(), os.path.join(MODEL_DIR, "encoder_stage1.pth"))
    torch.save(projector.state_dict(), os.path.join(MODEL_DIR, "projector_stage2.pth"))
    
    # torch.save(model.state_dict(), "final_simcse_model.pth")    



#DBì— ìˆëŠ” Item load -> positives(dropout) item ì¦ê°• -> collateê°€ì„œ í”¼ì³ í† í¬ë‚˜ì´ì € í•˜ê³  í…ì„œí™”
#ì´í›„ í…ì„œ ì•„ì´í…œíƒ€ì›Œê°€ì„œ trnsf-> std, re cross att í•˜ê³  ì§„í–‰
#ProductItem(DB) -> ItemTower(1ì°¨ì•„ì´í…œí…ì„œ) -> opt tensor í•™ìŠµ (1ì°¨í•™ìŠµ)  
@train_router.post("/run")
def test_line(

    encoder_instance: CoarseToFineItemTower = Depends(get_global_encoder), 
    projector_instance: OptimizedItemTower = Depends(get_global_projector)
):
    
    train_simcse_from_db(
        encoder=encoder_instance,
        projector=projector_instance
    )
    
    return {"message": "SimCSE training task initiated and completed."}




# ------------------------------------------------------
# User Tower Training Task
# ------------------------------------------------------


class UserTowerTrainDataset(Dataset):
    def __init__(self, 
                 user_data_list: List[dict], 
                 product_id_map: Dict[int, int],
                 max_seq_len: int = 50):
        """
        user_data_list: [
            {'history': [101, 102], 'target': 103, 'gender': 1, 'age': 2}, ...
        ]
        """
        self.data = user_data_list
        self.max_seq_len = max_seq_len
        self.product_id_map = product_id_map

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data[idx]
        
        # 1. History ID Mapping & Padding
        raw_history = row['history']
        mapped_history = [self.product_id_map.get(pid, 0) for pid in raw_history] # ì—†ìœ¼ë©´ 0(PAD)
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸° (Truncate or Pad)
        seq_len = len(mapped_history)
        if seq_len > self.max_seq_len:
            mapped_history = mapped_history[-self.max_seq_len:] # ìµœê·¼ ê²ƒë§Œ ìœ ì§€
        else:
            mapped_history = mapped_history + [0] * (self.max_seq_len - seq_len) # ë’¤ì— 0 ì±„ì›€

        # 2. Target Item Mapping
        target_db_id = row['target_idx'] 
        target_idx = self.product_id_map.get(target_db_id, 0)
        
        # 3. Profile Data
        gender = row.get('gender', 0)
        age = row.get('age', 0)
        season = row.get('season', 0)
        

        return {
            "history": torch.tensor(mapped_history, dtype=torch.long),
            "target_idx": torch.tensor(target_idx, dtype=torch.long), # ì •ë‹µ ì•„ì´í…œì˜ Model Index
            "gender": torch.tensor(gender, dtype=torch.long),
            "age": torch.tensor(age, dtype=torch.long),
            "season": torch.tensor(season, dtype=torch.long)
        }


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm

# ==========================================
# 1. Dataset ì •ì˜
# ==========================================
class UserSessionDataset(Dataset):
    def __init__(self, 
                 user_sessions: list,   # [{'history':[], 'season':0, 'gender':0, 'target_item_id':10}, ...]
                 max_len: int = 50):
        self.data = user_sessions
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data[idx]
        
        # History Padding (Pre-padding or Post-padding)
        # ë³´í†µ TransformerëŠ” Post-padding + Maskingì„ ì“°ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” 0ì´ Pad IDë¼ê³  ê°€ì •
        history = row['history']
        if len(history) > self.max_len:
            history = history[-self.max_len:] # ìµœê·¼ê±°ë§Œ
        else:
            history = history + [0] * (self.max_len - len(history))
            
        return {
            'history': torch.tensor(history, dtype=torch.long),
            'season': torch.tensor(row['season'], dtype=torch.long),
            'gender': torch.tensor(row['gender'], dtype=torch.long),
            'target_item_id': torch.tensor(row['target_item_id'], dtype=torch.long),
            # ë§Œì•½ Item Towerê°€ Featureë¥¼ ì…ë ¥ë°›ì•„ì•¼ í•œë‹¤ë©´ ì—¬ê¸°ì— item_featuresë„ í¬í•¨ë˜ì–´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ IDë¡œ Item Towerì—ì„œ ë²¡í„°ë¥¼ ë£©ì—…í•œë‹¤ê³  ê°€ì •
        }

# ==========================================
# 2. In-batch Negative Loss (Contrastive)
# ==========================================
class InfoNCELoss(nn.Module):
    """
    ë°°ì¹˜ ë‚´ì˜ ë‹¤ë¥¸ ìƒ˜í”Œë“¤ì„ Negativeë¡œ í™œìš©í•˜ëŠ” íš¨ìœ¨ì ì¸ Loss
    """
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, user_vectors, item_vectors):
        """
        user_vectors: (Batch, Dim)
        item_vectors: (Batch, Dim) - Positive Pairs
        """
        # Similarity Matrix: (Batch, Batch)
        # (B, D) @ (D, B) -> (B, B)
        scores = torch.matmul(user_vectors, item_vectors.T)
        
        # Scaling
        scores = scores / self.temperature
        
        # Labels: ëŒ€ê°ì„ (Diagonal)ì´ ì •ë‹µ (0ë²ˆì§¸ ìœ ì €ëŠ” 0ë²ˆì§¸ ì•„ì´í…œì´ ì •ë‹µ)
        batch_size = user_vectors.size(0)
        labels = torch.arange(batch_size).to(user_vectors.device)
        
        loss = self.criterion(scores, labels)
  
        return loss
    
import torch.optim as optim
from tqdm import tqdm
from torch.utils.data import DataLoader





DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def train_final_user_tower(
    user_tower: FinalUserTower,
    pretrained_item_matrix: torch.Tensor, # Loss ê³„ì‚°ìš© (Target/Teacher)
    train_loader: DataLoader,
    epochs: int = 10,
    lr: float = 1e-4,
):
    # 1. Setup
    user_tower.to(DEVICE)
    pretrained_item_matrix = pretrained_item_matrix.to(DEVICE)
    
    optimizer = optim.AdamW(user_tower.parameters(), lr=lr, weight_decay=1e-4)
    loss_fn = InfoNCELoss(temperature=0.07).to(DEVICE)
    
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=lr, 
        steps_per_epoch=len(train_loader), epochs=epochs
    )

    print(f"ğŸš€ Start Training FinalUserTower on {DEVICE}...")
    user_tower.train()
    
    for epoch in range(epochs):
        total_loss = 0
        step = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in progress_bar:
            # 2. Input Data Preparation
            history = batch['history'].to(DEVICE)       # (Batch, Seq)
            season = batch['season'].to(DEVICE)         # (Batch, )
            gender = batch['gender'].to(DEVICE)         # (Batch, )
            
            target_idx = batch['target_idx'].to(DEVICE) # (Batch, ) - ì •ë‹µ ì•„ì´í…œ Index
            
            # -----------------------------------------------------------
            # A. Ground Truth (Target Item Vectors)
            # -----------------------------------------------------------
            # ë¯¸ë¦¬ ê³„ì‚°ëœ ì•„ì´í…œ í–‰ë ¬ì—ì„œ ì •ë‹µ ë²¡í„°ë¥¼ ì§ì ‘ ê°€ì ¸ì˜´ (Teacher)
            # pretrained_item_matrix: (Total_Items, 128)
            with torch.no_grad():
                target_item_vectors = pretrained_item_matrix[target_idx]
                # íƒ€ê²Ÿ ë²¡í„°ë„ ì •ê·œí™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (Modelì´ Normalizeë¥¼ ì“´ë‹¤ë©´ ì—¬ê¸°ë„ í•´ì•¼ í•¨)
                target_item_vectors = F.normalize(target_item_vectors, p=2, dim=1)

            # -----------------------------------------------------------
            # B. User Representation (Student)
            # -----------------------------------------------------------
            # [Call] FinalUserTower.forward(history_ids, season_idx, gender_idx)
            user_vectors = user_tower(history, season, gender)
            
            # -----------------------------------------------------------
            # C. Contrastive Loss
            # -----------------------------------------------------------
            loss = loss_fn(user_vectors, target_item_vectors)
            
            # D. Optimization
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(user_tower.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            step += 1
            progress_bar.set_postfix({'loss': f"{loss.item():.4f}"})
            
        print(f"ğŸ“Š Epoch {epoch+1} Avg Loss: {total_loss / step:.4f}")
        
    print("âœ… Training Finished.")
    return user_tower

