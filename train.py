
from typing import Any, Dict
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from sqlalchemy import select
import torch
from tqdm import tqdm
from database import ProductInferenceInput, ProductInput, SessionLocal
from utils.dependencies import get_global_batch_size, get_global_encoder, get_global_projector
from model import CoarseToFineItemTower, OptimizedItemTower, SimCSEModelWrapper, SimCSERecSysDataset
import torch.nn as nn
import torch.nn.functional as F
from pytorch_metric_learning import losses
from torch.utils.data import DataLoader
import os


DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_router = APIRouter()
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

class TrainingItem(BaseModel):

    product_id: int
    feature_data: Dict[str, Any]

def collate_simcse(batch):
    from APIController.serving_controller import preprocess_batch_input
    """(View1, View2) ë¦¬ìŠ¤íŠ¸ -> Tensor ë³€í™˜"""
    view1_list = [item[0] for item in batch]
    view2_list = [item[1] for item in batch]
    
    t_std1, t_re1 = preprocess_batch_input(view1_list)
    t_std2, t_re2 = preprocess_batch_input(view2_list)
    
    return t_std1, t_re1, t_std2, t_re2


## ë©”ëª¨ë¦¬ ìµœì í™”: db_session.query(Model).all() ëŒ€ì‹  select(...).mappings().all()ì„ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”


def train_simcse_from_db(    
    encoder: nn.Module,       
    projector: nn.Module,
    batch_size: int = Depends(get_global_batch_size),
    epochs: int = 5,
    lr: float = 1e-4
):
    print("ðŸš€ Fetching data from DB...")
    
    # í˜¹ì‹œ ëª¨ë¥¼ taskbackgroundë–„ë¬¸ì— ì¼ë‹¨.
    db_session = SessionLocal()
    
    
    stmt = select(ProductInferenceInput.product_id, ProductInferenceInput.feature_data)
    result = db_session.execute(stmt).mappings().all()
    
    if not result:
        print("âŒ No data found.")
        return

    # [ìˆ˜ì • 2] Dictionary -> Pydantic ë³€í™˜
    products_list = []
    for row in result:
        # row['feature_data'] ì ‘ê·¼
        f_data = row['feature_data']
        p_input = TrainingItem(
            product_id=row['product_id'],
            feature_data=f_data
        )
        products_list.append(p_input)
        
    print(f"âœ… Loaded {len(products_list)} items.")
    
    # 3. ëª¨ë¸ ì„¤ì •
    model = SimCSEModelWrapper(encoder, projector).to(DEVICE)
    model.train() 
    
    # OptimizerëŠ” ë‘ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ í•™ìŠµí•´ì•¼ í•¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    
    # Loss Function (Contrastive Learning)
    loss_func = losses.NTXentLoss(temperature=0.07)
    
    dataset = SimCSERecSysDataset(products_list, dropout_prob=0.2)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True, 
        collate_fn=collate_simcse,
        drop_last=True
    )
    
    print("ðŸ”¥ Starting Training Loop...")
    
    # 5. Training Loop
    for epoch in range(epochs):
        total_loss = 0
        step = 0
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for t_std1, t_re1, t_std2, t_re2 in progress:
            t_std1, t_re1 = t_std1.to(DEVICE), t_re1.to(DEVICE)
            t_std2, t_re2 = t_std2.to(DEVICE), t_re2.to(DEVICE)
            
            optimizer.zero_grad()
            
            
            # Forward (Cross-Attention)
            emb1 = model(t_std1, t_re1)
            emb2 = model(t_std2, t_re2)
            
            # Contrastive Loss Calculation
            embeddings = torch.cat([emb1, emb2], dim=0)
            
            # Label generation
            # ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ 0~N ë¼ë²¨ì„ ë§Œë“¤ê³  ë‘ ë²ˆ ë°˜ë³µ
            batch_curr = emb1.size(0)
            labels = torch.arange(batch_curr).to(DEVICE)
            labels = torch.cat([labels, labels], dim=0)
            
            loss = loss_func(embeddings, labels)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            step += 1
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
        print(f"Epoch {epoch+1} Avg Loss: {total_loss/step:.4f}")
        
    print("Training Finished.")
    

    
    print("ðŸ’¾ Saving models...")
    torch.save(encoder.state_dict(), os.path.join(MODEL_DIR, "encoder_stage1.pth"))
    torch.save(projector.state_dict(), os.path.join(MODEL_DIR, "projector_stage2.pth"))
    
    # torch.save(model.state_dict(), "final_simcse_model.pth")    



#DBì— ìžˆëŠ” Item load -> positives(dropout) item ì¦ê°• -> collateê°€ì„œ í”¼ì³ í† í¬ë‚˜ì´ì € í•˜ê³  í…ì„œí™”
#ì´í›„ í…ì„œ ì•„ì´í…œíƒ€ì›Œê°€ì„œ trnsf-> std, re cross att í•˜ê³  ì§„í–‰
#ProductItem(DB) -> ItemTower(1ì°¨ì•„ì´í…œí…ì„œ) -> opt tensor í•™ìŠµ (1ì°¨í•™ìŠµ)  
@train_router.post("/run")
def test_line(

    encoder_instance: CoarseToFineItemTower = Depends(get_global_encoder), 
    projector_instance: OptimizedItemTower = Depends(get_global_projector)
):
    
    train_simcse_from_db(
        encoder=encoder_instance,
        projector=projector_instance
    )
    
    return {"message": "SimCSE training task initiated and completed."}


# real inference(new)
# ProductItem(DB) ë°›ì•„ì„œ ì•„ì´í…œíƒ€ì›Œë¡œì§ ê±°ì¹¨ -> ìµœì í™” í–‰ë ¬ë¡œ ê°’ ìž¬ì •ë ¬ -> ì‹¤ì œ itemtensor í™•ë³´