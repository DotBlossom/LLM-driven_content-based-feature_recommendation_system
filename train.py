
from typing import Any, Dict, List
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from requests import Session
from sqlalchemy import select
import torch
from tqdm import tqdm
from APIController.serving_controller import fetch_training_data_from_db, load_pretrained_vectors_from_db
from database import ProductInferenceInput, ProductInput, SessionLocal, get_db
from utils.dependencies import get_global_batch_size, get_global_encoder, get_global_projector
from model import CoarseToFineItemTower, OptimizedItemTower, SimCSEModelWrapper, SimCSERecSysDataset
import torch.nn as nn
import torch.nn.functional as F
from pytorch_metric_learning import losses
from torch.utils.data import DataLoader
import os


DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_router = APIRouter()
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

class TrainingItem(BaseModel):

    product_id: int
    feature_data: Dict[str, Any]

def collate_simcse(batch):
    from APIController.serving_controller import preprocess_batch_input
    """(View1, View2) ë¦¬ìŠ¤íŠ¸ -> Tensor ë³€í™˜"""
    view1_list = [item[0] for item in batch]
    view2_list = [item[1] for item in batch]
    
    t_std1, t_re1 = preprocess_batch_input(view1_list)
    t_std2, t_re2 = preprocess_batch_input(view2_list)
    
    return t_std1, t_re1, t_std2, t_re2


## ë©”ëª¨ë¦¬ ìµœì í™”: db_session.query(Model).all() ëŒ€ì‹  select(...).mappings().all()ì„ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”


def train_simcse_from_db(    
    encoder: nn.Module,       
    projector: nn.Module,
    batch_size: int,
    epochs: int = 5,
    lr: float = 1e-4
):
    print("ğŸš€ Fetching data from DB...")
    
    # í˜¹ì‹œ ëª¨ë¥¼ taskbackgroundë–„ë¬¸ì— ì¼ë‹¨.
    db_session = SessionLocal()
    
    
    stmt = select(ProductInferenceInput.product_id, ProductInferenceInput.feature_data)
    result = db_session.execute(stmt).mappings().all()
    
    if not result:
        print("âŒ No data found.")
        return

    # [ìˆ˜ì • 2] Dictionary -> Pydantic ë³€í™˜
    products_list = []
    for row in result:
        # row['feature_data'] ì ‘ê·¼
        f_data = row['feature_data']
        p_input = TrainingItem(
            product_id=row['product_id'],
            feature_data=f_data
        )
        products_list.append(p_input)
        
    print(f"âœ… Loaded {len(products_list)} items.")
    
    # 3. ëª¨ë¸ ì„¤ì •
    model = SimCSEModelWrapper(encoder, projector).to(DEVICE)
    model.train() 
    
    # OptimizerëŠ” ë‘ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ í•™ìŠµí•´ì•¼ í•¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    
    # Loss Function (Contrastive Learning)
    loss_func = losses.NTXentLoss(temperature=0.07)
    
    dataset = SimCSERecSysDataset(products_list, dropout_prob=0.2)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True, 
        collate_fn=collate_simcse,
        drop_last=True
    )
    
    print("ğŸ”¥ Starting Training Loop...")
    
    # 5. Training Loop
    for epoch in range(epochs):
        total_loss = 0
        step = 0
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for t_std1, t_re1, t_std2, t_re2 in progress:
            t_std1, t_re1 = t_std1.to(DEVICE), t_re1.to(DEVICE)
            t_std2, t_re2 = t_std2.to(DEVICE), t_re2.to(DEVICE)
            
            optimizer.zero_grad()
            
            
            # Forward (Cross-Attention)
            emb1 = model(t_std1, t_re1)
            emb2 = model(t_std2, t_re2)
            
            # Contrastive Loss Calculation
            embeddings = torch.cat([emb1, emb2], dim=0)
            
            # Label generation
            # ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ 0~N ë¼ë²¨ì„ ë§Œë“¤ê³  ë‘ ë²ˆ ë°˜ë³µ
            batch_curr = emb1.size(0)
            labels = torch.arange(batch_curr).to(DEVICE)
            labels = torch.cat([labels, labels], dim=0)
            
            loss = loss_func(embeddings, labels)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            step += 1
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
        print(f"Epoch {epoch+1} Avg Loss: {total_loss/step:.4f}")
        
    print("Training Finished.")
    

    
    print("ğŸ’¾ Saving models...")
    torch.save(encoder.state_dict(), os.path.join(MODEL_DIR, "encoder_stage1.pth"))
    torch.save(projector.state_dict(), os.path.join(MODEL_DIR, "projector_stage2.pth"))
    
    # torch.save(model.state_dict(), "final_simcse_model.pth")    



#DBì— ìˆëŠ” Item load -> positives(dropout) item ì¦ê°• -> collateê°€ì„œ í”¼ì³ í† í¬ë‚˜ì´ì € í•˜ê³  í…ì„œí™”
#ì´í›„ í…ì„œ ì•„ì´í…œíƒ€ì›Œê°€ì„œ trnsf-> std, re cross att í•˜ê³  ì§„í–‰
#ProductItem(DB) -> ItemTower(1ì°¨ì•„ì´í…œí…ì„œ) -> opt tensor í•™ìŠµ (1ì°¨í•™ìŠµ)  
@train_router.post("/run")
def test_line(

    encoder_instance: CoarseToFineItemTower = Depends(get_global_encoder), 
    projector_instance: OptimizedItemTower = Depends(get_global_projector)
):
    
    train_simcse_from_db(
        encoder=encoder_instance,
        projector=projector_instance
    )
    
    return {"message": "SimCSE training task initiated and completed."}


# real inference(new)
# ProductItem(DB) ë°›ì•„ì„œ ì•„ì´í…œíƒ€ì›Œë¡œì§ ê±°ì¹¨ -> ìµœì í™” í–‰ë ¬ë¡œ ê°’ ì¬ì •ë ¬ -> ì‹¤ì œ itemtensor í™•ë³´








import torch.optim as optim
import torch.nn as nn
from models import SymmetricUserTower # ì´ì „ì— ì‘ì„±í•œ ëª¨ë¸ í´ë˜ìŠ¤ import

from torch.utils.data import Dataset, DataLoader


class UserTowerTrainDataset(Dataset):
    def __init__(self, 
                 user_data_list: List[dict], 
                 product_id_map: Dict[int, int], 
                 max_seq_len: int = 50):
        """
        user_data_list: [
            {'history': [101, 102], 'target': 103, 'gender': 1, 'age': 2}, ...
        ]
        """
        self.data = user_data_list
        self.product_id_map = product_id_map # DB ID -> Model Index ë³€í™˜ê¸°
        self.max_seq_len = max_seq_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data[idx]
        
        # 1. History ID Mapping & Padding
        raw_history = row['history']
        mapped_history = [self.product_id_map.get(pid, 0) for pid in raw_history] # ì—†ìœ¼ë©´ 0(PAD)
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸° (Truncate or Pad)
        seq_len = len(mapped_history)
        if seq_len > self.max_seq_len:
            mapped_history = mapped_history[-self.max_seq_len:] # ìµœê·¼ ê²ƒë§Œ ìœ ì§€
        else:
            mapped_history = mapped_history + [0] * (self.max_seq_len - seq_len) # ë’¤ì— 0 ì±„ì›€

        # 2. Target Item Mapping
        target_db_id = row['target']
        target_idx = self.product_id_map.get(target_db_id, 0)
        
        # 3. Profile Data
        gender = row.get('gender', 0)
        age = row.get('age', 0)

        return {
            "history": torch.tensor(mapped_history, dtype=torch.long),
            "target_idx": torch.tensor(target_idx, dtype=torch.long), # ì •ë‹µ ì•„ì´í…œì˜ Model Index
            "gender": torch.tensor(gender, dtype=torch.long),
            "age": torch.tensor(age, dtype=torch.long)
        }


def train_user_tower_task(
    db_session: Session = Depends(get_db), 
    epochs: int = 5, 
    batch_size: int = 512, 
    lr: float = 1e-4
):
    print("\nğŸš€ [Task Started] User Tower Training...")
    
    # 1. Pre-trained Vector ë¡œë“œ (Lookup Table ì¤€ë¹„)
    pretrained_matrix, product_id_map = load_pretrained_vectors_from_db(db_session)
    num_total_products = len(product_id_map)
    
    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    model = SymmetricUserTower(
        num_total_products=num_total_products,
        max_seq_len=50,
        input_dim=128
    )
    
    # â­ í•µì‹¬: í•™ìŠµëœ ì•„ì´í…œ ë²¡í„° ì£¼ì… ë° ë™ê²°
    model.load_pretrained_weights(pretrained_matrix, freeze=True)
    model.to(DEVICE)
    model.train() # í•™ìŠµ ëª¨ë“œ
    
    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (Dummy Logic - ì‹¤ì œë¡œëŠ” DB User Log í…Œì´ë¸”ì—ì„œ ì¿¼ë¦¬í•´ì•¼ í•¨)
    # TODO: ì‹¤ì œ DBì—ì„œ ìœ ì € ë¡œê·¸(UserInteraction)ë¥¼ ê¸ì–´ì˜¤ëŠ” ë¡œì§ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”
    print("ğŸ“Š Fetching user interaction data...")
    
    train_data = fetch_training_data_from_db(db_session, min_interactions=2)
    
    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨ (Safety Check)
    if len(train_data) < batch_size:
        print("âš ï¸ Warning: Not enough data to train. At least one batch needed.")
       
    
    
    dataset = UserTowerTrainDataset(train_data, product_id_map)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # 4. Optimizer & Loss
    optimizer = optim.AdamW(model.parameters(), lr=lr)
    
    # CrossEntropyLossë¥¼ ì‚¬ìš© (In-batch Negative ë°©ì‹)
    # ì •ë‹µ ë¼ë²¨ì€ í•­ìƒ ëŒ€ê°ì„ (0, 1, 2...)ì´ ë¨
    criterion = nn.CrossEntropyLoss()

    # 5. Training Loop
    print(f"ğŸ”¥ Start Training for {epochs} epochs...")
    
    for epoch in range(epochs):
        total_loss = 0
        steps = 0
        
        for batch in dataloader:
            history = batch['history'].to(DEVICE)     # (B, L)
            target_idx = batch['target_idx'].to(DEVICE) # (B,) - ì •ë‹µ ì•„ì´í…œì˜ Index
            gender = batch['gender'].to(DEVICE)
            age = batch['age'].to(DEVICE)
            
            profile_data = {'gender': gender, 'age': age}
            
            optimizer.zero_grad()
            
            # (A) User Vector ìƒì„± (B, 128)
            user_vectors = model(history, profile_data)
            
            # (B) Target Item Vector ì¡°íšŒ (B, 128)
            # User Towerê°€ í’ˆê³  ìˆëŠ” Lookup Table(Frozen)ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì„œ ì •ë‹µ ë²¡í„°ë¥¼ ê°€ì ¸ì˜´
            target_item_vectors = model.item_embedding(target_idx)
            
            # (C) In-batch Negative Similarity Calculation (Logits)
            # (B, 128) @ (128, B) -> (B, B) Matrix
            # [i, j]ëŠ” ië²ˆì§¸ ìœ ì €ì™€ jë²ˆì§¸ ì•„ì´í…œ(Target)ì˜ ìœ ì‚¬ë„
            logits = torch.matmul(user_vectors, target_item_vectors.T)
            
            # Temperature Scaling (Optional, SimCSEì²˜ëŸ¼ 0.05 ë“±ìœ¼ë¡œ ë‚˜ëˆ„ê¸°ë„ í•¨)
            logits = logits / 0.1 
            
            # (D) Labels
            # ië²ˆì§¸ ìœ ì €ëŠ” ië²ˆì§¸ ì•„ì´í…œì´ ì •ë‹µì„ -> labels = [0, 1, 2, ..., B-1]
            labels = torch.arange(logits.size(0)).to(DEVICE)
            
            # (E) Loss & Backprop
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            steps += 1
            
        avg_loss = total_loss / steps if steps > 0 else 0
        print(f"   Shape: Epoch {epoch+1}/{epochs} | Avg Loss: {avg_loss:.4f}")

    # 6. Save Model
    save_path = "user_tower_latest.pth"
    torch.save(model.state_dict(), save_path)
    print(f"ğŸ’¾ Model saved to {save_path}")
    print("âœ… User Tower Training Finished.")