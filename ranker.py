from catboost import CatBoostRanker, Pool
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

class GBDTRankingModel:
    """
    [CatBoost Re-ranker]
    ìœ ì € ë²¡í„°ì™€ ì•„ì´í…œ ë²¡í„°ë¥¼ ê²°í•©í•˜ì—¬ í´ë¦­ í™•ë¥ (Rank)ì„ ì˜ˆì¸¡
    """
    def __init__(self, model_path="catboost_ranker.cbm"):
        self.model_path = model_path
        self.model = CatBoostRanker(
            iterations=1000,          # íŠ¸ë¦¬ ê°œìˆ˜ (í•™ìŠµëŸ‰)
            learning_rate=0.03,       # í•™ìŠµë¥ 
            depth=6,                  # íŠ¸ë¦¬ì˜ ê¹Šì´ (í”¼ì²˜ í¬ë¡œì‹± ë³µìž¡ë„)
            loss_function='YetiRank', # ëž­í‚¹ ì „ìš© ì†ì‹¤í•¨ìˆ˜ (NDCG ìµœì í™”)
            eval_metric='NDCG',
            verbose=100,
            task_type="GPU"           # GPUê°€ ìžˆë‹¤ë©´ "GPU"ë¡œ ë³€ê²½ ê°€ëŠ¥
        )
        self.is_fitted = False

    def train(self, user_vectors, item_vectors, labels, group_ids):
        """
        Args:
            user_vectors: (N, 128) numpy array
            item_vectors: (N, 128) numpy array
            labels: (N,) 0 or 1 (í´ë¦­ ì—¬ë¶€)
            group_ids: (N,) ìœ ì € ID (ì¿¼ë¦¬ ë‹¨ìœ„ ê·¸ë£¹í•‘ì„ ìœ„í•´ í•„ìˆ˜)
        """
        # 1. Feature Engineering
        # ìœ ì € ë²¡í„°ì™€ ì•„ì´í…œ ë²¡í„°ë¥¼ ì˜†ìœ¼ë¡œ ë¶™ìž…ë‹ˆë‹¤. (Concatenation)
        # ì¶”ê°€ë¡œ 'ë‚´ì ê°’(ìœ ì‚¬ë„)'ì„ í”¼ì²˜ë¡œ 
        dot_product = np.sum(user_vectors * item_vectors, axis=1, keepdims=True)
        X = np.hstack([user_vectors, item_vectors, dot_product])
        
        # 2. CatBoost Pool ìƒì„±
        train_pool = Pool(
            data=X,
            label=labels,
            group_id=group_ids # "ì´ ìœ ì € ì•ˆì—ì„œ ìˆœì„œë¥¼ ë§žì¶°ë¼"ë¼ëŠ” ëœ»
        )
        
        # 3. í•™ìŠµ
        print("ðŸŒ² Start Training CatBoost Ranker...")
        self.model.fit(train_pool)
        self.is_fitted = True
        
        # 4. ì €ìž¥
        self.model.save_model(self.model_path)
        print(f"âœ… Model saved to {self.model_path}")

    def predict(self, user_vec, item_vecs):
        """
        [Inference]
        user_vec: (128,)
        item_vecs: (K, 128) - í›„ë³´ ì•„ì´í…œ Kê°œ
        Returns: (K,) scores
        """
        if not self.is_fitted:
            # ëª¨ë¸ íŒŒì¼ì´ ìžˆìœ¼ë©´ ë¡œë“œ
            try:
                self.model.load_model(self.model_path)
                self.is_fitted = True
            except:
                # í•™ìŠµëœ ì  ì—†ìœ¼ë©´ ëžœë¤ ì ìˆ˜ ë°˜í™˜ (Cold Start ë°©ì–´)
                return np.random.rand(len(item_vecs))

        # 1. User Vector í™•ìž¥ (Broadcasting)
        # (128,) -> (K, 128)
        K = len(item_vecs)
        user_batch = np.tile(user_vec, (K, 1))
        
        # 2. Feature ìƒì„± (Trainê³¼ ë™ì¼í•´ì•¼ í•¨)
        dot_product = np.sum(user_batch * item_vecs, axis=1, keepdims=True)
        X_test = np.hstack([user_batch, item_vecs, dot_product])
        
        # 3. ì˜ˆì¸¡
        return self.model.predict(X_test)





'''
class CrossNet(nn.Module):
    """
    [Cross Network]
    í”¼ì²˜ ê°„ì˜ ëª…ì‹œì ì¸ ìƒí˜¸ìž‘ìš©(Interaction)ì„ í•™ìŠµí•©ë‹ˆë‹¤.
    ìˆ˜ì‹: x_{l+1} = x_0 * (W_l * x_l + b_l) + x_l
    """
    def __init__(self, input_dim, num_layers=3):
        super().__init__()
        self.num_layers = num_layers
        # ê° ì¸µë§ˆë‹¤ ê°€ì¤‘ì¹˜ì™€ íŽ¸í–¥ì„ ê°€ì§
        self.kernels = nn.ParameterList([
            nn.Parameter(torch.nn.init.xavier_normal_(torch.empty(input_dim, 1))) 
            for _ in range(num_layers)
        ])
        self.biases = nn.ParameterList([
            nn.Parameter(torch.zeros(input_dim)) 
            for _ in range(num_layers)
        ])

    def forward(self, x):
        x_0 = x # ì›ë³¸ ìž…ë ¥ ìœ ì§€
        x_l = x
        
        for i in range(self.num_layers):
            # 1. Linear Projection (W * x)
            # (B, D) @ (D, 1) -> (B, 1) : ìŠ¤ì¹¼ë¼ ê°’ ìƒì„±
            linear_proj = torch.matmul(x_l, self.kernels[i]) + self.biases[i] 
            
            # 2. Feature Crossing (x_0 * Scalar)
            # ì›ë³¸ ìž…ë ¥ x_0ì— ìŠ¤ì¹¼ë¼ë¥¼ ê³±í•´ ëª¨ë“  í”¼ì²˜ ê°„ì˜ êµì°¨ íš¨ê³¼ë¥¼ ëƒ„
            # + x_l (Residual Connection)
            x_l = x_0 * linear_proj + x_l
            
        return x_l

class RankingModel(nn.Module):
    """
    [DCN-V2 Re-ranker]
    Retrieval ëª¨ë¸ì´ ë½‘ì€ í›„ë³´êµ°(Top-N)ì„ ì •ë°€ ì±„ì 
    """
    def __init__(self, user_dim=128, item_dim=128, context_dim=20):
        super().__init__()
        
        total_input_dim = user_dim + item_dim + context_dim
        
        # 1. Cross Network (Explicit Interaction)
        self.cross_net = CrossNet(total_input_dim, num_layers=3)
        
        # 2. Deep Network (Implicit Interaction)
        self.deep_net = nn.Sequential(
            nn.Linear(total_input_dim, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.GELU(),
        )
        
        # 3. Final Prediction Head
        # Cross ì¶œë ¥(D) + Deep ì¶œë ¥(128) -> Score(1)
        self.final_head = nn.Linear(total_input_dim + 128, 1)
        
        
    def forward(self, user_emb, item_emb, context_emb=None):
        # (B, 128), (B, 128), (B, 20)
        
        # 1. Feature Concatenation (Early Interaction)
        if context_emb is not None:
            x = torch.cat([user_emb, item_emb, context_emb], dim=1)
        else:
            x = torch.cat([user_emb, item_emb], dim=1)
            
        # 2. Dual Path Processing
        cross_out = self.cross_net(x) # (B, Total_D)
        deep_out = self.deep_net(x)   # (B, 128)
        
        # 3. Stack & Predict
        stacked = torch.cat([cross_out, deep_out], dim=1)
        logits = self.final_head(stacked)
        
        # 4. Score (0~1 Probability)
        return torch.sigmoid(logits)
    
    
    @torch.no_grad() # ì¶”ë¡  ì „ìš©ì´ë¯€ë¡œ Gradient ê³„ì‚° ë”
    def predict_for_user(self, user_vec, item_vecs, context_vec=None):
        """
        [Inference Helper]
        1ëª…ì˜ ìœ ì € ë²¡í„°ë¥¼ Nê°œì˜ ì•„ì´í…œ ë²¡í„°ì— ë§žì¶°ì„œ í™•ìž¥(Broadcasting)í•˜ê³  ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            user_vec: (1, 128) ë˜ëŠ” (128,) - ìœ ì € 1ëª…ì˜ ë²¡í„°
            item_vecs: (N, 128) - í›„ë³´ ì•„ì´í…œ Nê°œì˜ ë²¡í„°
            context_vec: (1, 20) - í˜„ìž¬ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒ)
        """
        # 1. ì°¨ì› ì •ë¦¬ (1ì°¨ì›ì´ë©´ 2ì°¨ì›ìœ¼ë¡œ)
        if user_vec.dim() == 1:
            user_vec = user_vec.unsqueeze(0) # (128,) -> (1, 128)
            
        # 2. ê°œìˆ˜ í™•ì¸ (N)
        num_candidates = item_vecs.size(0)
        
        # 3. í™•ìž¥ (Broadcasting)
        # (1, 128) -> (N, 128) ë¡œ ë³µì‚¬
        user_batch = user_vec.expand(num_candidates, -1)
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ìžˆë‹¤ë©´ ë™ì¼í•˜ê²Œ í™•ìž¥
        context_batch = None
        if context_vec is not None:
            if context_vec.dim() == 1:
                context_vec = context_vec.unsqueeze(0)
            context_batch = context_vec.expand(num_candidates, -1)
            
        # 4. Forward í˜¸ì¶œ
        # ì´ì œ user_batchì™€ item_vecsì˜ í¬ê¸°ê°€ (N, 128)ë¡œ ê°™ìœ¼ë¯€ë¡œ forward ì‚¬ìš© ê°€ëŠ¥
        scores = self.forward(user_batch, item_vecs, context_batch)
        
        return scores.squeeze() # (N, 1) -> (N,) í˜•íƒœë¡œ 
        
'''