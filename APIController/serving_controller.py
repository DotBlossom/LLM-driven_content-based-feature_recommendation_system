
import logging
import os
from fastapi import BackgroundTasks, FastAPI, Depends, HTTPException, APIRouter,status
from pydantic import BaseModel, Field
import torch.nn.functional as F
import torch
from torch.nn.utils.rnn import pad_sequence
from typing import Any, Dict, List, Optional, Tuple
from database import ProductInferenceInput, ProductInferenceVectors, UserSession, get_db
#from inference import RecommendationService
#from train import UserTowerTrainDataset, train_final_user_tower, train_simcse_from_db #train_user_tower_task
#from utils.dependencies import get_global_batch_size, get_global_encoder, get_global_projector #get_global_rec_service
from item_tower import HybridItemTower, OptimizedItemTower, train_simcse_from_db
from utils.dependencies import get_global_batch_size, get_global_encoder, get_global_projector
from utils.inference_utils import generate_and_save_item_vectors
import utils.vocab as vocab 
import numpy as np
# from model import ALL_FIELD_KEYS, CoarseToFineItemTower, FinalUserTower, OptimizedItemTower, SimCSEModelWrapper, load_pretrained_vectors_from_db
from sqlalchemy.orm import Session
from sqlalchemy.dialects.postgresql import insert  
import torch.nn as nn

serving_controller_router = APIRouter()
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_DIR = "models"



class ProductInput(BaseModel):
    """
    [ëª¨ë¸ ì…ë ¥ìš© Pydantic ìŠ¤í‚¤ë§ˆ]
    DB ORM ê°ì²´ì—ì„œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    """
    product_id: int
    feature_data: Dict[str, Any]


## process-pending -> product_serviceë¡œ ìŠ¤í‚¤ë§ˆë¡œë”© ë³€ê²½




# API 3 ì…ë ¥ìš©
class ProductIdListSchema(BaseModel):
    product_ids: List[int]





            
@serving_controller_router.post("/train/item-tower")
def train_item_tower(encoder: nn.Module = Depends(get_global_encoder),
                     projector: nn.Module = Depends(get_global_projector),
                     db: Session = Depends(get_db),
                     batch_size: int = Depends(get_global_batch_size),
                     epochs: int = 5,
                     lr: float = 5e-5,
                     checkpoint_path : str =None ):
            
    train_simcse_from_db(encoder, projector, db_session=db, batch_size=batch_size, epochs=epochs, lr = lr, checkpoint_path= checkpoint_path)
'''
class ItemTowerFineTuneRequest(BaseModel):
    checkpoint_path: str = Field(..., description="Epoch 3 ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ì˜ˆ: models/encoder_ep03...pth)")
    epochs: int = Field(2, description="ì¶”ê°€ í•™ìŠµ ì—í¬í¬ (ê¸°ë³¸ 2)")
    batch_size: int = Field(64, description="ë°°ì¹˜ ì‚¬ì´ì¦ˆ")
    lr: float = Field(5e-5, description="Fine-tuning í•™ìŠµë¥  (ê¸°ë³¸ 5e-5)")
    dropout_prob: float = Field(0.2, description="Fine-tuning ë“œë¡­ì•„ì›ƒ (ê¸°ë³¸ 0.2)")
    temperature: float = Field(0.08, description="Temperature (ê¸°ë³¸ 0.08)")

@serving_controller_router.post("/train/item-tower/finetune/sync")
def start_item_tower_finetune_sync(
    req: ItemTowerFineTuneRequest,
    db: Session = Depends(get_db)  # DB ì„¸ì…˜ ì£¼ì…
):
    """
    [ë™ê¸° ì‹¤í–‰] Item Tower Fine-tuning
    - ìš”ì²­ì„ ë³´ë‚´ë©´ í•™ìŠµì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¸ë‹¤ê°€ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - í´ë¼ì´ì–¸íŠ¸ Timeoutì„ ë§¤ìš° ê¸¸ê²Œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    
    # 1. ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ í™•ì¸
    if not os.path.exists(req.checkpoint_path):
        raise HTTPException(status_code=404, detail=f"Checkpoint file not found: {req.checkpoint_path}")

    print(f"â³ [Sync] Fine-tuning requested. This may take a while...")

    try:
        # ì´ íŒŒë¼ë¯¸í„°ë“¤ì€ í”„ë¡œì íŠ¸ ì„¤ì •(config)ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒìˆ˜ë¡œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
        encoder = HybridItemTower(
            std_vocab_size=STD_VOCAB_SIZE,
            num_std_fields=NUM_STD_FIELDS,
            embed_dim=EMBED_DIM,
            output_dim=EMBED_DIM
        )
        projector = OptimizedItemTower(
            input_dim=EMBED_DIM, 
            output_dim=EMBED_DIM
        )
    # 3. í•™ìŠµ í•¨ìˆ˜ ì§ì ‘ í˜¸ì¶œ (ì—¬ê¸°ì„œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    try:
        train_simcse_from_db(
            encoder = Depends(get_global_encoder),
            projector = Depends(get_global_projector),
            db_session=db,              # ì£¼ì…ë°›ì€ ì„¸ì…˜ ì „ë‹¬
            batch_size=req.batch_size,
            epochs=req.epochs,
            lr=req.lr,
            checkpoint_path=req.checkpoint_path,  # âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
            dropout_prob=req.dropout_prob,        # âœ… ë“œë¡­ì•„ì›ƒ ì ìš© (0.2)
            temperature=req.temperature
        )
    except Exception as e:
        print(f"âŒ Training Error: {e}")
        raise HTTPException(status_code=500, detail=f"Training Failed: {str(e)}")

    # 4. ì™„ë£Œ í›„ ì‘ë‹µ ë°˜í™˜
    return {
        "status": "success",
        "message": "Fine-tuning completed successfully.",
        "details": {
            "resumed_from": req.checkpoint_path,
            "trained_epochs": req.epochs,
            "final_lr": req.lr,
            "used_dropout": req.dropout_prob
        }
    }

'''
class VectorUpdateResponse(BaseModel):
    status: str
    message: str
    saved_path_matrix: str
    saved_path_ids: str
    item_count: int
    vector_shape: list
@serving_controller_router.post("/bg/inference/refresh-item-vectors" ,response_model=VectorUpdateResponse)
def update_item_vectors_api(
    save_dir: str = "models",  # ì €ì¥ ê²½ë¡œë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë°›ì„ ìˆ˜ ìˆê²Œ í•¨
    db: Session = Depends(get_db),
    checkpoint_path : Optional[str] = None
):
    """
    [ê´€ë¦¬ììš©] DBì˜ ëª¨ë“  ì•„ì´í…œì„ ë¡œë“œí•˜ì—¬ Pre-trained Vector Matrixë¥¼ ìƒì„± ë° ê°±ì‹ í•©ë‹ˆë‹¤.
    - User Tower í•™ìŠµ ì „ì— ë°˜ë“œì‹œ ìˆ˜í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    - ìˆ˜í–‰ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ëŒ€ëŸ‰ ë°ì´í„° ì‹œ BackgroundTasks ê¶Œì¥)
    """
    try:
        print(f"ğŸ”„ [API] Request received: Update item vectors in '{save_dir}'")
        
        # 1. ë²¡í„° ìƒì„± í•¨ìˆ˜ í˜¸ì¶œ (ë¦¬íŒ©í† ë§ëœ í•¨ìˆ˜)
        # ë°˜í™˜ê°’: (Tensor, List[str])
        final_tensor, ordered_ids = generate_and_save_item_vectors(db, save_dir,checkpoint_path=checkpoint_path)
        
        if final_tensor is None:
             raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Vector generation returned None. Check server logs."
            )

        # 2. ê²°ê³¼ ì‘ë‹µ ìƒì„±
        tensor_path = os.path.join(save_dir, "pretrained_item_matrix.pt")
        ids_path = os.path.join(save_dir, "item_ids.pt")

        return VectorUpdateResponse(
            status="success",
            message="Item vectors successfully updated and aligned.",
            saved_path_matrix=tensor_path,
            saved_path_ids=ids_path,
            item_count=len(ordered_ids),
            vector_shape=list(final_tensor.shape)
        )

    except Exception as e:
        print(f"âŒ [API Error] {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update vectors: {str(e)}"
        )
'''

def preprocess_batch_input(products: List[Any]) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    [Residual Field Embeddingìš© ì „ì²˜ë¦¬ - Batch Optimization Ver.]
    
    ìµœì í™” ì›ë¦¬:
    1. ê°œë³„ í† í¬ë‚˜ì´ì§• í˜¸ì¶œ(N*Më²ˆ)ì„ ì œê±°í•˜ê³ ,
    2. ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ ëª¨ì•„ì„œ ë‹¨ í•œ ë²ˆì˜ Batch Tokenizing ìˆ˜í–‰
    """

            
    # ë°°ì¹˜ í¬ê¸° ë° í•„ë“œ ìˆ˜ ê³„ì‚°
    B = len(products)
    F = len(ALL_FIELD_KEYS)
    S = vocab.RE_MAX_TOKEN_LEN
    
    # 1. ê²°ê³¼ í…ì„œ ë¯¸ë¦¬ ì´ˆê¸°í™” (ì „ë¶€ PADë¡œ ì±„ì›€)
    # ì´ë ‡ê²Œ í•˜ë©´ ë°ì´í„°ê°€ ì—†ëŠ” ê³³(None/Empty)ì€ ê±´ë“œë¦´ í•„ìš”ê°€ ì—†ì–´ì§ (ìë™ íŒ¨ë”© íš¨ê³¼)
    # Shape: (Batch, Num_Fields, Seq_Len)
    t_re_batch = torch.full((B, F, S), vocab.RE_TOKENIZER.pad_token_id, dtype=torch.long, device=DEVICE)
    
    batch_std_ids = []
    
    # [Batch Tokenizingì„ ìœ„í•œ ìˆ˜ì§‘í†µ]
    flat_texts = []      # í† í¬ë‚˜ì´ì§• í•  í…ìŠ¤íŠ¸ë“¤
    flat_indices = []    # ê·¸ í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ê°ˆ ìœ„ì¹˜ (batch_idx, field_idx)
    
    for i, product in enumerate(products):
        
        raw_feature_data: Dict[str, Any] = getattr(product, 'feature_data', {})
        
        feature_data = raw_feature_data.copy()
        flatten_geometry_features(feature_data)
        
        clothes_data = feature_data.get("clothes", {})
        re_data = feature_data.get("reinforced_feature_value", {})
        
        # 1-3. ë°ì´í„° ì„¹ì…˜ ë¶„ë¦¬
        clothes_data = feature_data.get("clothes", {})
        re_data = feature_data.get("reinforced_feature_value", {})


        # ========================================================
        
        row_std_ids = []
        
        for j, key in enumerate(ALL_FIELD_KEYS):
            
            # --- A. STD ID (Lookupì€ ë¹ ë¥´ë¯€ë¡œ ë£¨í”„ ìœ ì§€) ---
            std_val = clothes_data.get(key)
            if isinstance(std_val, list):
                std_val = std_val[0] if std_val else None
            
            s_id = vocab.get_std_id(key, std_val)
            row_std_ids.append(s_id)
            
            # --- B. RE Text ìˆ˜ì§‘ (í† í¬ë‚˜ì´ì§• X) ---
            re_val_list = re_data.get(key)
            re_text = None
            
            if re_val_list:
                if isinstance(re_val_list, list) and len(re_val_list) > 0:
                    re_text = str(re_val_list[0])
                elif isinstance(re_val_list, str):
                    re_text = re_val_list
            
            # ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìˆ˜ì§‘ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if re_text and re_text.strip():
                flat_texts.append(re_text)
                flat_indices.append((i, j)) # ì¢Œí‘œ ê¸°ì–µ (ië²ˆì§¸ ìƒí’ˆ, jë²ˆì§¸ í•„ë“œ)
        
        batch_std_ids.append(row_std_ids)

    # 2. [í•µì‹¬] Batch Tokenization (ë‹¨ 1íšŒ í˜¸ì¶œ)
    if flat_texts:
        # Rust ê¸°ë°˜ì˜ ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜í–‰
        encoded = vocab.RE_TOKENIZER(
            flat_texts,
            padding='max_length',
            max_length=S,
            truncation=True,
            return_tensors='pt'
        )
        
        # encoded['input_ids'] shape: (N_valid_texts, Seq_Len)
        valid_tokens = encoded['input_ids'].to(DEVICE)
        
        # 3. [Scatter] ê²°ê³¼ í…ì„œì— ì œìë¦¬ ì°¾ì•„ ë„£ê¸° (Fancy Indexing)
        # rows: ë°°ì¹˜ ì¸ë±ìŠ¤ë“¤, cols: í•„ë“œ ì¸ë±ìŠ¤ë“¤
        rows = [idx[0] for idx in flat_indices]
        cols = [idx[1] for idx in flat_indices]
        
        # í•œ ë²ˆì— í• ë‹¹ (forë¬¸ ì—†ì´ í…ì„œ ì—°ì‚°ìœ¼ë¡œ ì²˜ë¦¬)
        t_re_batch[rows, cols] = valid_tokens

    # 4. STD í…ì„œ ë³€í™˜
    t_std_batch = torch.tensor(batch_std_ids, dtype=torch.long, device=DEVICE)

    return t_std_batch, t_re_batch



def generate_item_vectors(
    products: List[ProductInferenceInput], 
    encoder: nn.Module 
    
) -> Dict[int, List[float]]:
    """
    [Core Inference Logic]
    ProductInput ë¦¬ìŠ¤íŠ¸ -> Encoder(Stage1) -> L2 Normalize -> {product_id: vector} ë°˜í™˜
    """
    if not products:
        return {}

    # 1. ëª¨ë¸ Wrapper ì„¤ì • ë° Eval ëª¨ë“œ
    model = encoder.to(DEVICE)
    model.eval()

    # 2. ì „ì²˜ë¦¬ (collate_fn ë¡œì§ í¬í•¨ëœ í•¨ìˆ˜ ì‚¬ìš© ê°€ì •)
    try:
        t_std, t_re = preprocess_batch_input(products)
    except Exception as e:
        print(f"âŒ Preprocessing Error: {e}")
        return {}

    t_std = t_std.to(DEVICE)
    t_re = t_re.to(DEVICE)

    # 3. ì¶”ë¡  (No Grad)
    with torch.no_grad():
        raw_v = model(t_std, t_re)
        final_vectors_tensor = F.normalize(raw_v, p=2, dim=1)
    # 4. ê²°ê³¼ ë³€í™˜
    vectors_list = final_vectors_tensor.cpu().numpy().tolist()
    
    result_map = {}
    for idx, product in enumerate(products):
        result_map[product.product_id] = vectors_list[idx]

    return result_map




def run_pipeline_and_save(
    db_session: Session, 
    products: List[ProductInferenceInput],
    encoder: nn.Module     
    
):
    """
    [ê³µí†µ ë¡œì§] 
    DB ê°ì²´ ë¦¬ìŠ¤íŠ¸ -> Pydantic ë³€í™˜ -> ì¶”ë¡  -> ë²¡í„° ì €ì¥ -> Flag ì—…ë°ì´íŠ¸
    """
    if not products:
        return 0

    # 1. DB ê°ì²´(ORM)ë¥¼ ëª¨ë¸ ì…ë ¥ìš© Pydantic ê°ì²´ë¡œ ë³€í™˜
    input_list = [
        ProductInput(product_id=p.product_id, feature_data=p.feature_data)
        for p in products
    ]


    # 1-1. load

    encoder_path = os.path.join(MODEL_DIR, "encoder_stage1.pth")
    #projector_path = os.path.join(MODEL_DIR, "projector_stage2.pth")

    if os.path.exists(encoder_path): #and os.path.exists(projector_path):
        try:
            encoder_state = torch.load(encoder_path, map_location=DEVICE)
            #projector_state = torch.load(projector_path, map_location=DEVICE)
            
            encoder.load_state_dict(encoder_state)
            #projector.load_state_dict(projector_state)
            
            print("âœ… Models loaded successfully.")
        except Exception as e:
            print(f"âŒ Error loading state dicts: {e}")
            raise e
    else:
        raise FileNotFoundError(f"âŒ Model files not found in {MODEL_DIR}")


    # 2. ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (generate_item_vectors í˜¸ì¶œ)
    #    ê²°ê³¼ëŠ” {product_id: [0.12, 0.55, ...]} í˜•íƒœ
    try:
        vector_map = generate_item_vectors(input_list, encoder)
    except Exception as e:
        print(f"âŒ Inference Failed: {e}")
        raise e

    # 3. ê²°ê³¼ ì €ì¥ ë° í”Œë˜ê·¸ ì—…ë°ì´íŠ¸
    for p in products:
        # í˜¹ì‹œ ëª¨ë¥¼ ì—ëŸ¬ë¡œ íŠ¹ì • IDê°€ ëˆ„ë½ëëŠ”ì§€ í™•ì¸
        if p.product_id not in vector_map:
            continue
            
        vector_val = vector_map[p.product_id]
        
        # ë²¡í„° í…Œì´ë¸”ì— ì €ì¥ (Upsert ë¡œì§)
        existing_vec = db_session.query(ProductInferenceVectors).filter_by(id=p.product_id).first()
        if existing_vec:
            existing_vec.vector_embedding= vector_val
        else:
            new_vec = ProductInferenceVectors(id=p.product_id, vector_embedding=vector_val)
            db_session.add(new_vec)
        
        # [ì‘ì—… ì™„ë£Œ Flag ì²˜ë¦¬]
        p.is_vectorized = True
    
    db_session.commit()
    print("âœ… Saved Item Vectors (by encoder) successfully.")
    return len(vector_map)





# --- API 2. í•™ìŠµ ìš”ì²­ (Background Task) ---
@serving_controller_router.post("/train/start")
async def start_training(background_tasks: BackgroundTasks,
                         encoder_instance: CoarseToFineItemTower = Depends(get_global_encoder), 
                         projector_instance: OptimizedItemTower = Depends(get_global_projector),
                         g_batch_size: int = Depends(get_global_batch_size)):
    """
    [API 2] DBì— ìˆëŠ” ë°ì´í„°ë¡œ SimCSE í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. (ë¹„ë™ê¸° ì‹¤í–‰)
    """
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ë„˜ê¹€ (APIëŠ” ì¦‰ì‹œ ì‘ë‹µ)
    background_tasks.add_task(train_simcse_from_db,
        encoder=encoder_instance,
        projector=projector_instance,
        batch_size = g_batch_size
    )
    
    return {"message": "Training started in the background.", "status": "processing"}


# batch size ë§ì¶°ì•¼í•¨
@serving_controller_router.post("/vectors/process-pending")
def process_pending_vectors(
    batch_size: int = Depends(get_global_batch_size),
    db: Session = Depends(get_db),
    # [ìˆ˜ì •] ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì£¼ì…
    encoder: CoarseToFineItemTower = Depends(get_global_encoder)

):
    total_processed_count = 0
    
    while True:
        # 1. ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë°ì´í„° ì¡°íšŒ (batch_size? ì¼ë‹¨)
        pending_products = db.query(ProductInferenceInput)\
                             .filter(ProductInferenceInput.is_vectorized == False)\
                             .limit(batch_size)\
                             .all()
        
        if not pending_products:
            break

        # 2. ê³µí†µ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        current_count = run_pipeline_and_save(db, pending_products, encoder)
        
        total_processed_count += current_count

    if total_processed_count == 0:
        return {"status": "success", "message": "No pending products to process."}

    return {
        "status": "success", 
        "processed_count": total_processed_count, 
        "message": f"All pending batches processed successfully. (Total: {total_processed_count})"
    }
    
# ------------------------------------------------------------------
# API 3. íŠ¹ì • ID ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë²¡í„°í™” (On-Demand Processing)
# ------------------------------------------------------------------
@serving_controller_router.post("/vectors/process-by-ids")
def process_vectors_by_ids(
    payload: ProductIdListSchema, 
    db: Session = Depends(get_db),
    # [ìˆ˜ì •] ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì£¼ì…
    encoder: CoarseToFineItemTower = Depends(get_global_encoder)
    #projector: OptimizedItemTower = Depends(get_global_projector)
):
    # 1. ID ì¡°íšŒ
    target_products = db.query(ProductInferenceInput)\
                        .filter(ProductInferenceInput.product_id.in_(payload.product_ids))\
                        .all()
    
    if not target_products:
        raise HTTPException(status_code=404, detail="No products found for given IDs.")

    # 2. ê³µí†µ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ëª¨ë¸ ì „ë‹¬)
    processed_count = run_pipeline_and_save(db, target_products, encoder)
    
    return {
        "status": "success", 
        "processed_count": processed_count, 
        "message": "On-demand processing completed."
    }




# ------------------------------------------------------------------
# API 4. User Tower Train
# ------------------------------------------------------------------

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



class TrainRequest(BaseModel):
    epochs: int = 5
    batch_size: int = 1
    learning_rate: float = 1e-4
    max_seq_len: int = 50
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ì˜µì…˜)
    save_path: str = "./user_tower_latest.pth"

class TrainResponse(BaseModel):
    status: str
    final_avg_loss: float
    trained_epochs: int
    model_save_path: str
    message: str

# ==========================================
# 2. Service Layer (Pipeline Logic)
# ==========================================
def preprocess_db_sessions(sessions: list, product_id_map: dict, max_seq_len: int) -> List[dict]:
    """DB ì„¸ì…˜ ë°ì´í„°ë¥¼ í•™ìŠµìš© ë°ì´í„°(Dict)ë¡œ ë³€í™˜"""
    data = []
    
    # ë§¤í•‘ í…Œì´ë¸”
    GENDER_MAP = {'M': 1, 'F': 2, 'UNKNOWN': 0}
    SEASON_MAP = {'SPRING_AUTUMN': 1, 'SUMMER': 2, 'WINTER': 3, 'UNKNOWN': 0}
    
    for sess in sessions:
        if not sess.events: continue
        
        # 1. Sort Events
        events = sorted(sess.events, key=lambda e: e.timestamp)
        if len(events) < 2: continue # ìµœì†Œ 2ê°œ (History 1 + Target 1)
        
        # 2. Map Product IDs
        seq_indices = [product_id_map.get(e.product_id, 0) for e in events]
        
        # 3. Create Row
        # ë§ˆì§€ë§‰ ì•„ì´í…œ = Target, ê·¸ ì´ì „ = History
        data.append({
            'history': seq_indices[:-1], 
            'target_idx': seq_indices[-1],
            'gender': GENDER_MAP.get(sess.user.gender, 0),
            'season': SEASON_MAP.get(sess.season, 0),
            'age': 0 # í•„ìš”ì‹œ ì¶”ê°€ êµ¬í˜„
        })
        
    return data

def run_training_pipeline(db: Session, config: TrainRequest) -> TrainResponse:
    logger.info("ğŸš€ Starting Training Pipeline via API...")

    # 1. Load Pretrained Item Vectors (Fixed)
    # item_matrix: (Num_Total_Items + 1, 128)
    item_matrix, id_map = load_pretrained_vectors_from_db(db)
    logger.info(f"âœ… Loaded {len(id_map)} item vectors from DB.")

    # 2. Fetch User Sessions (Training Data)
    # ì‹¤ì œë¡œëŠ” ê¸°ê°„ ì¿¼ë¦¬ ë“±ì„ ì¶”ê°€í•´ì•¼ í•¨
    sessions = db.query(UserSession).join(UserSession.user).join(UserSession.events).limit(5000).all()
    
    if not sessions:
        raise HTTPException(status_code=400, detail="No session data found in DB.")

    # 3. Preprocessing
    training_data = preprocess_db_sessions(sessions, id_map, config.max_seq_len)
    logger.info(f"âœ… Prepared {len(training_data)} training samples.")
    
    if len(training_data) == 0:
        raise HTTPException(status_code=400, detail="Valid training data is empty after preprocessing.")

    # 4. DataLoader Setup
    dataset = UserTowerTrainDataset(training_data, id_map, max_seq_len=config.max_seq_len)
    train_loader = torch.utils.data.DataLoader(
        dataset, batch_size=config.batch_size, shuffle=True
    )

    # 5. Initialize Final Use
    #r Tower
    num_total_items = item_matrix.size(0)
    user_tower = FinalUserTower(
        num_total_products=num_total_items - 1,
        pretrained_item_matrix=item_matrix, # Weight ì´ˆê¸°í™”ìš©
        max_seq_len=config.max_seq_len
    )

    # 6. Execute Training Loop
    # train_final_user_tower í•¨ìˆ˜ê°€ í•™ìŠµëœ ëª¨ë¸ì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
    trained_model = train_final_user_tower(
        user_tower=user_tower,
        pretrained_item_matrix=item_matrix, # Loss ê³„ì‚°ìš©
        train_loader=train_loader,
        epochs=config.epochs,
        lr=config.learning_rate
    )

    # 7. Save Model
    torch.save(trained_model.state_dict(), config.save_path)
    logger.info(f"ğŸ’¾ Model saved to {config.save_path}")

    return TrainResponse(
        status="success",
        final_avg_loss=0.0, # Loopì—ì„œ ë§ˆì§€ë§‰ Lossë¥¼ ë¦¬í„´ë°›ë„ë¡ ìˆ˜ì • í•„ìš” (ì—¬ê¸°ì„  Dummy)
        trained_epochs=config.epochs,
        model_save_path=config.save_path,
        message=f"Training completed with {len(training_data)} samples."
    )

# ==========================================
# 3. API Endpoints
# ==========================================
@serving_controller_router.post("/train/user-tower", response_model=TrainResponse)
def trigger_training_job(req: TrainRequest, db: Session = Depends(get_db)):
    """
    ìœ ì € íƒ€ì›Œ í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (Synchronous for Demo)
    ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” BackgroundTasks ë˜ëŠ” Celeryë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    """
    try:
        result = run_training_pipeline(db, req)
        return result
    except Exception as e:
        logger.error(f"Training Failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))










'''

