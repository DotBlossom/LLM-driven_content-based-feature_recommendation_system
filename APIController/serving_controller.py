
import logging
import os
from fastapi import BackgroundTasks, FastAPI, Depends, HTTPException, APIRouter
from pydantic import BaseModel
import torch.nn.functional as F
import torch
from torch.nn.utils.rnn import pad_sequence
from typing import Any, Dict, List, Tuple
from database import ProductInferenceInput, ProductInferenceVectors, UserSession, get_db
#from inference import RecommendationService
from train import UserTowerTrainDataset, train_final_user_tower, train_simcse_from_db #train_user_tower_task
from utils.dependencies import get_global_batch_size, get_global_encoder, get_global_projector #get_global_rec_service
import utils.vocab as vocab 
import numpy as np
from model import ALL_FIELD_KEYS, CoarseToFineItemTower, FinalUserTower, OptimizedItemTower, SimCSEModelWrapper, load_pretrained_vectors_from_db
from sqlalchemy.orm import Session
from sqlalchemy.dialects.postgresql import insert  
import torch.nn as nn


serving_controller_router = APIRouter()
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_DIR = "models"



class ProductInput(BaseModel):
    """
    [ëª¨ë¸ ì…ë ¥ìš© Pydantic ìŠ¤í‚¤ë§ˆ]
    DB ORM ê°ì²´ì—ì„œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
    """
    product_id: int
    feature_data: Dict[str, Any]


## process-pending -> product_serviceë¡œ ìŠ¤í‚¤ë§ˆë¡œë”© ë³€ê²½




# API 3 ì…ë ¥ìš©
class ProductIdListSchema(BaseModel):
    product_ids: List[int]


import torch
from typing import List, Tuple, Dict, Any

# ì „ì—­ ë³€ìˆ˜ ALL_FIELD_KEYSê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆ: ALL_FIELD_KEYS = ["category", "season", "color", ...] 

def flatten_geometry_features(feature_data: Dict[str, Any]) -> None:
    """
    feature_data ë‚´ë¶€ì˜ 'structural.geometry'ë¥¼ ì°¾ì•„ì„œ
    ìƒìœ„ ë ˆë²¨ì¸ 'reinforced_feature_value'ì— 'geo_' ì ‘ë‘ì–´ë¡œ í’€ì–´ëƒ…ë‹ˆë‹¤.
    (In-place modification)
    """
    re_data = feature_data.get("reinforced_feature_value", {})
    if not re_data:
        return

    # geometry ë°ì´í„°ê°€ ìˆìœ¼ë©´ êº¼ëƒ„ (Dictionaryì—ì„œ pop)
    geometry_data = re_data.pop("structural.geometry", None)
    
    # ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¼ë©´ í¼ì³ì„œ ìƒìœ„ì— ë³‘í•©
    if geometry_data and isinstance(geometry_data, dict):
        for sub_key, sub_val in geometry_data.items():
            # ì˜ˆ: width_flow -> geo_width_flow
            new_key = f"geo_{sub_key}"
            re_data[new_key] = sub_val

def preprocess_batch_input(products: List[Any]) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    [Residual Field Embeddingìš© ì „ì²˜ë¦¬ - Batch Optimization Ver.]
    
    ìµœì í™” ì›ë¦¬:
    1. ê°œë³„ í† í¬ë‚˜ì´ì§• í˜¸ì¶œ(N*Më²ˆ)ì„ ì œê±°í•˜ê³ ,
    2. ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ ëª¨ì•„ì„œ ë‹¨ í•œ ë²ˆì˜ Batch Tokenizing ìˆ˜í–‰
    """

            
    # ë°°ì¹˜ í¬ê¸° ë° í•„ë“œ ìˆ˜ ê³„ì‚°
    B = len(products)
    F = len(ALL_FIELD_KEYS)
    S = vocab.RE_MAX_TOKEN_LEN
    
    # 1. ê²°ê³¼ í…ì„œ ë¯¸ë¦¬ ì´ˆê¸°í™” (ì „ë¶€ PADë¡œ ì±„ì›€)
    # ì´ë ‡ê²Œ í•˜ë©´ ë°ì´í„°ê°€ ì—†ëŠ” ê³³(None/Empty)ì€ ê±´ë“œë¦´ í•„ìš”ê°€ ì—†ì–´ì§ (ìë™ íŒ¨ë”© íš¨ê³¼)
    # Shape: (Batch, Num_Fields, Seq_Len)
    t_re_batch = torch.full((B, F, S), vocab.RE_TOKENIZER.pad_token_id, dtype=torch.long, device=DEVICE)
    
    batch_std_ids = []
    
    # [Batch Tokenizingì„ ìœ„í•œ ìˆ˜ì§‘í†µ]
    flat_texts = []      # í† í¬ë‚˜ì´ì§• í•  í…ìŠ¤íŠ¸ë“¤
    flat_indices = []    # ê·¸ í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ê°ˆ ìœ„ì¹˜ (batch_idx, field_idx)
    
    for i, product in enumerate(products):
        
        raw_feature_data: Dict[str, Any] = getattr(product, 'feature_data', {})
        
        feature_data = raw_feature_data.copy()
        flatten_geometry_features(feature_data)
        
        clothes_data = feature_data.get("clothes", {})
        re_data = feature_data.get("reinforced_feature_value", {})
        
        # 1-3. ë°ì´í„° ì„¹ì…˜ ë¶„ë¦¬
        clothes_data = feature_data.get("clothes", {})
        re_data = feature_data.get("reinforced_feature_value", {})


        # ========================================================
        
        row_std_ids = []
        
        for j, key in enumerate(ALL_FIELD_KEYS):
            
            # --- A. STD ID (Lookupì€ ë¹ ë¥´ë¯€ë¡œ ë£¨í”„ ìœ ì§€) ---
            std_val = clothes_data.get(key)
            if isinstance(std_val, list):
                std_val = std_val[0] if std_val else None
            
            s_id = vocab.get_std_id(key, std_val)
            row_std_ids.append(s_id)
            
            # --- B. RE Text ìˆ˜ì§‘ (í† í¬ë‚˜ì´ì§• X) ---
            re_val_list = re_data.get(key)
            re_text = None
            
            if re_val_list:
                if isinstance(re_val_list, list) and len(re_val_list) > 0:
                    re_text = str(re_val_list[0])
                elif isinstance(re_val_list, str):
                    re_text = re_val_list
            
            # ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìˆ˜ì§‘ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            if re_text and re_text.strip():
                flat_texts.append(re_text)
                flat_indices.append((i, j)) # ì¢Œí‘œ ê¸°ì–µ (ië²ˆì§¸ ìƒí’ˆ, jë²ˆì§¸ í•„ë“œ)
        
        batch_std_ids.append(row_std_ids)

    # 2. [í•µì‹¬] Batch Tokenization (ë‹¨ 1íšŒ í˜¸ì¶œ)
    if flat_texts:
        # Rust ê¸°ë°˜ì˜ ê³ ì† ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜í–‰
        encoded = vocab.RE_TOKENIZER(
            flat_texts,
            padding='max_length',
            max_length=S,
            truncation=True,
            return_tensors='pt'
        )
        
        # encoded['input_ids'] shape: (N_valid_texts, Seq_Len)
        valid_tokens = encoded['input_ids'].to(DEVICE)
        
        # 3. [Scatter] ê²°ê³¼ í…ì„œì— ì œìë¦¬ ì°¾ì•„ ë„£ê¸° (Fancy Indexing)
        # rows: ë°°ì¹˜ ì¸ë±ìŠ¤ë“¤, cols: í•„ë“œ ì¸ë±ìŠ¤ë“¤
        rows = [idx[0] for idx in flat_indices]
        cols = [idx[1] for idx in flat_indices]
        
        # í•œ ë²ˆì— í• ë‹¹ (forë¬¸ ì—†ì´ í…ì„œ ì—°ì‚°ìœ¼ë¡œ ì²˜ë¦¬)
        t_re_batch[rows, cols] = valid_tokens

    # 4. STD í…ì„œ ë³€í™˜
    t_std_batch = torch.tensor(batch_std_ids, dtype=torch.long, device=DEVICE)

    return t_std_batch, t_re_batch



def generate_item_vectors(
    products: List[ProductInferenceInput], 
    encoder: nn.Module 
    
) -> Dict[int, List[float]]:
    """
    [Core Inference Logic]
    ProductInput ë¦¬ìŠ¤íŠ¸ -> Encoder(Stage1) -> L2 Normalize -> {product_id: vector} ë°˜í™˜
    """
    if not products:
        return {}

    # 1. ëª¨ë¸ Wrapper ì„¤ì • ë° Eval ëª¨ë“œ
    model = encoder.to(DEVICE)
    model.eval()

    # 2. ì „ì²˜ë¦¬ (collate_fn ë¡œì§ í¬í•¨ëœ í•¨ìˆ˜ ì‚¬ìš© ê°€ì •)
    try:
        t_std, t_re = preprocess_batch_input(products)
    except Exception as e:
        print(f"âŒ Preprocessing Error: {e}")
        return {}

    t_std = t_std.to(DEVICE)
    t_re = t_re.to(DEVICE)

    # 3. ì¶”ë¡  (No Grad)
    with torch.no_grad():
        raw_v = model(t_std, t_re)
        final_vectors_tensor = F.normalize(raw_v, p=2, dim=1)
    # 4. ê²°ê³¼ ë³€í™˜
    vectors_list = final_vectors_tensor.cpu().numpy().tolist()
    
    result_map = {}
    for idx, product in enumerate(products):
        result_map[product.product_id] = vectors_list[idx]

    return result_map




def run_pipeline_and_save(
    db_session: Session, 
    products: List[ProductInferenceInput],
    encoder: nn.Module     
    
):
    """
    [ê³µí†µ ë¡œì§] 
    DB ê°ì²´ ë¦¬ìŠ¤íŠ¸ -> Pydantic ë³€í™˜ -> ì¶”ë¡  -> ë²¡í„° ì €ì¥ -> Flag ì—…ë°ì´íŠ¸
    """
    if not products:
        return 0

    # 1. DB ê°ì²´(ORM)ë¥¼ ëª¨ë¸ ì…ë ¥ìš© Pydantic ê°ì²´ë¡œ ë³€í™˜
    input_list = [
        ProductInput(product_id=p.product_id, feature_data=p.feature_data)
        for p in products
    ]


    # 1-1. load

    encoder_path = os.path.join(MODEL_DIR, "encoder_stage1.pth")
    #projector_path = os.path.join(MODEL_DIR, "projector_stage2.pth")

    if os.path.exists(encoder_path): #and os.path.exists(projector_path):
        try:
            encoder_state = torch.load(encoder_path, map_location=DEVICE)
            #projector_state = torch.load(projector_path, map_location=DEVICE)
            
            encoder.load_state_dict(encoder_state)
            #projector.load_state_dict(projector_state)
            
            print("âœ… Models loaded successfully.")
        except Exception as e:
            print(f"âŒ Error loading state dicts: {e}")
            raise e
    else:
        raise FileNotFoundError(f"âŒ Model files not found in {MODEL_DIR}")


    # 2. ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (generate_item_vectors í˜¸ì¶œ)
    #    ê²°ê³¼ëŠ” {product_id: [0.12, 0.55, ...]} í˜•íƒœ
    try:
        vector_map = generate_item_vectors(input_list, encoder)
    except Exception as e:
        print(f"âŒ Inference Failed: {e}")
        raise e

    # 3. ê²°ê³¼ ì €ì¥ ë° í”Œë˜ê·¸ ì—…ë°ì´íŠ¸
    for p in products:
        # í˜¹ì‹œ ëª¨ë¥¼ ì—ëŸ¬ë¡œ íŠ¹ì • IDê°€ ëˆ„ë½ëëŠ”ì§€ í™•ì¸
        if p.product_id not in vector_map:
            continue
            
        vector_val = vector_map[p.product_id]
        
        # ë²¡í„° í…Œì´ë¸”ì— ì €ì¥ (Upsert ë¡œì§)
        existing_vec = db_session.query(ProductInferenceVectors).filter_by(id=p.product_id).first()
        if existing_vec:
            existing_vec.vector_embedding= vector_val
        else:
            new_vec = ProductInferenceVectors(id=p.product_id, vector_embedding=vector_val)
            db_session.add(new_vec)
        
        # [ì‘ì—… ì™„ë£Œ Flag ì²˜ë¦¬]
        p.is_vectorized = True
    
    db_session.commit()
    print("âœ… Saved Item Vectors (by encoder) successfully.")
    return len(vector_map)





# --- API 2. í•™ìŠµ ìš”ì²­ (Background Task) ---
@serving_controller_router.post("/train/start")
async def start_training(background_tasks: BackgroundTasks,
                         encoder_instance: CoarseToFineItemTower = Depends(get_global_encoder), 
                         projector_instance: OptimizedItemTower = Depends(get_global_projector),
                         g_batch_size: int = Depends(get_global_batch_size)):
    """
    [API 2] DBì— ìˆëŠ” ë°ì´í„°ë¡œ SimCSE í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. (ë¹„ë™ê¸° ì‹¤í–‰)
    """
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ë„˜ê¹€ (APIëŠ” ì¦‰ì‹œ ì‘ë‹µ)
    background_tasks.add_task(train_simcse_from_db,
        encoder=encoder_instance,
        projector=projector_instance,
        batch_size = g_batch_size
    )
    
    return {"message": "Training started in the background.", "status": "processing"}


# batch size ë§ì¶°ì•¼í•¨
@serving_controller_router.post("/vectors/process-pending")
def process_pending_vectors(
    batch_size: int = Depends(get_global_batch_size),
    db: Session = Depends(get_db),
    # [ìˆ˜ì •] ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì£¼ì…
    encoder: CoarseToFineItemTower = Depends(get_global_encoder)

):
    total_processed_count = 0
    
    while True:
        # 1. ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë°ì´í„° ì¡°íšŒ (batch_size? ì¼ë‹¨)
        pending_products = db.query(ProductInferenceInput)\
                             .filter(ProductInferenceInput.is_vectorized == False)\
                             .limit(batch_size)\
                             .all()
        
        if not pending_products:
            break

        # 2. ê³µí†µ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        current_count = run_pipeline_and_save(db, pending_products, encoder)
        
        total_processed_count += current_count

    if total_processed_count == 0:
        return {"status": "success", "message": "No pending products to process."}

    return {
        "status": "success", 
        "processed_count": total_processed_count, 
        "message": f"All pending batches processed successfully. (Total: {total_processed_count})"
    }
    
# ------------------------------------------------------------------
# API 3. íŠ¹ì • ID ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë²¡í„°í™” (On-Demand Processing)
# ------------------------------------------------------------------
@serving_controller_router.post("/vectors/process-by-ids")
def process_vectors_by_ids(
    payload: ProductIdListSchema, 
    db: Session = Depends(get_db),
    # [ìˆ˜ì •] ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì£¼ì…
    encoder: CoarseToFineItemTower = Depends(get_global_encoder)
    #projector: OptimizedItemTower = Depends(get_global_projector)
):
    # 1. ID ì¡°íšŒ
    target_products = db.query(ProductInferenceInput)\
                        .filter(ProductInferenceInput.product_id.in_(payload.product_ids))\
                        .all()
    
    if not target_products:
        raise HTTPException(status_code=404, detail="No products found for given IDs.")

    # 2. ê³µí†µ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ëª¨ë¸ ì „ë‹¬)
    processed_count = run_pipeline_and_save(db, target_products, encoder)
    
    return {
        "status": "success", 
        "processed_count": processed_count, 
        "message": "On-demand processing completed."
    }




# ------------------------------------------------------------------
# API 4. User Tower Train
# ------------------------------------------------------------------

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)



class TrainRequest(BaseModel):
    epochs: int = 5
    batch_size: int = 1
    learning_rate: float = 1e-4
    max_seq_len: int = 50
    
    # ëª¨ë¸ ì €ì¥ ê²½ë¡œ (ì˜µì…˜)
    save_path: str = "./user_tower_latest.pth"

class TrainResponse(BaseModel):
    status: str
    final_avg_loss: float
    trained_epochs: int
    model_save_path: str
    message: str

# ==========================================
# 2. Service Layer (Pipeline Logic)
# ==========================================
def preprocess_db_sessions(sessions: list, product_id_map: dict, max_seq_len: int) -> List[dict]:
    """DB ì„¸ì…˜ ë°ì´í„°ë¥¼ í•™ìŠµìš© ë°ì´í„°(Dict)ë¡œ ë³€í™˜"""
    data = []
    
    # ë§¤í•‘ í…Œì´ë¸”
    GENDER_MAP = {'M': 1, 'F': 2, 'UNKNOWN': 0}
    SEASON_MAP = {'SPRING_AUTUMN': 1, 'SUMMER': 2, 'WINTER': 3, 'UNKNOWN': 0}
    
    for sess in sessions:
        if not sess.events: continue
        
        # 1. Sort Events
        events = sorted(sess.events, key=lambda e: e.timestamp)
        if len(events) < 2: continue # ìµœì†Œ 2ê°œ (History 1 + Target 1)
        
        # 2. Map Product IDs
        seq_indices = [product_id_map.get(e.product_id, 0) for e in events]
        
        # 3. Create Row
        # ë§ˆì§€ë§‰ ì•„ì´í…œ = Target, ê·¸ ì´ì „ = History
        data.append({
            'history': seq_indices[:-1], 
            'target_idx': seq_indices[-1],
            'gender': GENDER_MAP.get(sess.user.gender, 0),
            'season': SEASON_MAP.get(sess.season, 0),
            'age': 0 # í•„ìš”ì‹œ ì¶”ê°€ êµ¬í˜„
        })
        
    return data

def run_training_pipeline(db: Session, config: TrainRequest) -> TrainResponse:
    logger.info("ğŸš€ Starting Training Pipeline via API...")

    # 1. Load Pretrained Item Vectors (Fixed)
    # item_matrix: (Num_Total_Items + 1, 128)
    item_matrix, id_map = load_pretrained_vectors_from_db(db)
    logger.info(f"âœ… Loaded {len(id_map)} item vectors from DB.")

    # 2. Fetch User Sessions (Training Data)
    # ì‹¤ì œë¡œëŠ” ê¸°ê°„ ì¿¼ë¦¬ ë“±ì„ ì¶”ê°€í•´ì•¼ í•¨
    sessions = db.query(UserSession).join(UserSession.user).join(UserSession.events).limit(5000).all()
    
    if not sessions:
        raise HTTPException(status_code=400, detail="No session data found in DB.")

    # 3. Preprocessing
    training_data = preprocess_db_sessions(sessions, id_map, config.max_seq_len)
    logger.info(f"âœ… Prepared {len(training_data)} training samples.")
    
    if len(training_data) == 0:
        raise HTTPException(status_code=400, detail="Valid training data is empty after preprocessing.")

    # 4. DataLoader Setup
    dataset = UserTowerTrainDataset(training_data, id_map, max_seq_len=config.max_seq_len)
    train_loader = torch.utils.data.DataLoader(
        dataset, batch_size=config.batch_size, shuffle=True
    )

    # 5. Initialize Final Use
    #r Tower
    num_total_items = item_matrix.size(0)
    user_tower = FinalUserTower(
        num_total_products=num_total_items - 1,
        pretrained_item_matrix=item_matrix, # Weight ì´ˆê¸°í™”ìš©
        max_seq_len=config.max_seq_len
    )

    # 6. Execute Training Loop
    # train_final_user_tower í•¨ìˆ˜ê°€ í•™ìŠµëœ ëª¨ë¸ì„ ë°˜í™˜í•œë‹¤ê³  ê°€ì •
    trained_model = train_final_user_tower(
        user_tower=user_tower,
        pretrained_item_matrix=item_matrix, # Loss ê³„ì‚°ìš©
        train_loader=train_loader,
        epochs=config.epochs,
        lr=config.learning_rate
    )

    # 7. Save Model
    torch.save(trained_model.state_dict(), config.save_path)
    logger.info(f"ğŸ’¾ Model saved to {config.save_path}")

    return TrainResponse(
        status="success",
        final_avg_loss=0.0, # Loopì—ì„œ ë§ˆì§€ë§‰ Lossë¥¼ ë¦¬í„´ë°›ë„ë¡ ìˆ˜ì • í•„ìš” (ì—¬ê¸°ì„  Dummy)
        trained_epochs=config.epochs,
        model_save_path=config.save_path,
        message=f"Training completed with {len(training_data)} samples."
    )

# ==========================================
# 3. API Endpoints
# ==========================================
@serving_controller_router.post("/train/user-tower", response_model=TrainResponse)
def trigger_training_job(req: TrainRequest, db: Session = Depends(get_db)):
    """
    ìœ ì € íƒ€ì›Œ í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (Synchronous for Demo)
    ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” BackgroundTasks ë˜ëŠ” Celeryë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.
    """
    try:
        result = run_training_pipeline(db, req)
        return result
    except Exception as e:
        logger.error(f"Training Failed: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))










'''


@serving_controller_router.post("/train/user-tower/start")
async def start_user_tower_training(
    background_tasks: BackgroundTasks,
    epochs: int = 5,
    batch_size: int = 4,
    lr: float = 1e-4,
    db: Session = Depends(get_db)
):
    """
    [User Tower Training API]
    1. DBì—ì„œ í•™ìŠµëœ Item Vectorë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.
    2. ìœ ì € ë¡œê·¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ User Towerë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. (ë°±ê·¸ë¼ìš´ë“œ)
    """
    
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ë“±ë¡
    # ì£¼ì˜: db ì„¸ì…˜ì€ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ê°€ ëë‚  ë•Œê¹Œì§€ ì‚´ì•„ìˆì–´ì•¼ í•˜ê±°ë‚˜,
    # íƒœìŠ¤í¬ ë‚´ë¶€ì—ì„œ ìƒˆë¡œ ìƒì„±í•˜ëŠ” ê²ƒì´ ì•ˆì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì „ë‹¬í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” scoped_session ì‚¬ìš© ê¶Œì¥.
    background_tasks.add_task(
        train_user_tower_task,
        db_session=db,
        epochs=epochs,
        batch_size=batch_size,
        lr=lr
    )
    
    return {
        "status": "success",
        "message": "User Tower training started in background.",
        "config": {
            "epochs": epochs,
            "batch_size": batch_size,
            "lr": lr
        }
    }


@serving_controller_router.get("/recommend/{user_id}")
def recommend_products_to_user(
    user_id: int, 
    top_k: int = 5,
    db: Session = Depends(get_db),
    rec_service: RecommendationService = Depends(get_global_rec_service)
):
    """
    [User-to-Item ì¶”ì²œ]
    1. ìœ ì €ì˜ í˜„ì¬ ìƒíƒœ(ì´ë ¥)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ User Vector ì¶”ë¡ 
    2. DBì—ì„œ ìœ ì‚¬í•œ ìƒí’ˆ ê²€ìƒ‰
    """

    if rec_service is None:
        raise HTTPException(status_code=503, detail="Recommendation Service not ready")
    
    try:
        # 1. User Vector ì¶”ë¡ 
        user_vector = rec_service.get_user_vector(db, user_id)
        
        # 2. ìœ ì‚¬ ìƒí’ˆ ê²€ìƒ‰ (Retrieval)
        candidates = rec_service.retrieve_similar_items(db, user_vector, top_k=top_k)
        
        # 3. ê²°ê³¼ í¬ë§·íŒ…
        response = []
        for pid, category, dist in candidates:
            response.append({
                "product_id": pid,
                "category": category,
                "score": 1 - dist # Cosine DistanceëŠ” 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìœ¼ë¯€ë¡œ Scoreë¡œ ë³€í™˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
            })
            
        return {
            "user_id": user_id,
            "recommendations": response
        }
        
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))




@serving_controller_router.get("/recommend/ranker/{user_id}")
def recommend_products_to_user_ranker(
    user_id: int, 
    top_k: int = 5, # ìµœì¢…ì ìœ¼ë¡œ ë³´ì—¬ì¤„ ê°œìˆ˜ (ì˜ˆ: 10ê°œ)
    db: Session = Depends(get_db),
    rec_service: RecommendationService = Depends(get_global_rec_service)
):
    """
    [2-Stage Recommendation Pipeline]
    Stage 1. Retrieval: User Vectorì™€ ìœ ì‚¬í•œ í›„ë³´êµ°ì„ ë„‰ë„‰í•˜ê²Œ ê²€ìƒ‰ (Top-K * 5)
    Stage 2. Ranking: DCN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í›„ë³´êµ°ì„ ì •ë°€ ì¬ì •ë ¬
    """

    if rec_service is None:
        raise HTTPException(status_code=503, detail="Recommendation Service not ready")
    
    try:
        # ==========================================
        # [Stage 1] Retrieval (Candidate Generation)
        # ==========================================
        
        # 1. User Vector ì¶”ë¡  (ê¸°ì¡´ ë¡œì§)
        user_vector_np = rec_service.get_user_vector(db, user_id)
        
        # 2. í›„ë³´êµ° ê²€ìƒ‰ (Retrieval)
        # ë­í‚¹ ëª¨ë¸ì´ ì¬ì •ë ¬í•  ì—¬ì§€ë¥¼ ì£¼ê¸° ìœ„í•´, ìš”ì²­ëœ top_kë³´ë‹¤ ë” ë§ì´(ì˜ˆ: 5ë°°) ê²€ìƒ‰í•©ë‹ˆë‹¤.
        candidate_k = top_k * 2
        # candidates êµ¬ì¡°: [(pid, category, dist), ...]
        candidates = rec_service.retrieve_similar_items(db, user_vector_np, top_k=candidate_k)
        
        if not candidates:
            return {"user_id": user_id, "recommendations": []}

        # ==========================================
        # [Stage 2] Ranking (Re-ranking)
        # ==========================================
        
        # 3. ë­í‚¹ ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        candidate_pids = [c[0] for c in candidates]
        
        # 3-1. í›„ë³´ ì•„ì´í…œë“¤ì˜ ë²¡í„° ì¡°íšŒ (DB Query)
        # {pid: vector_list} í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ ê°€ì •
        item_vector_map = rec_service.get_item_vectors_by_ids(db, candidate_pids)
        
        # 3-2. Tensor ë³€í™˜ ì¤€ë¹„
        valid_candidates = [] # ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ìœ íš¨í•œ í›„ë³´ë§Œ í•„í„°ë§
        item_vectors_list = []
        
        for pid, category, dist in candidates:
            if pid in item_vector_map:
                valid_candidates.append({
                    "product_id": pid,
                    "category": category,
                    "base_score": 1 - dist # Retrieval ì ìˆ˜ (ì°¸ê³ ìš©)
                })
                item_vectors_list.append(item_vector_map[pid])
        
        if not valid_candidates:
             raise HTTPException(status_code=404, detail="Candidate vectors not found")

        # Tensor ë³€í™˜
        user_tensor = torch.tensor(user_vector_np, dtype=torch.float32).to(DEVICE) # (128,)
        item_tensor = torch.tensor(item_vectors_list, dtype=torch.float32).to(DEVICE) # (N, 128)
        
        # Context Vector (ì„ íƒ ì‚¬í•­)
        # ë§Œì•½ ì‹œê°„ëŒ€, ìš”ì¼ ë“±ì˜ ì»¨í…ìŠ¤íŠ¸ í”¼ì²˜ë¥¼ ì“´ë‹¤ë©´ ì—¬ê¸°ì„œ ìƒì„±
        # í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  ê°€ì • (None ì „ë‹¬) ë˜ëŠ” 0 ë²¡í„°
        context_tensor = None 
        # context_tensor = torch.zeros(20, dtype=torch.float32).to(DEVICE) 

        # 4. ë­í‚¹ ëª¨ë¸ ì˜ˆì¸¡ (Inference)
        # rec_service ë‚´ë¶€ì— ë¡œë“œëœ ranking_model ì‚¬ìš©
        # predict_for_userëŠ” (N,) í˜•íƒœì˜ í™•ë¥ ê°’(Score)ì„ ë°˜í™˜
        ranking_scores = rec_service.ranking_model.predict_for_user(
            user_vec=user_tensor,
            item_vecs=item_tensor,
            context_vec=context_tensor
        )
        
        # 5. ì ìˆ˜ í• ë‹¹ ë° ì •ë ¬
        # Tensorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        scores_list = ranking_scores.tolist()
        
        for i, candidate in enumerate(valid_candidates):
            candidate["ranking_score"] = scores_list[i]
            
        # ë­í‚¹ ì ìˆ˜(ranking_score) ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        valid_candidates.sort(key=lambda x: x["ranking_score"], reverse=True)
        
        # 6. ìµœì¢… Top-K ìë¥´ê¸°
        final_recommendations = valid_candidates[:top_k]
        
        return {
            "user_id": user_id,
            "count": len(final_recommendations),
            "recommendations": final_recommendations
        }
        
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        # ë¡œê·¸ ê¸°ë¡ í•„ìš”
        print(f"Error in recommendation: {e}")
        raise HTTPException(status_code=500, detail="Internal Server Error")



# productList -> (CoarseToFineItemTower)ë¥¼ I : Nê°œë¡œ í™•ì¥?
# I : productList(featureForm)


# ê°€ìƒì˜ ProductInput íƒ€ì…ê³¼ vocab ê°ì²´ (ê¸°ì¡´ ì½”ë“œ ë¬¸ë§¥ ë”°ë¦„)
# DEVICE, vocab ë“±ì€ ì „ì—­ ë³€ìˆ˜ í˜¹ì€ ì¸ìë¡œ ê´€ë¦¬ëœë‹¤ê³  ê°€ì •



## Batch API Layer

class EmbeddingRequestItem(BaseModel):

    product_id: int
    feature_data: Dict[str, Any]

    class Config:
        # Pydanticì—ê²Œ SQLAlchemy ê°ì²´ë¡œë¶€í„° ì†ì„±ì„ ì½ì–´ì˜¤ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤. (í•µì‹¬)
        from_attributes = True

@serving_controller_router.post("/update-vectors")
def process_and_save_vectors(
    products: List[EmbeddingRequestItem], 
    db: Session = Depends(get_db),
    batch_size: int = 32
):
    

    #1. ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ì˜ë¼ ëª¨ë¸ì— í†µê³¼ì‹œí‚´
    #2. ê²°ê³¼ ë²¡í„°ì™€ ì›ë³¸ ìƒí’ˆì˜ ID, Categoryë¥¼ ë§¤í•‘
    #3. DBì— ì¦‰ì‹œ ì €ì¥ (Upsert)

    CoarseToFineItemTower.eval()
    total_count = len(products)
    print(f"ì´ {total_count}ê°œì˜ ìƒí’ˆ ë²¡í„° ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    with torch.no_grad():
        # 1. ë°°ì¹˜ ë‹¨ìœ„ ë£¨í”„
        for i in range(0, total_count, batch_size):
            # -------------------------------------------------------
            # A. ë°ì´í„° ì¤€ë¹„ & ëª¨ë¸ Inference
            # -------------------------------------------------------
            batch_products = products[i : i + batch_size]
            
            # (ì´ì „ ë‹¨ê³„ì—ì„œ ë§Œë“  ì „ì²˜ë¦¬ í•¨ìˆ˜)
            t_std, t_re = preprocess_batch_input(batch_products)
            
            # ëª¨ë¸ ì‹¤í–‰ -> (Batch_Size, 128)
            batch_output = CoarseToFineItemTower(t_std, t_re)
            
            # CPUë¡œ ì´ë™ ë° Numpy ë³€í™˜
            batch_vectors = batch_output.cpu().numpy()

            # -------------------------------------------------------
            # B. ID ë§¤í•‘ ë° DB ê°ì²´ ìƒì„±
            # -------------------------------------------------------
            # batch_products[k] ì™€ batch_vectors[k] ëŠ” ì„œë¡œ ê°™ì€ ìƒí’ˆì…ë‹ˆë‹¤.
            
            # DB ì‘ì—…ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ìƒì„± (Bulk Insertìš©)
            insert_data_list = []
            
            for product, vector in zip(batch_products, batch_vectors):
                insert_data_list.append({
                    "id": product.product_id,                  # PK
                    "category": product.feature_data.clothes.category,      # ë©”íƒ€ë°ì´í„° ìˆ˜ì •í•„ìš”!!!!!!!!!!!!
                    "vector_pre": vector.tolist(),     # numpy array -> list[float]
                    # "vector_triplet": None           # í•„ìš”í•˜ë‹¤ë©´ null ì²˜ë¦¬ or ìƒëµ
                })

            # -------------------------------------------------------
            # C. DB ì €ì¥ (Upsert ì²˜ë¦¬)
            # -------------------------------------------------------
            if insert_data_list:
                # PostgreSQLì˜ INSERT ... ON CONFLICT DO UPDATE êµ¬ë¬¸ ì‚¬ìš©
                stmt = insert(Vectors).values(insert_data_list)
                
                # PK(id)ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´, vector_preì™€ categoryë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
                upsert_stmt = stmt.on_conflict_do_update(
                    index_elements=['id'],  # ì¶©ëŒ ê¸°ì¤€ ì»¬ëŸ¼ (PK)
                    set_={
                        "vector_pre": stmt.excluded.vector_pre,
                        "category": stmt.excluded.category
                    }
                )
                
                db.execute(upsert_stmt)
                db.commit() # ë°°ì¹˜ ë‹¨ìœ„ ì»¤ë°‹ (ë©”ëª¨ë¦¬ ì ˆì•½ ë° íŠ¸ëœì­ì…˜ ê´€ë¦¬)
                
            print(f"Processing... {min(i + batch_size, total_count)} / {total_count}")

    print("ëª¨ë“  ë²¡í„° ì €ì¥ ì™„ë£Œ.")
    
'''