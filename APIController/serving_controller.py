
import os
from fastapi import BackgroundTasks, FastAPI, Depends, HTTPException, APIRouter
from pydantic import BaseModel
import torch.nn.functional as F
import torch
from torch.nn.utils.rnn import pad_sequence
from typing import Any, Dict, List, Tuple
from database import ProductInferenceInput, ProductInferenceVectors, ProductInput, Vectors, get_db
from train import train_simcse_from_db, train_user_tower_task
from utils.dependencies import get_global_batch_size, get_global_encoder, get_global_projector
import utils.vocab as vocab 
import numpy as np
from model import ALL_FIELD_KEYS, CoarseToFineItemTower, OptimizedItemTower, SimCSEModelWrapper
from sqlalchemy.orm import Session
from sqlalchemy.dialects.postgresql import insert  
import torch.nn as nn

serving_controller_router = APIRouter()
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
MODEL_DIR = "models"

# API 3 ì…ë ¥ìš©
class ProductIdListSchema(BaseModel):
    product_ids: List[int]


import torch
from typing import List, Tuple, Dict, Any

# ì „ì—­ ë³€ìˆ˜ ALL_FIELD_KEYSê°€ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
# ì˜ˆ: ALL_FIELD_KEYS = ["category", "season", "color", ...] 

def preprocess_batch_input(products: List[ProductInput]) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    [Residual Field Embeddingìš© ì „ì²˜ë¦¬]
    ë”•ì…”ë„ˆë¦¬ë¥¼ ìˆœíšŒí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê³ ì •ëœ 'ALL_FIELD_KEYS'ë¥¼ ìˆœíšŒí•˜ì—¬
    Tensorì˜ ê° ì¸ë±ìŠ¤ê°€ í•­ìƒ íŠ¹ì • í•„ë“œ(ì†ì„±)ë¥¼ ê°€ë¦¬í‚¤ë„ë¡ ì •ë ¬í•©ë‹ˆë‹¤.
    """
    batch_std_ids = []
    batch_re_ids = []
    
    for product in products:
        # 1. ë°ì´í„° ì¶”ì¶œ
        feature_data: Dict[str, Any] = getattr(product, 'feature_data', {})
        clothes_data = feature_data.get("clothes", {})
        re_data = feature_data.get("reinforced_feature_value", {})
        
        row_std_ids = []
        row_re_ids = []

        # 2. [í•µì‹¬] ê³ ì •ëœ Key ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒ (ìˆœì„œ ë° ìœ„ì¹˜ ë³´ì¥)
        for key in ALL_FIELD_KEYS:
            
            # --- A. STD ID ì¶”ì¶œ ---
            std_val = clothes_data.get(key)
            
            # ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš© (ë‹¨ì¼ ë¼ë²¨ ê°€ì •)
            if isinstance(std_val, list) and len(std_val) > 0:
                std_val = std_val[0]
            elif isinstance(std_val, list) and len(std_val) == 0:
                std_val = None
                
            # vocab.pyì˜ í•¨ìˆ˜ í˜¸ì¶œ (Key ì •ë³´ë„ í•¨ê»˜ ì „ë‹¬í•˜ì—¬ í™•ì¥ì„± í™•ë³´)
            # ê°’ì´ ì—†ìœ¼ë©´(None) ë‚´ë¶€ì—ì„œ 0(PAD) ë°˜í™˜
            s_id = vocab.get_std_id(key, std_val)
            row_std_ids.append(s_id)
            
            
            # --- B. RE ID ì¶”ì¶œ (Hashing) ---
            re_val_list = re_data.get(key)
            re_val = None
            
            # RE ë°ì´í„°ëŠ” ë³´í†µ List í˜•íƒœì´ë¯€ë¡œ ì²« ë²ˆì§¸ ê°’ ì¶”ì¶œ
            if re_val_list and isinstance(re_val_list, list) and len(re_val_list) > 0:
                re_val = re_val_list[0]
            elif isinstance(re_val_list, str):
                re_val = re_val_list
            
            # Hashing í•¨ìˆ˜ í˜¸ì¶œ (ì €ì¥ X, ì¦‰ì‹œ ë³€í™˜)
            # ê°’ì´ ì—†ìœ¼ë©´(None) ë‚´ë¶€ì—ì„œ 0(PAD) ë°˜í™˜
            r_id = vocab.get_re_hash_id(re_val)
            row_re_ids.append(r_id)

        # 3. í–‰ ë‹¨ìœ„ ì¶”ê°€
        # ì´ì œ row_std_idsì˜ ê¸¸ì´ëŠ” í•­ìƒ len(ALL_FIELD_KEYS)ë¡œ ê³ ì •ë¨
        batch_std_ids.append(row_std_ids)
        batch_re_ids.append(row_re_ids)
    
    # 4. í…ì„œ ë³€í™˜ (pad_sequence ë¶ˆí•„ìš” -> torch.tensorë¡œ ì§ë³€í™˜)
    # Shape: (Batch_Size, Num_Fields)
    t_std_batch = torch.tensor(batch_std_ids, dtype=torch.long, device=DEVICE)
    t_re_batch = torch.tensor(batch_re_ids, dtype=torch.long, device=DEVICE)

    return t_std_batch, t_re_batch



def generate_item_vectors(
    products: List[ProductInput], 
    encoder: nn.Module 
    
) -> Dict[int, List[float]]:
    """
    [Core Inference Logic]
    ProductInput ë¦¬ìŠ¤íŠ¸ -> Encoder(Stage1) -> L2 Normalize -> {product_id: vector} ë°˜í™˜
    """
    if not products:
        return {}

    # 1. ëª¨ë¸ Wrapper ì„¤ì • ë° Eval ëª¨ë“œ
    model = encoder.to(DEVICE)
    model.eval()

    # 2. ì „ì²˜ë¦¬ (collate_fn ë¡œì§ í¬í•¨ëœ í•¨ìˆ˜ ì‚¬ìš© ê°€ì •)
    try:
        t_std, t_re = preprocess_batch_input(products)
    except Exception as e:
        print(f"âŒ Preprocessing Error: {e}")
        return {}

    t_std = t_std.to(DEVICE)
    t_re = t_re.to(DEVICE)

    # 3. ì¶”ë¡  (No Grad)
    with torch.no_grad():
        raw_v = model(t_std, t_re)
        final_vectors_tensor = F.normalize(raw_v, p=2, dim=1)
    # 4. ê²°ê³¼ ë³€í™˜
    vectors_list = final_vectors_tensor.cpu().numpy().tolist()
    
    result_map = {}
    for idx, product in enumerate(products):
        result_map[product.product_id] = vectors_list[idx]

    return result_map




def run_pipeline_and_save(
    db_session: Session, 
    products: List[ProductInferenceInput],
    encoder: nn.Module     
    
):
    """
    [ê³µí†µ ë¡œì§] 
    DB ê°ì²´ ë¦¬ìŠ¤íŠ¸ -> Pydantic ë³€í™˜ -> ì¶”ë¡  -> ë²¡í„° ì €ì¥ -> Flag ì—…ë°ì´íŠ¸
    """
    if not products:
        return 0

    # 1. DB ê°ì²´(ORM)ë¥¼ ëª¨ë¸ ì…ë ¥ìš© Pydantic ê°ì²´ë¡œ ë³€í™˜
    input_list = [
        ProductInput(product_id=p.product_id, feature_data=p.feature_data)
        for p in products
    ]


    # 1-1. load

    encoder_path = os.path.join(MODEL_DIR, "encoder_stage1.pth")
    #projector_path = os.path.join(MODEL_DIR, "projector_stage2.pth")

    if os.path.exists(encoder_path): #and os.path.exists(projector_path):
        try:
            encoder_state = torch.load(encoder_path, map_location=DEVICE)
            #projector_state = torch.load(projector_path, map_location=DEVICE)
            
            encoder.load_state_dict(encoder_state)
            #projector.load_state_dict(projector_state)
            
            print("âœ… Models loaded successfully.")
        except Exception as e:
            print(f"âŒ Error loading state dicts: {e}")
            raise e
    else:
        raise FileNotFoundError(f"âŒ Model files not found in {MODEL_DIR}")


    # 2. ì‹¤ì œ ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰ (generate_item_vectors í˜¸ì¶œ)
    #    ê²°ê³¼ëŠ” {product_id: [0.12, 0.55, ...]} í˜•íƒœ
    try:
        vector_map = generate_item_vectors(input_list, encoder)
    except Exception as e:
        print(f"âŒ Inference Failed: {e}")
        raise e

    # 3. ê²°ê³¼ ì €ì¥ ë° í”Œë˜ê·¸ ì—…ë°ì´íŠ¸
    for p in products:
        # í˜¹ì‹œ ëª¨ë¥¼ ì—ëŸ¬ë¡œ íŠ¹ì • IDê°€ ëˆ„ë½ëëŠ”ì§€ í™•ì¸
        if p.product_id not in vector_map:
            continue
            
        vector_val = vector_map[p.product_id]
        
        # ë²¡í„° í…Œì´ë¸”ì— ì €ì¥ (Upsert ë¡œì§)
        existing_vec = db_session.query(ProductInferenceVectors).filter_by(id=p.product_id).first()
        if existing_vec:
            existing_vec.vector_embedding= vector_val
        else:
            new_vec = ProductInferenceVectors(id=p.product_id, vector_embedding=vector_val)
            db_session.add(new_vec)
        
        # [ì‘ì—… ì™„ë£Œ Flag ì²˜ë¦¬]
        p.is_vectorized = True
    
    db_session.commit()
    print("âœ… Saved Item Vectors (by encoder) successfully.")
    return len(vector_map)





# --- API 2. í•™ìŠµ ìš”ì²­ (Background Task) ---
@serving_controller_router.post("/train/start")
async def start_training(background_tasks: BackgroundTasks,
                         encoder_instance: CoarseToFineItemTower = Depends(get_global_encoder), 
                         projector_instance: OptimizedItemTower = Depends(get_global_projector),
                         g_batch_size: int = Depends(get_global_batch_size)):
    """
    [API 2] DBì— ìˆëŠ” ë°ì´í„°ë¡œ SimCSE í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. (ë¹„ë™ê¸° ì‹¤í–‰)
    """
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ë„˜ê¹€ (APIëŠ” ì¦‰ì‹œ ì‘ë‹µ)
    background_tasks.add_task(train_simcse_from_db,
        encoder=encoder_instance,
        projector=projector_instance,
        batch_size = g_batch_size
    )
    
    return {"message": "Training started in the background.", "status": "processing"}


# batch size ë§ì¶°ì•¼í•¨
@serving_controller_router.post("/vectors/process-pending")
def process_pending_vectors(
    batch_size: int = Depends(get_global_batch_size),
    db: Session = Depends(get_db),
    # [ìˆ˜ì •] ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì£¼ì…
    encoder: CoarseToFineItemTower = Depends(get_global_encoder),
    projector: OptimizedItemTower = Depends(get_global_projector)
):
    # 1. ì²˜ë¦¬ë˜ì§€ ì•Šì€ ë°ì´í„° ì¡°íšŒ
    pending_products = db.query(ProductInferenceInput)\
                         .filter(ProductInferenceInput.is_vectorized == False)\
                         .limit(batch_size)\
                         .all()
    
    if not pending_products:
        return {"status": "success", "message": "No pending products to process."}

    # 2. ê³µí†µ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ëª¨ë¸ ì „ë‹¬)
    processed_count = run_pipeline_and_save(db, pending_products, encoder)
    
    return {
        "status": "success", 
        "processed_count": processed_count, 
        "message": "Batch processing completed."
    }


# ------------------------------------------------------------------
# API 3. íŠ¹ì • ID ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë²¡í„°í™” (On-Demand Processing)
# ------------------------------------------------------------------
@serving_controller_router.post("/vectors/process-by-ids")
def process_vectors_by_ids(
    payload: ProductIdListSchema, 
    db: Session = Depends(get_db),
    # [ìˆ˜ì •] ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì£¼ì…
    encoder: CoarseToFineItemTower = Depends(get_global_encoder)
    #projector: OptimizedItemTower = Depends(get_global_projector)
):
    # 1. ID ì¡°íšŒ
    target_products = db.query(ProductInferenceInput)\
                        .filter(ProductInferenceInput.product_id.in_(payload.product_ids))\
                        .all()
    
    if not target_products:
        raise HTTPException(status_code=404, detail="No products found for given IDs.")

    # 2. ê³µí†µ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ëª¨ë¸ ì „ë‹¬)
    processed_count = run_pipeline_and_save(db, target_products, encoder)
    
    return {
        "status": "success", 
        "processed_count": processed_count, 
        "message": "On-demand processing completed."
    }



# ------------------------------------------------------------------
# API 4. User Tower Train
# ------------------------------------------------------------------



def load_pretrained_vectors_from_db(db_session: Session) -> Tuple[torch.Tensor, Dict[int, int]]:
    """
    [Stage 0] ë°ì´í„° ì¤€ë¹„
    DBì˜ ProductInferenceVectors í…Œì´ë¸”ì—ì„œ (ID, Vector)ë¥¼ ë¡œë“œí•˜ì—¬
    ëª¨ë¸ ì´ˆê¸°í™”ìš© Matrixì™€ ID Mappingì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("â³ [DB Loader] Fetching product vectors from DB...")
    
    # 1. DB Query: IDì™€ Servingìš© ë²¡í„°(128d)ë§Œ ê°€ì ¸ì˜´
    results = db_session.query(
        ProductInferenceVectors.id, 
        ProductInferenceVectors.vector_serving
    ).filter(
        ProductInferenceVectors.vector_serving.isnot(None)
    ).all()
    
    if not results:
        raise ValueError("âŒ DBì— ì €ì¥ëœ ì•„ì´í…œ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤! Item Tower ì¶”ë¡ ì„ ë¨¼ì € ìˆ˜í–‰í•˜ì„¸ìš”.")

    # 2. ë©”íƒ€ë°ì´í„° ì„¤ì •
    num_products = len(results)
    vector_dim = 128  # Item Tower Output Dimension
    
    # 0ë²ˆ ì¸ë±ìŠ¤ëŠ” Paddingì„ ìœ„í•´ ë¹„ì›Œë‘  (Index 1ë¶€í„° ì‹œì‘)
    # Shape: (ì „ì²´ìƒí’ˆìˆ˜ + 1, 128)
    embedding_matrix = torch.zeros((num_products + 1, vector_dim), dtype=torch.float32)
    
    id_map = {} # Real DB ID -> Model Index (0, 1, 2...)
    
    # 3. ë§¤íŠ¸ë¦­ìŠ¤ ì±„ìš°ê¸°
    print(f"ğŸ“¦ [DB Loader] Processing {num_products} items...")
    
    for idx, (real_id, vector_list) in enumerate(results, start=1):
        # vector_listê°€ ë¬¸ìì—´ì´ë‚˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë³€í™˜ ì²˜ë¦¬ í•„ìš”í•  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” List[float]ë¼ê³  ê°€ì •
        
        # ID ë§¤í•‘ (DB ID 1050 -> Model Index 1)
        id_map[real_id] = idx 
        
        # í…ì„œ í• ë‹¹
        embedding_matrix[idx] = torch.tensor(vector_list, dtype=torch.float32)
        
    print(f"âœ… [DB Loader] Matrix Created. Shape: {embedding_matrix.shape}")
    
    return embedding_matrix, id_map




@serving_controller_router.post("/train/user-tower/start")
async def start_user_tower_training(
    background_tasks: BackgroundTasks,
    epochs: int = 10,
    batch_size: int = 512,
    lr: float = 1e-4,
    db: Session = Depends(get_db)
):
    """
    [User Tower Training API]
    1. DBì—ì„œ í•™ìŠµëœ Item Vectorë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.
    2. ìœ ì € ë¡œê·¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ User Towerë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. (ë°±ê·¸ë¼ìš´ë“œ)
    """
    
    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ë“±ë¡
    # ì£¼ì˜: db ì„¸ì…˜ì€ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ê°€ ëë‚  ë•Œê¹Œì§€ ì‚´ì•„ìˆì–´ì•¼ í•˜ê±°ë‚˜,
    # íƒœìŠ¤í¬ ë‚´ë¶€ì—ì„œ ìƒˆë¡œ ìƒì„±í•˜ëŠ” ê²ƒì´ ì•ˆì „í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì „ë‹¬í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” scoped_session ì‚¬ìš© ê¶Œì¥.
    background_tasks.add_task(
        train_user_tower_task,
        db_session=db,
        epochs=epochs,
        batch_size=batch_size,
        lr=lr
    )
    
    return {
        "status": "success",
        "message": "User Tower training started in background.",
        "config": {
            "epochs": epochs,
            "batch_size": batch_size,
            "lr": lr
        }
    }












# productList -> (CoarseToFineItemTower)ë¥¼ I : Nê°œë¡œ í™•ì¥?
# I : productList(featureForm)


# ê°€ìƒì˜ ProductInput íƒ€ì…ê³¼ vocab ê°ì²´ (ê¸°ì¡´ ì½”ë“œ ë¬¸ë§¥ ë”°ë¦„)
# DEVICE, vocab ë“±ì€ ì „ì—­ ë³€ìˆ˜ í˜¹ì€ ì¸ìë¡œ ê´€ë¦¬ëœë‹¤ê³  ê°€ì •



## Batch API Layer
'''
class EmbeddingRequestItem(BaseModel):

    product_id: int
    feature_data: Dict[str, Any]

    class Config:
        # Pydanticì—ê²Œ SQLAlchemy ê°ì²´ë¡œë¶€í„° ì†ì„±ì„ ì½ì–´ì˜¤ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤. (í•µì‹¬)
        from_attributes = True

@serving_controller_router.post("/update-vectors")
def process_and_save_vectors(
    products: List[EmbeddingRequestItem], 
    db: Session = Depends(get_db),
    batch_size: int = 32
):
    

    #1. ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ì˜ë¼ ëª¨ë¸ì— í†µê³¼ì‹œí‚´
    #2. ê²°ê³¼ ë²¡í„°ì™€ ì›ë³¸ ìƒí’ˆì˜ ID, Categoryë¥¼ ë§¤í•‘
    #3. DBì— ì¦‰ì‹œ ì €ì¥ (Upsert)

    CoarseToFineItemTower.eval()
    total_count = len(products)
    print(f"ì´ {total_count}ê°œì˜ ìƒí’ˆ ë²¡í„° ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    with torch.no_grad():
        # 1. ë°°ì¹˜ ë‹¨ìœ„ ë£¨í”„
        for i in range(0, total_count, batch_size):
            # -------------------------------------------------------
            # A. ë°ì´í„° ì¤€ë¹„ & ëª¨ë¸ Inference
            # -------------------------------------------------------
            batch_products = products[i : i + batch_size]
            
            # (ì´ì „ ë‹¨ê³„ì—ì„œ ë§Œë“  ì „ì²˜ë¦¬ í•¨ìˆ˜)
            t_std, t_re = preprocess_batch_input(batch_products)
            
            # ëª¨ë¸ ì‹¤í–‰ -> (Batch_Size, 128)
            batch_output = CoarseToFineItemTower(t_std, t_re)
            
            # CPUë¡œ ì´ë™ ë° Numpy ë³€í™˜
            batch_vectors = batch_output.cpu().numpy()

            # -------------------------------------------------------
            # B. ID ë§¤í•‘ ë° DB ê°ì²´ ìƒì„±
            # -------------------------------------------------------
            # batch_products[k] ì™€ batch_vectors[k] ëŠ” ì„œë¡œ ê°™ì€ ìƒí’ˆì…ë‹ˆë‹¤.
            
            # DB ì‘ì—…ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ ìƒì„± (Bulk Insertìš©)
            insert_data_list = []
            
            for product, vector in zip(batch_products, batch_vectors):
                insert_data_list.append({
                    "id": product.product_id,                  # PK
                    "category": product.feature_data.clothes.category,      # ë©”íƒ€ë°ì´í„° ìˆ˜ì •í•„ìš”!!!!!!!!!!!!
                    "vector_pre": vector.tolist(),     # numpy array -> list[float]
                    # "vector_triplet": None           # í•„ìš”í•˜ë‹¤ë©´ null ì²˜ë¦¬ or ìƒëµ
                })

            # -------------------------------------------------------
            # C. DB ì €ì¥ (Upsert ì²˜ë¦¬)
            # -------------------------------------------------------
            if insert_data_list:
                # PostgreSQLì˜ INSERT ... ON CONFLICT DO UPDATE êµ¬ë¬¸ ì‚¬ìš©
                stmt = insert(Vectors).values(insert_data_list)
                
                # PK(id)ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´, vector_preì™€ categoryë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.
                upsert_stmt = stmt.on_conflict_do_update(
                    index_elements=['id'],  # ì¶©ëŒ ê¸°ì¤€ ì»¬ëŸ¼ (PK)
                    set_={
                        "vector_pre": stmt.excluded.vector_pre,
                        "category": stmt.excluded.category
                    }
                )
                
                db.execute(upsert_stmt)
                db.commit() # ë°°ì¹˜ ë‹¨ìœ„ ì»¤ë°‹ (ë©”ëª¨ë¦¬ ì ˆì•½ ë° íŠ¸ëœì­ì…˜ ê´€ë¦¬)
                
            print(f"Processing... {min(i + batch_size, total_count)} / {total_count}")

    print("ëª¨ë“  ë²¡í„° ì €ì¥ ì™„ë£Œ.")
    
'''    