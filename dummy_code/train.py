'''
from typing import Any, Dict, List
from fastapi import APIRouter, Depends
from pydantic import BaseModel
from requests import Session
from sqlalchemy import select
import torch
from tqdm import tqdm
from transformers import get_linear_schedule_with_warmup
from database import ProductInferenceInput, SessionLocal, get_db
from utils.dependencies import get_global_batch_size, get_global_encoder, get_global_projector
from model import CoarseToFineItemTower, FinalUserTower, OptimizedItemTower, SimCSEModelWrapper, SimCSERecSysDataset
import torch.nn as nn
import torch.nn.functional as F
from pytorch_metric_learning import losses
from torch.utils.data import DataLoader, Dataset
import os
#from model import SymmetricUserTower 
import torch.optim as optim



DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
train_router = APIRouter()
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)



# ------------------------------------------------------
# Item Tower Training Task
# ------------------------------------------------------

class TrainingItem(BaseModel):

    product_id: int
    feature_data: Dict[str, Any]

def collate_simcse(batch):
    from APIController.serving_controller import preprocess_batch_input
    """(View1, View2) ë¦¬ìŠ¤íŠ¸ -> Tensor ë³€í™˜"""
    view1_list = [item[0] for item in batch]
    view2_list = [item[1] for item in batch]
    
    t_std1, t_re1 = preprocess_batch_input(view1_list)
    t_std2, t_re2 = preprocess_batch_input(view2_list)
    
    return t_std1, t_re1, t_std2, t_re2


## ë©”ëª¨ë¦¬ ìµœì í™”: db_session.query(Model).all() ëŒ€ì‹  select(...).mappings().all()ì„ ì‚¬ìš©í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”


def train_simcse_from_db(    
    encoder: nn.Module,       
    projector: nn.Module,
    batch_size: int = Depends(get_global_batch_size),
    epochs: int = 20,
    lr: float = 1e-4
):
    print("ğŸš€ Fetching data from DB...")
    
    # í˜¹ì‹œ ëª¨ë¥¼ taskbackgroundë–„ë¬¸ì— ì¼ë‹¨.
    db_session = SessionLocal()
    
    
    stmt = select(ProductInferenceInput.product_id, ProductInferenceInput.feature_data)
    result = db_session.execute(stmt).mappings().all()
    
    if not result:
        print("âŒ No data found.")
        return

    # [ìˆ˜ì • 2] Dictionary -> Pydantic ë³€í™˜
    products_list = []
    for row in result:
        # row['feature_data'] ì ‘ê·¼
        f_data = row['feature_data']
        p_input = TrainingItem(
            product_id=row['product_id'],
            feature_data=f_data
        )
        products_list.append(p_input)
        
    print(f"âœ… Loaded {len(products_list)} items.")
    
    # 3. ëª¨ë¸ ì„¤ì •
    model = SimCSEModelWrapper(encoder, projector).to(DEVICE)
    model.train() 
    
    # OptimizerëŠ” ë‘ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ í•™ìŠµí•´ì•¼ í•¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    

    
    # Loss Function (Contrastive Learning)
    loss_func = losses.NTXentLoss(temperature=0.07)
    
    dataset = SimCSERecSysDataset(products_list, dropout_prob=0.2)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True, 
        collate_fn=collate_simcse,
        drop_last=True
    )

    # [ì¶”ê°€] ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (Warmup 10%)
    total_steps = len(dataloader) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * 0.1),
        num_training_steps=total_steps
    )

    
    print("ğŸ”¥ Starting Training Loop...")
    
    # 5. Training Loop
    for epoch in range(epochs):
        total_loss = 0
        step = 0
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for t_std1, t_re1, t_std2, t_re2 in progress:

            t_std1, t_re1 = t_std1.to(DEVICE), t_re1.to(DEVICE)
            t_std2, t_re2 = t_std2.to(DEVICE), t_re2.to(DEVICE)
            
            optimizer.zero_grad()
            
            
            # Forward 
            emb1 = model(t_std1, t_re1)
            emb2 = model(t_std2, t_re2)
            
            # Contrastive Loss Calculation
            embeddings = torch.cat([emb1, emb2], dim=0)
            
            # Label generation
            # ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ 0~N ë¼ë²¨ì„ ë§Œë“¤ê³  ë‘ ë²ˆ ë°˜ë³µ
            batch_curr = emb1.size(0)
            labels = torch.arange(batch_curr).to(DEVICE)
            labels = torch.cat([labels, labels], dim=0)
            
            loss = loss_func(embeddings, labels)
            
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            step += 1
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
        if epochs % 10 == 0:
            print(f"Epoch {epoch+1} Avg Loss: {total_loss/step:.4f}")
        
    print("Training Finished.")
    

    
    print("ğŸ’¾ Saving models...")
    torch.save(encoder.state_dict(), os.path.join(MODEL_DIR, "encoder_stage1.pth"))
    torch.save(projector.state_dict(), os.path.join(MODEL_DIR, "projector_stage2.pth"))
    
    # torch.save(model.state_dict(), "final_simcse_model.pth")    



#DBì— ìˆëŠ” Item load -> positives(dropout) item ì¦ê°• -> collateê°€ì„œ í”¼ì³ í† í¬ë‚˜ì´ì € í•˜ê³  í…ì„œí™”
#ì´í›„ í…ì„œ ì•„ì´í…œíƒ€ì›Œê°€ì„œ trnsf-> std, re cross att í•˜ê³  ì§„í–‰
#ProductItem(DB) -> ItemTower(1ì°¨ì•„ì´í…œí…ì„œ) -> opt tensor í•™ìŠµ (1ì°¨í•™ìŠµ)  
@train_router.post("/run")
def test_line(

    encoder_instance: CoarseToFineItemTower = Depends(get_global_encoder), 
    projector_instance: OptimizedItemTower = Depends(get_global_projector)
):
    
    train_simcse_from_db(
        encoder=encoder_instance,
        projector=projector_instance
    )
    
    return {"message": "SimCSE training task initiated and completed."}

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from typing import List, Tuple
from tqdm import tqdm

# [ê°€ì •] ì•ì„œ ì •ì˜í•œ HybridGNNUserTower í´ë˜ìŠ¤ì™€ ë°ì´í„° ëª¨ë¸ì´ ìˆë‹¤ê³  ê°€ì •
# from user_model import HybridGNNUserTower
# from pytorch_metric_learning import losses  # í˜¹ì€ ê¸°ì¡´ losses ì‚¬ìš©

# ------------------------------------------------------
# 1. Dataset & Collate (User Views Preparation)
# ------------------------------------------------------

class UserTrainingItem(BaseModel):
    user_id: int
    history_ids: List[int]  # [101, 202, 505, ...]

class UserContrastiveDataset(Dataset):
    def __init__(self, users_list: List[UserTrainingItem], max_len=50):
        self.users = users_list
        self.max_len = max_len

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        user = self.users[idx]
        # View 1: User ID (for GNN)
        uid = user.user_id
        
        # View 2: History Sequence (for Transformer)
        seq = user.history_ids
        
        # Padding Logic (Simple version)
        seq = seq[-self.max_len:] # Truncate
        pad_len = self.max_len - len(seq)
        seq_padded = seq + [0] * pad_len # 0 is PAD ID
        
        return uid, torch.tensor(seq_padded, dtype=torch.long)

def collate_user_cl(batch):
    """
    UserCLì€ ì¦ê°•(Augmentation)ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    ëª¨ë¸ ìì²´ê°€ ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ View (GNN vs Seq)ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
    """
    user_ids = [item[0] for item in batch]
    history_seqs = [item[1] for item in batch]
    
    # Tensor Stack
    user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)
    history_tensor = torch.stack(history_seqs)
    
    return user_ids_tensor, history_tensor

# ------------------------------------------------------
# 2. User Tower Training Logic (Cross-View CL)
# ------------------------------------------------------

def train_user_tower_contrastive(
    user_tower: nn.Module,   # HybridGNNUserTower Instance
    adj_matrix: torch.Tensor, # Pre-computed Graph Adjacency Matrix
    batch_size: int = 256,
    epochs: int = 10,
    lr: float = 1e-4
):
    print("ğŸš€ [UserTower] Fetching User Data...")
    
    # DB Load Logic (Mock)
    # db_session = SessionLocal()
    # users_data = db_session.query(User).all() ...
    
    # ì˜ˆì‹œ ë°ì´í„° ìƒì„±
    train_users_list = [
        UserTrainingItem(user_id=i, history_ids=[1, 2, 3]) for i in range(1000)
    ]
    print(f"âœ… Loaded {len(train_users_list)} users.")

    # Model Setup
    model = user_tower.to(DEVICE)
    model.train()
    
    # Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    
    # Loss Function (NTXentLoss)
    # GNN ë²¡í„°ì™€ Seq ë²¡í„°ë¥¼ Positive Pairë¡œ ë´…ë‹ˆë‹¤.
    loss_func = losses.NTXentLoss(temperature=0.1) 

    dataset = UserContrastiveDataset(train_users_list)
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        collate_fn=collate_user_cl,
        drop_last=True
    )
    
    # Scheduler
    total_steps = len(dataloader) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps
    )

    print("ğŸ”¥ [UserTower] Starting Contrastive Training...")
    
    adj_matrix = adj_matrix.to(DEVICE) # GNNìš© ê·¸ë˜í”„ í–‰ë ¬

    for epoch in range(epochs):
        total_loss = 0
        step = 0
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        for user_ids, history_seqs in progress:
            
            user_ids = user_ids.to(DEVICE)
            history_seqs = history_seqs.to(DEVICE)
            
            optimizer.zero_grad()
            
            # --- [Core Logic: Get Two Views] ---
            
            # View 1: GNN Representation (Long-term)
            # forward ì „ì²´ë¥¼ ë¶€ë¥´ëŠ”ê²Œ ì•„ë‹ˆë¼, ë‚´ë¶€ ì¸ì½”ë”ë§Œ ë”°ë¡œ í˜¸ì¶œí•´ì•¼ í•¨
            all_gnn_embs, _ = model.gnn_encoder(adj_matrix)
            view_gnn = all_gnn_embs[user_ids] # (Batch, Dim)
            
            # View 2: Sequential Representation (Short-term)
            view_seq = model.seq_encoder(history_seqs) # (Batch, Dim)
            
            # --- [Projection Head] ---
            # Contrastive Learningì€ ë³´í†µ ë³„ë„ì˜ Projection Headë¥¼ íƒœì›Œì„œ ê³„ì‚°í•¨
            # (í•™ìŠµ í›„ì—ëŠ” ë²„ë¦¬ê±°ë‚˜, Fusion Headë¡œ ì‚¬ìš©)
            # ì—¬ê¸°ì„œëŠ” ëª¨ë¸ì˜ fusion_headë¥¼ ê³µìœ í•´ì„œ ì“°ê±°ë‚˜, 
            # ë‹¨ìˆœíˆ ë²¡í„° ìì²´ë¥¼ ë¹„êµí•´ë„ ë¨. (ì—¬ê¸°ì„  ê°„ë‹¨íˆ ë²¡í„° ë¹„êµ)
            
            # Contrastive Loss Calculation
            # (Batch, Dim) vs (Batch, Dim)
            embeddings = torch.cat([view_gnn, view_seq], dim=0)
            
            # Labels: (0, 1, ... B-1, 0, 1, ... B-1)
            batch_curr = view_gnn.size(0)
            labels = torch.arange(batch_curr).to(DEVICE)
            labels = torch.cat([labels, labels], dim=0)
            
            loss = loss_func(embeddings, labels)
            
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            step += 1
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
    print("ğŸ’¾ Saving User Tower...")
    torch.save(model.state_dict(), "user_tower_contrastive.pth")


# ------------------------------------------------------
# User Tower Training Task
# ------------------------------------------------------


class UserTowerTrainDataset(Dataset):
    def __init__(self, 
                 user_data_list: List[dict], 
                 product_id_map: Dict[int, int],
                 max_seq_len: int = 50):
        """
        user_data_list: [
            {'history': [101, 102], 'target': 103, 'gender': 1, 'age': 2}, ...
        ]
        """
        self.data = user_data_list
        self.max_seq_len = max_seq_len
        self.product_id_map = product_id_map

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data[idx]
        
        # 1. History ID Mapping & Padding
        raw_history = row['history']
        mapped_history = [self.product_id_map.get(pid, 0) for pid in raw_history] # ì—†ìœ¼ë©´ 0(PAD)
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶”ê¸° (Truncate or Pad)
        seq_len = len(mapped_history)
        if seq_len > self.max_seq_len:
            mapped_history = mapped_history[-self.max_seq_len:] # ìµœê·¼ ê²ƒë§Œ ìœ ì§€
        else:
            mapped_history = mapped_history + [0] * (self.max_seq_len - seq_len) # ë’¤ì— 0 ì±„ì›€

        # 2. Target Item Mapping
        target_db_id = row['target_idx'] 
        target_idx = self.product_id_map.get(target_db_id, 0)
        
        # 3. Profile Data
        gender = row.get('gender', 0)
        age = row.get('age', 0)
        season = row.get('season', 0)
        

        return {
            "history": torch.tensor(mapped_history, dtype=torch.long),
            "target_idx": torch.tensor(target_idx, dtype=torch.long), # ì •ë‹µ ì•„ì´í…œì˜ Model Index
            "gender": torch.tensor(gender, dtype=torch.long),
            "age": torch.tensor(age, dtype=torch.long),
            "season": torch.tensor(season, dtype=torch.long)
        }


import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm

# ==========================================
# 1. Dataset ì •ì˜
# ==========================================
class UserSessionDataset(Dataset):
    def __init__(self, 
                 user_sessions: list,   # [{'history':[], 'season':0, 'gender':0, 'target_item_id':10}, ...]
                 max_len: int = 50):
        self.data = user_sessions
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data[idx]
        
        # History Padding (Pre-padding or Post-padding)
        # ë³´í†µ TransformerëŠ” Post-padding + Maskingì„ ì“°ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” 0ì´ Pad IDë¼ê³  ê°€ì •
        history = row['history']
        if len(history) > self.max_len:
            history = history[-self.max_len:] # ìµœê·¼ê±°ë§Œ
        else:
            history = history + [0] * (self.max_len - len(history))
            
        return {
            'history': torch.tensor(history, dtype=torch.long),
            'season': torch.tensor(row['season'], dtype=torch.long),
            'gender': torch.tensor(row['gender'], dtype=torch.long),
            'target_item_id': torch.tensor(row['target_item_id'], dtype=torch.long),
            # ë§Œì•½ Item Towerê°€ Featureë¥¼ ì…ë ¥ë°›ì•„ì•¼ í•œë‹¤ë©´ ì—¬ê¸°ì— item_featuresë„ í¬í•¨ë˜ì–´ì•¼ í•¨
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ IDë¡œ Item Towerì—ì„œ ë²¡í„°ë¥¼ ë£©ì—…í•œë‹¤ê³  ê°€ì •
        }

# ==========================================
# 2. In-batch Negative Loss (Contrastive)
# ==========================================
class InfoNCELoss(nn.Module):
    """
    ë°°ì¹˜ ë‚´ì˜ ë‹¤ë¥¸ ìƒ˜í”Œë“¤ì„ Negativeë¡œ í™œìš©í•˜ëŠ” íš¨ìœ¨ì ì¸ Loss
    """
    def __init__(self, temperature=0.1):
        super().__init__()
        self.temperature = temperature
        self.criterion = nn.CrossEntropyLoss()

    def forward(self, user_vectors, item_vectors):
        """
        user_vectors: (Batch, Dim)
        item_vectors: (Batch, Dim) - Positive Pairs
        """
        # Similarity Matrix: (Batch, Batch)
        # (B, D) @ (D, B) -> (B, B)
        scores = torch.matmul(user_vectors, item_vectors.T)
        
        # Scaling
        scores = scores / self.temperature
        
        # Labels: ëŒ€ê°ì„ (Diagonal)ì´ ì •ë‹µ (0ë²ˆì§¸ ìœ ì €ëŠ” 0ë²ˆì§¸ ì•„ì´í…œì´ ì •ë‹µ)
        batch_size = user_vectors.size(0)
        labels = torch.arange(batch_size).to(user_vectors.device)
        
        loss = self.criterion(scores, labels)
  
        return loss
    
import torch.optim as optim
from tqdm import tqdm
from torch.utils.data import DataLoader





DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def train_final_user_tower(
    user_tower: FinalUserTower,
    pretrained_item_matrix: torch.Tensor, # Loss ê³„ì‚°ìš© (Target/Teacher)
    train_loader: DataLoader,
    epochs: int = 10,
    lr: float = 1e-4,
):
    # 1. Setup
    user_tower.to(DEVICE)
    pretrained_item_matrix = pretrained_item_matrix.to(DEVICE)
    
    optimizer = optim.AdamW(user_tower.parameters(), lr=lr, weight_decay=1e-4)
    loss_fn = InfoNCELoss(temperature=0.07).to(DEVICE)
    
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=lr, 
        steps_per_epoch=len(train_loader), epochs=epochs
    )

    print(f"ğŸš€ Start Training FinalUserTower on {DEVICE}...")
    user_tower.train()
    
    for epoch in range(epochs):
        total_loss = 0
        step = 0
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in progress_bar:
            # 2. Input Data Preparation
            history = batch['history'].to(DEVICE)       # (Batch, Seq)
            season = batch['season'].to(DEVICE)         # (Batch, )
            gender = batch['gender'].to(DEVICE)         # (Batch, )
            
            target_idx = batch['target_idx'].to(DEVICE) # (Batch, ) - ì •ë‹µ ì•„ì´í…œ Index
            
            # -----------------------------------------------------------
            # A. Ground Truth (Target Item Vectors)
            # -----------------------------------------------------------
            # ë¯¸ë¦¬ ê³„ì‚°ëœ ì•„ì´í…œ í–‰ë ¬ì—ì„œ ì •ë‹µ ë²¡í„°ë¥¼ ì§ì ‘ ê°€ì ¸ì˜´ (Teacher)
            # pretrained_item_matrix: (Total_Items, 128)
            with torch.no_grad():
                target_item_vectors = pretrained_item_matrix[target_idx]
                # íƒ€ê²Ÿ ë²¡í„°ë„ ì •ê·œí™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (Modelì´ Normalizeë¥¼ ì“´ë‹¤ë©´ ì—¬ê¸°ë„ í•´ì•¼ í•¨)
                target_item_vectors = F.normalize(target_item_vectors, p=2, dim=1)

            # -----------------------------------------------------------
            # B. User Representation (Student)
            # -----------------------------------------------------------
            # [Call] FinalUserTower.forward(history_ids, season_idx, gender_idx)
            user_vectors = user_tower(history, season, gender)
            
            # -----------------------------------------------------------
            # C. Contrastive Loss
            # -----------------------------------------------------------
            loss = loss_fn(user_vectors, target_item_vectors)
            
            # D. Optimization
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(user_tower.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()
            
            total_loss += loss.item()
            step += 1
            progress_bar.set_postfix({'loss': f"{loss.item():.4f}"})
            
        print(f"ğŸ“Š Epoch {epoch+1} Avg Loss: {total_loss / step:.4f}")
        
    print("âœ… Training Finished.")
    return user_tower

'''