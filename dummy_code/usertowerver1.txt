lass SymmetricUserTower(nn.Module):
    def __init__(self, 
                 num_total_products: int,    # DBì— ìˆëŠ” ì´ ìƒí’ˆ ê°œìˆ˜ (Padding ì œì™¸)
                 max_seq_len: int = 50,
                 input_dim: int = 128,       # Item Vector ì°¨ì›
                 d_model: int = 128,
                 nhead: int = 4,
                 num_layers: int = 2,
                 dropout: float = 0.1):
        super().__init__()
        
        self.max_seq_len = max_seq_len
        
        # --- 1. Embeddings ---
        
        # (A) Item Lookup Table (Pre-trained)
        # num_embeddings = ìƒí’ˆê°œìˆ˜ + 1 (for Padding Index 0)
        self.item_embedding = nn.Embedding(num_total_products + 1, input_dim, padding_idx=0)
        
        # (B) Positional Embedding
        self.position_embedding = nn.Embedding(max_seq_len + 1, d_model)
        
        # (C) User Profile (ì˜ˆì‹œ)
        self.gender_emb = nn.Embedding(3, 16, padding_idx=0)
        self.age_emb = nn.Embedding(10, 16, padding_idx=0)
        self.profile_projector = nn.Sequential(
            nn.Linear(16 + 16, d_model),
            nn.LayerNorm(d_model),
            nn.GELU()
        )

        # --- 2. Encoder ---
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=d_model*4,
            batch_first=True, dropout=dropout, activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # --- 3. Head ---
        self.encoder_head = DeepResidualHead(input_dim=d_model, output_dim=d_model)
        
        # [Stage 2 ì˜ì—­: Projection & Matching]

        self.projector = nn.Sequential(
            nn.Linear(128, 128),
            nn.LayerNorm(128),
            nn.GELU(),
            nn.Linear(128, 128)
        )

    def load_pretrained_weights(self, pretrained_matrix: torch.Tensor, freeze: bool = True):
        """
        [í•µì‹¬ ë¡œì§] DBì—ì„œ ê°€ì ¸ì˜¨ ë²¡í„°ë¥¼ ì„ë² ë”© ë ˆì´ì–´ì— ë®ì–´ì”Œì›ë‹ˆë‹¤.
        """
        # ì°¨ì› ê²€ì‚¬
        if self.item_embedding.weight.shape != pretrained_matrix.shape:
            raise ValueError(f"Shape Mismatch! Model: {self.item_embedding.weight.shape}, DB: {pretrained_matrix.shape}")
            
        # 1. ê°€ì¤‘ì¹˜ ë³µì‚¬ (Copy)
        self.item_embedding.weight.data.copy_(pretrained_matrix)
        print("âœ… Pretrained Item Vectors Loaded into User Tower.")
        
        # 2. ê°€ì¤‘ì¹˜ ë™ê²° (Freeze) - ì•„ì´í…œ ë²¡í„°ëŠ” ë” ì´ìƒ í•™ìŠµë˜ì§€ ì•ŠìŒ (ì¼ë°˜ì )
        if freeze:
            self.item_embedding.weight.requires_grad = False
            print("â„ï¸ Item Embeddings are FROZEN (Not trainable).")
        else:
            print("ğŸ”¥ Item Embeddings are TRAINABLE (Fine-tuning mode).")

    def forward(self, history_ids, profile_data):
        # ... (ì´ì „ ì½”ë“œì™€ ë™ì¼: history_idsëŠ” ë§¤í•‘ëœ Model Indexì—¬ì•¼ í•¨) ...
        B, L = history_ids.shape
        device = history_ids.device
        
        # (A) Lookup -> (B, L, 128) : ì—¬ê¸°ì„œ DB ë²¡í„°ê°€ íŠ€ì–´ë‚˜ì˜´
        seq_emb = self.item_embedding(history_ids)
        
        # ... (ì´í•˜ ë™ì¼: Positional ë”í•˜ê³  Transformer í†µê³¼) ...
        positions = torch.arange(L, device=device).unsqueeze(0).expand(B, L)
        pos_emb = self.position_embedding(positions)
        seq_emb = seq_emb + pos_emb
        
        # Profile
        g_emb = self.gender_emb(profile_data.get('gender', torch.zeros(B, dtype=torch.long, device=device)))
        a_emb = self.age_emb(profile_data.get('age', torch.zeros(B, dtype=torch.long, device=device)))
        profile_feat = torch.cat([g_emb, a_emb], dim=1)
        user_token = self.profile_projector(profile_feat).unsqueeze(1)
        
        combined_seq = torch.cat([user_token, seq_emb], dim=1)
        
        key_padding_mask = (history_ids == 0)
        user_token_mask = torch.zeros((B, 1), dtype=torch.bool, device=device)
        combined_mask = torch.cat([user_token_mask, key_padding_mask], dim=1)
        
        output = self.transformer(combined_seq, src_key_padding_mask=combined_mask)
        user_vector = output[:, 0, :]
        
        user_rep = self.encoder_head(user_vector)
        user_final = self.projector(user_rep)
        
        # cos í•˜ë ¤ë©´ loadí•œ ì•„ì´í…œ ë²¡í„°(v=1)ì´ë‘ ë§ì¶°ì•¼í•˜ë‹ˆê¹Œ. ì´ê±°ê¸°ì¤€ìœ¼ë¡œ tower í•™ìŠµ?
        return F.normalize(user_final, p=2, dim=1)








        def train_user_tower_task(
    db_session: Session = Depends(get_db), 
    epochs: int = 30, 
    batch_size: int = Depends(get_global_batch_size), 
    lr: float = 1e-4,
    temperature: float = 0.075 # Loss dx ë‚®ìŒ : low , Loss div : High
):
    print("\nğŸš€ [Task Started] User Tower Training...")
    
    # 1. Pre-trained Vector ë¡œë“œ (Lookup Table ì¤€ë¹„)
    pretrained_matrix, product_id_map = load_pretrained_vectors_from_db(db_session)
    num_total_products = len(product_id_map)
    
    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    model = SymmetricUserTower(
        num_total_products=num_total_products,
        max_seq_len=50,
        input_dim=128
    )
    
    # â­ í•µì‹¬: í•™ìŠµëœ ì•„ì´í…œ ë²¡í„° ì£¼ì… ë° ë™ê²°
    model.load_pretrained_weights(pretrained_matrix, freeze=True)
    model.to(DEVICE)
    model.train() # í•™ìŠµ ëª¨ë“œ
    
    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„ (Dummy Logic - ì‹¤ì œë¡œëŠ” DB User Log í…Œì´ë¸”ì—ì„œ ì¿¼ë¦¬í•´ì•¼ í•¨)
    # TODO: ì‹¤ì œ DBì—ì„œ ìœ ì € ë¡œê·¸(UserInteraction)ë¥¼ ê¸ì–´ì˜¤ëŠ” ë¡œì§ìœ¼ë¡œ ëŒ€ì²´ í•„ìš”
    print("ğŸ“Š Fetching user interaction data...")
    
    train_data = fetch_training_data_from_db(db_session, min_interactions=2)
    print(f" ë°ì´í„° ê°œìˆ˜ í™•ì¸: {len(train_data)}ê°œ")
    # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨ (Safety Check)
    if len(train_data) < batch_size:
        print("âš ï¸ Warning: Not enough data to train. At least one batch needed.")
       
    
    
    dataset = UserTowerTrainDataset(train_data, product_id_map)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        drop_last=True
        )
    
    # 4. Optimizer & Loss
    optimizer = optim.AdamW(model.parameters(), lr=lr)
    
    # CrossEntropyLossë¥¼ ì‚¬ìš© (In-batch Negative ë°©ì‹)
    # ì •ë‹µ ë¼ë²¨ì€ í•­ìƒ ëŒ€ê°ì„ (0, 1, 2...)ì´ ë¨
    criterion = nn.CrossEntropyLoss()

    # 5. Training Loop
    print(f"ğŸ”¥ Start Training for {epochs} epochs (Temp={temperature})...")
    
    for epoch in range(epochs):
        total_loss = 0
        steps = 0
        
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch in progress:
            history = batch['history'].to(DEVICE)     # (B, L)
            target_idx = batch['target_idx'].to(DEVICE) # (B,)
            gender = batch['gender'].to(DEVICE)
            age = batch['age'].to(DEVICE)
            
            profile_data = {'gender': gender, 'age': age}
            
            optimizer.zero_grad()
            
            # -----------------------------------------------------------
            # (A) User Vector ìƒì„± (B, 128)
            # -> ì´ë¯¸ ëª¨ë¸ ë‚´ë¶€ì—ì„œ F.normalize ë˜ì–´ì„œ ë‚˜ì˜´ (ê¸¸ì´=1)
            # -----------------------------------------------------------
            user_vectors = model(history, profile_data)
            
            # -----------------------------------------------------------
            # (B) Target Item Vector ì¡°íšŒ (B, 128)
            # -> DBì—ì„œ ì˜¨ ë²¡í„°ì´ë¯€ë¡œ ì´ë¯¸ ì •ê·œí™” ë˜ì–´ ìˆìŒ (ê¸¸ì´=1)
            # -----------------------------------------------------------
            target_item_vectors = model.item_embedding(target_idx)
            
            # -----------------------------------------------------------
            # (C) Similarity (Logits) Calculation & Scaling [í•µì‹¬!]
            # -----------------------------------------------------------
            # ë‚´ì (Dot Product) ìˆ˜í–‰ -> ì •ê·œí™”ëœ ë²¡í„°ë¼ë¦¬ì˜ ë‚´ì ì´ë¯€ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì„ (-1.0 ~ 1.0)
            # (B, 128) x (128, B) = (B, B) Matrix
            sim_matrix = torch.matmul(user_vectors, target_item_vectors.T)
            
            # [Temperature Scaling]
            # ê°’ì˜ ë²”ìœ„ë¥¼ -1~1ì—ì„œ -10~10 (temp=0.1 ê¸°ì¤€)ìœ¼ë¡œ ë»¥íŠ€ê¸°í•´ì¤Œ.
            # ê·¸ë˜ì•¼ Softmaxê°€ ë¾°ì¡±í•´ì§€ê³ (Sharpening), Gradientê°€ ì˜ íë¦„.
            logits = sim_matrix / temperature 
            
            # -----------------------------------------------------------
            # (D) Labeling (In-batch Negative)
            # -----------------------------------------------------------
            # ië²ˆì§¸ ìœ ì €ëŠ” ië²ˆì§¸ ì•„ì´í…œ(ëŒ€ê°ì„ )ì´ ì •ë‹µ.
            # ë‚˜ë¨¸ì§€ëŠ” ì „ë¶€ Negative Sampleë¡œ ê°„ì£¼.
            labels = torch.arange(batch_size).to(DEVICE)
            
            # -----------------------------------------------------------
            # (E) Loss & Update
            # -----------------------------------------------------------
            loss = criterion(logits, labels)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            steps += 1
            
            # ì§„í–‰ë°”ì— í˜„ì¬ Loss í‘œì‹œ
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
        avg_loss = total_loss / steps if steps > 0 else 0
        print(f"   Epoch {epoch+1} Summary | Avg Loss: {avg_loss:.4f}")

    # 5. Save Model
    save_path = os.path.join(MODEL_DIR, "user_tower_symmetric_final.pth")
    torch.save(model.state_dict(), save_path)
    print(f"âœ… Training Complete. Model saved to {save_path}")
    
    
    
