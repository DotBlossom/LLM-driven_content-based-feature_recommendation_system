import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from tqdm import tqdm





def dataset_peek(dataset, processor):
    """Datasetì—ì„œ 1ê°œ ìƒ˜í”Œì„ êº¼ë‚´ ë¡œì§ì´ ì •í•©í•œì§€ ê²€ìˆ˜"""
    print("\nğŸ§ [Data Peek] Checking Sequence Integrity...")
    sample = dataset[0]
    
    # 1. ì‹œí€€ìŠ¤ Shift í™•ì¸
    ids = sample['item_ids'].tolist()
    targets = sample['target_ids'].tolist()
    
    # 0ì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ì‹¤ì œ ë°ì´í„° ì¸ë±ìŠ¤ ì°¾ê¸°
    first_idx = next((i for i, x in enumerate(ids) if x != 0), None)
    
    if first_idx is not None and first_idx < len(ids) - 1:
        print(f"   - Input Seq  (t):   ... {ids[first_idx:first_idx+3]}")
        print(f"   - Target Seq (t+1): ... {targets[first_idx:first_idx+3]}")
        if ids[first_idx+1] == targets[first_idx]:
            print("   âœ… Shift Logic: OK (Input[t+1] == Target[t])")
        else:
            print("   âŒ Shift Logic: ERROR! Target is not shifted correctly.")

    # 2. ìœ ì € ìŠ¤íƒœí‹± í”¼ì²˜ í™•ì¸
    print(f"   - Age Bucket ID: {sample['age_bucket'].item()}")
    print(f"   - Cont Feats Shape: {sample['cont_feats'].shape}")



class FeatureProcessor:
    def __init__(self, user_path, item_path, seq_path,base_processor=None):
        print("ğŸš€ Loading preprocessed features...")
        self.users = pd.read_parquet(user_path).drop_duplicates(subset=['customer_id']).set_index('customer_id')
        self.items = pd.read_parquet(item_path).drop_duplicates(subset=['article_id']).set_index('article_id')
        self.seqs = pd.read_parquet(seq_path).set_index('customer_id')

        # ì¸ë±ìŠ¤ íƒ€ì… ê°•ì œ (String)
        self.users.index = self.users.index.astype(str)
        self.items.index = self.items.index.astype(str)
        self.seqs.index = self.seqs.index.astype(str)

        # =================================================================
        # 1. ID Mappings (1-based, 0 is Padding)
        # =================================================================
        self.user_ids = self.seqs.index.tolist() # ì‹œí€€ìŠ¤ê°€ ì¡´ì¬í•˜ëŠ” ìœ ì €ë§Œ ëŒ€ìƒ
        self.user2id = {uid: i + 1 for i, uid in enumerate(self.users.index)}

    
        
        
        if base_processor is None:
            # Trainì¼ ë•Œ: ìƒˆë¡­ê²Œ ì•„ì´í…œ ë²ˆí˜¸í‘œ ìƒì„±
            self.item_ids = self.items.index.tolist()
            self.item2id = {iid: i + 1 for i, iid in enumerate(self.item_ids)}
            self.num_items = len(self.item_ids)
        else:
            # Validationì¼ ë•Œ: Trainì˜ ë²ˆí˜¸í‘œë¥¼ ê·¸ëŒ€ë¡œ ë¬¼ë ¤ë°›ìŒ
            self.item_ids = base_processor.item_ids
            self.item2id = base_processor.item2id
            self.num_items = base_processor.num_items
        
        
        self.num_items = len(self.item_ids)
        
        

        # =================================================================
        # 2. Fast Lookup Arrays for Dataset (__getitem__ ì†ë„ ìµœì í™”)
        # =================================================================
        print("âš¡ Building fast lookup tables...")
        
        # [A] User Features (ìœ ì € ID 1~Nìœ¼ë¡œ ë°”ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ ë°°ì—´í™”)
        num_users_total = len(self.users) + 1
        
        # Bucket / Categorical (LongTensorìš©)
        self.u_bucket_arr = np.zeros((num_users_total, 4), dtype=np.int64) 
        self.u_cat_arr = np.zeros((num_users_total, 5), dtype=np.int64)
        # Continuous (FloatTensorìš©)
        self.u_cont_arr = np.zeros((num_users_total, 4), dtype=np.float32)

        # ë§¤í•‘ ìˆ˜í–‰
        for uid, row in self.users.iterrows():
            if uid not in self.user2id: continue
            uidx = self.user2id[uid]
            
            # Buckets: age, price, cnt, recency
            self.u_bucket_arr[uidx] = [
                row['age_bucket'], row['user_avg_price_bucket'], 
                row['total_cnt_bucket'], row['recency_bucket']
            ]
            # Categoricals: channel, club, news, fn, active
            self.u_cat_arr[uidx] = [
                row['preferred_channel'], row['club_member_status_idx'],
                row['fashion_news_frequency_idx'], row['FN'], row['Active']
            ]
            # Continuous Scaled: price_std, last_diff, repurch, weekend
            self.u_cont_arr[uidx] = [
                row['price_std_scaled'], row['last_price_diff_scaled'],
                row['repurchase_ratio_scaled'], row['weekend_ratio_scaled']
            ]

        # [B] Item Side Info Lookup (ì•„ì´í…œ ID 1~Nìœ¼ë¡œ ë°”ë¡œ ì ‘ê·¼)
        # ì•„ì´í…œ ë°ì´í„° í”„ë ˆì„ì— type_id, color_id ë“±ì´ ìˆë‹¤ê³  ê°€ì •
        self.i_side_arr = np.zeros((self.num_items + 1, 4), dtype=np.int64)
        for iid, row in self.items.iterrows():
            if iid not in self.item2id: continue
            idx = self.item2id[iid]
            # ì „ì²˜ë¦¬ëœ ì•„ì´í…œ í”¼ì²˜ì— ë§ì¶° ì»¬ëŸ¼ëª… ìˆ˜ì • í•„ìš”
            self.i_side_arr[idx] = [
                row.get('type_id', 0), row.get('color_id', 0), 
                row.get('graphic_id', 0), row.get('section_id', 0)
            ]

    def get_logq_probs(self, device):
        """Negative Samplingì´ë‚˜ Loss ë³´ì •ì„ ìœ„í•œ ì•„ì´í…œ ë“±ì¥ í™•ë¥  Log ë°˜í™˜"""
        raw_probs = self.items['raw_probability'].reindex(self.item_ids).values
        eps = 1e-6
        sorted_probs = np.nan_to_num(raw_probs, nan=0.0) + eps
        sorted_probs /= sorted_probs.sum()
        
        log_q_values = np.log(sorted_probs).astype(np.float32)
        
        full_log_q = np.zeros(self.num_items + 1, dtype=np.float32)
        full_log_q[1:] = log_q_values 
        full_log_q[0] = -20.0 # Padding Index
    
        return torch.tensor(full_log_q, dtype=torch.float32).to(device)


    # FeatureProcessor í´ë˜ìŠ¤ ë‚´ë¶€ì— ì¶”ê°€í•  ë©”ì„œë“œ
    def analyze_distributions(self):
        import matplotlib.pyplot as plt
        import seaborn as sns
        import numpy as np
        print("\nğŸ“Š [Data Distribution Analysis]")
        print("-" * 50)

        # 1. ì‹œí€€ìŠ¤ ê¸¸ì´ ë¶„í¬ (max_len ê²°ì •ì˜ í•µì‹¬ ê·¼ê±°)
        seq_lengths = self.seqs['sequence_ids'].apply(len)
        
        print(f"ğŸ”¹ Sequence Length Stats:")
        print(f"   - Mean: {seq_lengths.mean():.2f}")
        print(f"   - Median: {seq_lengths.median()}")
        print(f"   - P90: {seq_lengths.quantile(0.9):.1f}")
        print(f"   - P95: {seq_lengths.quantile(0.95):.1f}")
        print(f"   - Max: {seq_lengths.max()}")

        plt.figure(figsize=(12, 5))
        
        # Left: Sequence Length Histogram
        plt.subplot(1, 2, 1)
        sns.histplot(seq_lengths, bins=50, kde=True, color='skyblue')
        plt.axvline(seq_lengths.quantile(0.95), color='red', linestyle='--', label='P95')
        plt.title("User Sequence Length Distribution")
        plt.xlabel("Length")
        plt.legend()

        # 2. ì£¼ìš” ìœ ì € ì¹´í…Œê³ ë¦¬ ë¶„í¬ (Age, Price Bucket ë“±)
        # u_bucket_arrì—ì„œ 0ë²ˆ(Age), 1ë²ˆ(Price) ì»¬ëŸ¼ ì¶”ì¶œ (Padding ì œì™¸í•˜ê³  1ë²ˆ ì¸ë±ìŠ¤ë¶€í„°)
        plt.subplot(1, 2, 2)
        ages = self.u_bucket_arr[1:, 0]
        sns.countplot(x=ages, palette='viridis')
        plt.title("User Age Bucket Distribution")
        plt.xlabel("Age Bucket ID")

        plt.tight_layout()
        plt.show()

        # 3. ì•„ì´í…œ ë“±ì¥ ë¹ˆë„ (Long-tail í™•ì¸)
        all_items_in_seqs = [iid for subseq in self.seqs['sequence_ids'] for iid in subseq]
        item_counts = pd.Series(all_items_in_seqs).value_counts()
        
        print(f"\nğŸ”¹ Item Interaction Stats:")
        print(f"   - Total Unique Items in Seqs: {len(item_counts)}")
        print(f"   - Top 10% items cover {item_counts.iloc[:int(len(item_counts)*0.1)].sum() / len(all_items_in_seqs) * 100:.1f}% of interactions")
        
        # 4. ID Mapping Coverage í™•ì¸ (ë””ë²„ê¹…ìš©)
        missing_items = [iid for iid in item_counts.index if iid not in self.item2id]
        if missing_items:
            print(f"âš ï¸ Warning: {len(missing_items)} items in sequences are NOT in item master!")
        else:
            print("âœ… Success: All items in sequences are correctly mapped.")
        
class SASRecDataset(Dataset):
    def __init__(self, processor: FeatureProcessor, max_len=30, is_train=True):
        self.processor = processor
        self.max_len = max_len
        self.is_train = is_train
        self.user_ids = processor.user_ids

    def __len__(self):
        return len(self.user_ids)

    def __getitem__(self, idx):
        user_id = self.user_ids[idx]
        u_mapped_id = self.processor.user2id.get(user_id, 0)
        
        # 1. ì‹œí€€ìŠ¤ ë¡œë“œ 
        seq_raw = self.processor.seqs.loc[user_id, 'sequence_ids']
        
        # 1-1. time deltas : 1ë…„ì „ ë™ì¼ê³„ì ˆì— êµ¬ë§¤í–ˆë˜ê±´? ìµœê·¼ì€? ë“±ë“±ì„ ë§¤í•‘
        time_deltas_raw = self.processor.seqs.loc[user_id, 'sequence_deltas']
        bins = np.array([0, 3, 7, 14, 30, 60, 180, 330, 395])
        time_buckets = np.digitize(time_deltas_raw, bins, right=False).tolist()
        
        
        seq = [self.processor.item2id.get(item, 0) for item in seq_raw]
        
        # =========================================================
        # 2. Causality Split (SASRec Shift Logic)
        # =========================================================
        if self.is_train:
            # í•™ìŠµ ì‹œ: inputê³¼ targetì„ ìœ„í•´ max_len + 1 ê°œë¥¼ ê°€ì ¸ì˜´
            seq = seq[-(self.max_len + 1):]
            time_buckets = time_buckets[-(self.max_len + 1):] # [ì‹ ê·œ ì¶”ê°€] íƒ€ì„ ë²„í‚·ë„ ë™ì¼í•˜ê²Œ ìŠ¬ë¼ì´ì‹±
            if len(seq) > 1:
                input_seq = seq[:-1]  # t ì‹œì ê¹Œì§€ì˜ ì…ë ¥
                target_seq = seq[1:]  # t+1 ì‹œì ì˜ ì •ë‹µ
                input_time = time_buckets[:-1] # [ì‹ ê·œ ì¶”ê°€] t ì‹œì ì˜ ì‹œê°„ ê°„ê²©
            else:
                input_seq = seq
                target_seq = seq # ë°©ì–´ ì½”ë“œ (ê¸¸ì´ê°€ 1ì¸ ê²½ìš°)
                input_time = time_buckets
        else:
            # ì¶”ë¡ /ê²€ì¦ ì‹œ: ìµœì‹  max_len ê°œë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš© (ë‹¤ìŒ 1ê°œë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´)
            input_seq = seq[-self.max_len:]
            target_seq = [] # Test loopì—ì„œ ì •ë‹µì„ ë³„ë„ë¡œ ì²˜ë¦¬
            input_time = time_buckets[-self.max_len:] # [ì‹ ê·œ ì¶”ê°€]

        # =========================================================
        # 3. Left Padding
        # =========================================================
        # ìµœê·¼ í–‰ë™ì´ ë°°ì—´ì˜ ëì— ì˜¤ë„ë¡ Left Paddingì„ ì ìš©
        pad_len = self.max_len - len(input_seq)
        input_padded = [0] * pad_len + input_seq
        time_padded = [0] * pad_len + input_time
        if self.is_train:
            target_padded = [0] * pad_len + target_seq
        else:
            target_padded = [0] * self.max_len

        # =========================================================
        # 4. Item Side Info Lookup (Sequence)
        # =========================================================
        # padding(0)ì¸ ê²½ìš° Lookup ë°°ì—´ì˜ 0ë²ˆì§¸ ì¸ë±ìŠ¤(0,0,0,0)ë¥¼ ê°€ì ¸ì˜´
        item_side_info = self.processor.i_side_arr[input_padded]
        
        type_ids = item_side_info[:, 0]
        color_ids = item_side_info[:, 1]
        graphic_ids = item_side_info[:, 2]
        section_ids = item_side_info[:, 3]

        # Padding Mask (Trueë©´ Transformerì—ì„œ ë¬´ì‹œ)
        padding_mask = [True] * pad_len + [False] * len(input_seq)

        # =========================================================
        # 5. User Features Lookup (Static)
        # =========================================================
        u_buckets = self.processor.u_bucket_arr[u_mapped_id]
        u_cats = self.processor.u_cat_arr[u_mapped_id]
        u_conts = self.processor.u_cont_arr[u_mapped_id]

        # =========================================================
        # 6. Return Tensors
        # =========================================================
        return {
            
            'user_ids': user_id,
            # Sequence
            'item_ids': torch.tensor(input_padded, dtype=torch.long),
            'target_ids': torch.tensor(target_padded, dtype=torch.long),
            'padding_mask': torch.tensor(padding_mask, dtype=torch.bool),
            'time_bucket_ids': torch.tensor(time_padded, dtype=torch.long),
            
            # Item Side Info
            'type_ids': torch.tensor(type_ids, dtype=torch.long),
            'color_ids': torch.tensor(color_ids, dtype=torch.long),
            'graphic_ids': torch.tensor(graphic_ids, dtype=torch.long),
            'section_ids': torch.tensor(section_ids, dtype=torch.long),
            
            # User Buckets
            'age_bucket': torch.tensor(u_buckets[0], dtype=torch.long),
            'price_bucket': torch.tensor(u_buckets[1], dtype=torch.long),
            'cnt_bucket': torch.tensor(u_buckets[2], dtype=torch.long),
            'recency_bucket': torch.tensor(u_buckets[3], dtype=torch.long),
            
            # User Categoricals
            'channel_ids': torch.tensor(u_cats[0], dtype=torch.long),
            'club_status_ids': torch.tensor(u_cats[1], dtype=torch.long),
            'news_freq_ids': torch.tensor(u_cats[2], dtype=torch.long),
            'fn_ids': torch.tensor(u_cats[3], dtype=torch.long),
            'active_ids': torch.tensor(u_cats[4], dtype=torch.long),
            
            # User Continuous
            'cont_feats': torch.tensor(u_conts, dtype=torch.float32)
        }
    
import torch
import torch.nn as nn
import torch.nn.functional as F

class SASRecUserTower(nn.Module):
    def __init__(self, args):
        super().__init__()
        self.d_model = args.d_model
        self.max_len = args.max_len
        self.dropout_rate = args.dropout

        # ==================================================================
        # 1. Sequence Embeddings (Dynamic: Short-term Intent)
        # ==================================================================
        self.item_proj = nn.Linear(args.pretrained_dim, self.d_model)
        self.item_id_emb = nn.Embedding(args.num_items + 1, self.d_model, padding_idx=0)
        
        self.type_emb = nn.Embedding(args.num_prod_types + 1, self.d_model, padding_idx=0)
        self.color_emb = nn.Embedding(args.num_colors + 1, self.d_model, padding_idx=0)
        self.graphic_emb = nn.Embedding(args.num_graphics + 1, self.d_model, padding_idx=0)
        self.section_emb = nn.Embedding(args.num_sections + 1, self.d_model, padding_idx=0)

        self.pos_emb = nn.Embedding(self.max_len, self.d_model)
        # ì‹œí€€ìŠ¤ í”¼ì²˜ìš© (item_id, time, type, color, graphic, section) -> 6ê°œ
        self.seq_gate = nn.Parameter(torch.ones(6)) 
        
        # ìŠ¤í…Œí‹± í”¼ì²˜ìš© (age, price, cnt, rec, chan, club, news, fn, act, cont) -> 10ê°œ
        self.static_gate = nn.Parameter(torch.ones(10))
        # [ì—…ë°ì´íŠ¸] Time-Aware ë²„í‚· ì„ë² ë”©
        num_time_buckets = 12 
        self.time_emb = nn.Embedding(num_time_buckets, self.d_model, padding_idx=0)
        
        self.emb_ln = nn.LayerNorm(self.d_model)
        self.emb_dropout = nn.Dropout(self.dropout_rate)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=args.nhead,
            dim_feedforward=self.d_model * 2,
            dropout=self.dropout_rate,
            activation='gelu',
            norm_first=True,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=args.num_layers)

        # ==================================================================
        # 2. Static Embeddings (Global: Long-term Preference)
        # ==================================================================
        #  (A) Categorical Embeddings (Cardinalityì— ë”°ë¥¸ íš¨ìœ¨ì  ì°¨ì› í• ë‹¹)
        
        # 10êµ¬ê°„ Bucket í”¼ì²˜ë“¤ (ìƒëŒ€ì ìœ¼ë¡œ ì •ë³´ëŸ‰ì´ ë§ìŒ) -> 16ì°¨ì›
        mid_dim = 16
        self.age_emb = nn.Embedding(11, mid_dim, padding_idx=0)      
        self.price_emb = nn.Embedding(11, mid_dim, padding_idx=0)    
        self.cnt_emb = nn.Embedding(11, mid_dim, padding_idx=0)      
        self.recency_emb = nn.Embedding(11, mid_dim, padding_idx=0)  

        # Binary ë° Low-Cardinality í”¼ì²˜ë“¤ -> 4ì°¨ì›
        low_dim = 4
        self.channel_emb = nn.Embedding(4, low_dim, padding_idx=0)   
        self.club_status_emb = nn.Embedding(4, low_dim, padding_idx=0) 
        self.news_freq_emb = nn.Embedding(3, low_dim, padding_idx=0)   
        self.fn_emb = nn.Embedding(3, low_dim, padding_idx=0)        
        self.active_emb = nn.Embedding(3, low_dim, padding_idx=0)    

        # (B) Continuous Features Projection
        # 4ì°¨ì›ì˜ ì—°ì†í˜• ë°ì´í„°ë¥¼ 16ì°¨ì›ìœ¼ë¡œ í‚¤ì›Œ ì„ë² ë”©ê³¼ ë³¼ë¥¨ì„ ë§ì¶¤
        self.num_cont_feats = 4
        cont_proj_dim = 16
        self.cont_proj = nn.Linear(self.num_cont_feats, cont_proj_dim)

        # ëª¨ë“  Static Featureì˜ Concat í›„ ì´ ì°¨ì› ê³„ì‚°
        # (16 * 4) + (4 * 5) + 16 = 64 + 20 + 16 = 100
        total_static_input_dim = (mid_dim * 4) + (low_dim * 5) + cont_proj_dim
        
        self.static_mlp = nn.Sequential(
            nn.Linear(total_static_input_dim, self.d_model),
            nn.LayerNorm(self.d_model),
            nn.GELU(),
            nn.Dropout(self.dropout_rate)
        )

        # ==================================================================
        # 3. Final Fusion & Output
        # ==================================================================
        self.output_proj = nn.Sequential(
            nn.Linear(self.d_model * 2, self.d_model),
            nn.LayerNorm(self.d_model),
            nn.GELU(),
            nn.Linear(self.d_model, self.d_model)
        )
        
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity='relu')
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            nn.init.constant_(module.bias, 0)
            nn.init.constant_(module.weight, 1.0)
    def get_causal_mask(self, seq_len, device):
        # float('-inf') ëŒ€ì‹  dtype=torch.boolì„ ì‚¬ìš©í•˜ì—¬ True/False í–‰ë ¬ë¡œ ìƒì„±
        return torch.triu(torch.ones(seq_len, seq_len, device=device, dtype=torch.bool), diagonal=1)
    
    def forward(self, 
                # Sequence Inputs (Batch, Seq)
                pretrained_vecs, item_ids, 
                time_bucket_ids, 
                type_ids, color_ids, graphic_ids, section_ids,
                # Static Categorical Inputs (Batch, )
                age_bucket, price_bucket, cnt_bucket, recency_bucket,
                channel_ids, club_status_ids, news_freq_ids, fn_ids, active_ids,
                # Static Continuous Inputs (Batch, 4)
                cont_feats, 
                padding_mask=None,
                training_mode=True
                ):
        
        device = item_ids.device
        seq_len = item_ids.size(1)
        
        s_g_raw = torch.sigmoid(self.seq_gate) 
        u_g_raw = torch.sigmoid(self.static_gate)
        
        s_mask = torch.tensor([1.0, 1.0, 0.0, 0.0, 0.0, 0.0], device=s_g_raw.device)
        s_g = s_g_raw * s_mask  # ê³±ì…ˆ ì—°ì‚°ì€ ìƒˆë¡œìš´ í…ì„œë¥¼ ìƒì„±í•˜ë¯€ë¡œ ì•ˆì „í•©ë‹ˆë‹¤.

        u_mask = torch.ones_like(u_g_raw)
        # u_mask[6:9] = 0.0 # í•„ìš”í•œ ê²½ìš° ì£¼ì„ í•´ì œ
        u_g = u_g_raw * u_mask
        
        # -----------------------------------------------------------
        # Phase 1: Sequence Encoding (Short-term)
        # -----------------------------------------------------------
        seq_emb = self.item_proj(pretrained_vecs) 
        seq_emb += self.item_id_emb(item_ids) * s_g[0]
        seq_emb += self.time_emb(time_bucket_ids) * s_g[1]
        seq_emb += self.type_emb(type_ids) * s_g[2]
        seq_emb += self.color_emb(color_ids) * s_g[3]
        seq_emb += self.graphic_emb(graphic_ids) * s_g[4]
        seq_emb += self.section_emb(section_ids) * s_g[5]
        
        positions = torch.arange(seq_len, device=device).unsqueeze(0)
        seq_emb += self.pos_emb(positions)
        
        seq_emb = self.emb_ln(seq_emb)
        seq_emb = self.emb_dropout(seq_emb)

        causal_mask = self.get_causal_mask(seq_len, device)
        output = self.transformer_encoder(
            seq_emb, 
            mask=causal_mask, 
            src_key_padding_mask=padding_mask
        )

        # -----------------------------------------------------------
        # Phase 2: Static Encoding (Long-term)
        # -----------------------------------------------------------
        #  Datasetì—ì„œ ì „ë‹¬ë°›ì€ ëª¨ë“  í”¼ì²˜ë“¤ì„ ê°œë³„ ì„ë² ë”©
        emb_age = self.age_emb(age_bucket) * u_g[0]
        emb_price = self.price_emb(price_bucket) * u_g[1]
        emb_cnt = self.cnt_emb(cnt_bucket) * u_g[2]
        emb_rec = self.recency_emb(recency_bucket) * u_g[3]
        
        emb_chan = self.channel_emb(channel_ids) * u_g[4]
        emb_club = self.club_status_emb(club_status_ids) * u_g[5]
        emb_news = self.news_freq_emb(news_freq_ids) * u_g[6]
        emb_fn = self.fn_emb(fn_ids) * u_g[7]
        emb_act = self.active_emb(active_ids) * u_g[8]
        
        # ì—°ì†í˜• ë³€ìˆ˜ì—ë„ ê²Œì´íŠ¸ ì ìš© ê°€ëŠ¥
        cont_proj_vec = F.relu(self.cont_proj(cont_feats)) * u_g[9]
        
        # Concat All Static Features
        static_input = torch.cat([
            emb_age, emb_price, emb_cnt, emb_rec,
            emb_chan, emb_club, emb_news, emb_fn, emb_act,
            cont_proj_vec
        ], dim=1)
        
        # MLP Processing
        user_profile_vec = self.static_mlp(static_input) # (Batch, d_model)

        # -----------------------------------------------------------
        # Phase 3: Late Fusion
        # -----------------------------------------------------------
        if training_mode:
            user_profile_expanded = user_profile_vec.unsqueeze(1).expand(-1, seq_len, -1)
            final_vec = torch.cat([output, user_profile_expanded], dim=-1)
            final_vec = self.output_proj(final_vec)
            
            return F.normalize(final_vec, p=2, dim=-1)
        else:
            user_intent_vec = output[:, -1, :] 
            final_vec = torch.cat([user_intent_vec, user_profile_vec], dim=-1)
            final_vec = self.output_proj(final_vec)
            
            return F.normalize(final_vec, p=2, dim=-1)
        # -----------------------------------------------------------
        # SEQ + pretrained vec -> Transformer -> User Intent Vector late fusion
        # -----------------------------------------------------------
    
    
    
# ==========================================
# 1. Loss Functions (In-Batch Negative + LogQ)
# ==========================================
def inbatch_corrected_logq_loss(user_emb, item_tower_emb, target_ids, log_q_tensor, temperature=0.1, lambda_logq=1.0):
    """
    In-Batch Negative Samplingê³¼ LogQ ë³´ì •ì´ ì ìš©ëœ íš¨ìœ¨ì ì¸ CrossEntropy Loss
    
    Args:
        user_emb: (N, Dim) - Batch ë‹¨ìœ„ ìœ ì € ë²¡í„° (Flatten ì ìš©ë¨)
        item_tower_emb: (Num_Items, Dim) - ì „ì²´ ì•„ì´í…œ ì„ë² ë”©
        target_ids: (N, ) - ì •ë‹µ ì•„ì´í…œ ID (Flatten ì ìš©ë¨)
        log_q_tensor: (Num_Items, ) - ì „ì²´ ì•„ì´í…œì˜ ë“±ì¥ í™•ë¥ (Log)
        temperature: (float) - Softmax Temperature
        lambda_logq: (float) - í¸í–¥ ì œì–´ ê°•ë„ (ë³´í†µ 1.0)
    """
    N = user_emb.size(0)
    
    # 1. ë°°ì¹˜ ë‚´ ë“±ì¥í•œ ì •ë‹µ ì•„ì´í…œë“¤ì˜ ì„ë² ë”©ë§Œ ì¶”ì¶œ (N, Dim)
    # ì „ì²´ 47,062ê°œê°€ ì•„ë‹Œ ë°°ì¹˜ ë‚´ Nê°œë§Œ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ê·¹ë„ë¡œ ì ˆì•½í•©ë‹ˆë‹¤.
    batch_item_emb = item_tower_emb[target_ids]
    
    # 2. In-Batch Logits ê³„ì‚° (N, N)
    # ië²ˆì§¸ ìœ ì € ë²¡í„°ì™€ jë²ˆì§¸ ì•„ì´í…œ ë²¡í„°ì˜ ë‚´ì  (ëŒ€ê°ì„  ì›ì†Œê°€ ì •ë‹µ)
    logits = torch.matmul(user_emb, batch_item_emb.T)
    logits.div_(temperature)

    # 3. LogQ í¸í–¥ ë³´ì • (Sampling Bias Correction)
    if lambda_logq > 0.0:
        # ë°°ì¹˜ ë‚´ ë“±ì¥í•œ ì•„ì´í…œë“¤ì˜ LogQ ê°’ ì¶”ì¶œ (N,)
        batch_log_q = log_q_tensor[target_ids]
        
        # Google RecSys ë…¼ë¬¸ ìˆ˜ì‹: s^c(x, y) = s(x, y) - log(P(y))
        # ì •ë‹µì´ë“  ì˜¤ë‹µì´ë“  í•´ë‹¹ ì•„ì´í…œì˜ ì¸ê¸°ë„(LogQ)ë§Œí¼ ë¡œì§“ì„ ê¹ì•„ì¤Œ
        # Broadcasting: (N, N) í–‰ë ¬ì˜ ê° ì—´(Column)ì—ì„œ í•´ë‹¹ ì•„ì´í…œì˜ LogQë¥¼ ëºŒ
        logits = logits - (batch_log_q.view(1, -1) * lambda_logq)

    # 4. ì •ë‹µ Label ìƒì„± (ëŒ€ê°ì„  ì¸ë±ìŠ¤: 0, 1, 2, ..., N-1)
    # ië²ˆì§¸ ìœ ì €ì˜ ì •ë‹µì€ ë°°ì¹˜ ë‚´ ië²ˆì§¸ ì•„ì´í…œì„
    # 1. ë°°ì¹˜ ë‚´ì— ë™ì¼í•œ ì•„ì´í…œì´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” (N, N) True/False ë§ˆìŠ¤í¬
    same_item_mask = torch.eq(target_ids.unsqueeze(1), target_ids.unsqueeze(0))
    
    # 2. ëŒ€ê°ì„ (ì§„ì§œ ìì‹ ì˜ ì •ë‹µ)ì€ ìœ ì§€í•´ì•¼ í•˜ë¯€ë¡œ ì œì™¸í•  ë§ˆìŠ¤í¬ ìƒì„±
    diag_mask = torch.eye(N, dtype=torch.bool, device=user_emb.device)
    
    # 3. ì§„ì§œ ì •ë‹µì´ ì•„ë‹ˆë©´ì„œ ì•„ì´í…œ IDë§Œ ê²¹ì¹˜ëŠ” 'ì–µìš¸í•œ ì˜¤ë‹µ(False Negatives)' ì¶”ì¶œ
    false_neg_mask = same_item_mask & ~diag_mask
    
    # 4. ì–µìš¸í•œ ì˜¤ë‹µë“¤ì˜ ë¡œì§“ì„ -infë¡œ ë®ì–´ì”Œì›Œ ëª¨ë¸ì´ í˜ë„í‹°ë¥¼ ì£¼ì§€ ëª»í•˜ê²Œ ì°¨ë‹¨
    logits.masked_fill_(false_neg_mask, float('-inf'))
    
    
    
    
    labels = torch.arange(N, device=user_emb.device)
    
    # 5. ìµœì¢… CrossEntropyLoss ê³„ì‚°
    return F.cross_entropy(logits, labels)


def duorec_loss_refined(user_emb_1, user_emb_2, target_ids, temperature=0.1, lambda_sup=0.1):
    """
    Supervised Contrastive Learning (SupCon) + NaN ë°©ì§€ ë° íŒ¨ë”© ì²˜ë¦¬ ì™„ë£Œ
    """
    batch_size = user_emb_1.size(0)
    device = user_emb_1.device
    
    # 1. ë²¡í„° ì •ê·œí™”
    z_i = F.normalize(user_emb_1, dim=1)
    z_j = F.normalize(user_emb_2, dim=1)
    
    # 2. Unsupervised Loss (InfoNCE)
    logits_unsup = torch.matmul(z_i, z_j.T) / temperature
    labels = torch.arange(batch_size, device=device)
    loss_unsup = F.cross_entropy(logits_unsup, labels)
    
    # 3. Supervised Loss
    loss_sup = torch.tensor(0.0, device=device)
    
    if lambda_sup > 0:
        targets = target_ids.view(-1, 1)
        
        # ê°™ì€ íƒ€ê²Ÿì„ ê³µìœ í•˜ëŠ” ìœ ì € Mask (Batch, Batch)
        mask = torch.eq(targets, targets.T).float()
        
        # [Fix 1: Padding ì˜¤ì¸ ë°©ì§€] íƒ€ê²Ÿì´ 0(Padding)ì¸ ìœ ì €ë“¤ì€ ì „ë¶€ ë§ˆìŠ¤í¬ 0ìœ¼ë¡œ ì´ˆê¸°í™”
        pad_mask = (targets == 0).float()
        mask = mask * (1 - pad_mask) 
        
        # ìê¸° ìì‹  ì œì™¸
        mask.fill_diagonal_(0)
        
        if mask.sum() > 0:
            logits_sup = torch.matmul(z_i, z_i.T) / temperature
            diag_mask = torch.eye(batch_size, device=device).bool()
            
            # ëŒ€ê°ì„ ì„ -infë¡œ ë§ˆìŠ¤í‚¹ (ìê¸° ìì‹  ì œì™¸)
            logits_sup.masked_fill_(diag_mask, float('-inf'))
            
            # Log Softmax ê³„ì‚°
            log_prob = F.log_softmax(logits_sup, dim=1)
            
            # [Fix 2: NaN í­íƒ„ ë°©ì§€] ëŒ€ê°ì„ ì˜ -infê°€ mask(0)ì™€ ê³±í•´ì ¸ NaNì´ ë˜ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ 0.0ìœ¼ë¡œ ë®ì–´ì”€
            log_prob = log_prob.masked_fill(diag_mask, 0.0)
            
            # Positive Sampleì´ ì¡´ì¬í•˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§
            valid_rows = mask.sum(1) > 0
            if valid_rows.sum() > 0:
                loss_sup_batch = -(mask[valid_rows] * log_prob[valid_rows]).sum(1) / mask[valid_rows].sum(1)
                loss_sup = loss_sup_batch.mean()
                
    return loss_unsup + (lambda_sup * loss_sup)


# ======================================================

def inbatch_hnm_corrected_loss_with_stats(
    user_emb, item_tower_emb, target_ids, log_q_tensor, 
    top_k_percent=0.01, hnm_threshold=0.90, temperature=0.1, lambda_logq=0.7, lambda_cl=0.2
):
    """
    Refactored HNM: Selection(Mining)ê³¼ Correction(LogQ)ì„ ë¶„ë¦¬í•˜ì—¬ 'ì§„ì§œ ë§¤ìš´ë§›' ì¶”ì¶œ
    """
    N = user_emb.size(0)
    device = user_emb.device
    
    # 1. ì •ê·œí™” ë° ê¸°ë³¸ ìœ ì‚¬ë„ ê³„ì‚°
    u_norm = F.normalize(user_emb, p=2, dim=1)
    i_batch_norm = F.normalize(item_tower_emb[target_ids], p=2, dim=1)
    
    # ìˆœìˆ˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ matrix (N, N)
    cos_sim = torch.matmul(u_norm, i_batch_norm.T) 

    # 2. ë§ˆìŠ¤í‚¹ (False Negative & Too Similar)
    same_item_mask = torch.eq(target_ids.unsqueeze(1), target_ids.unsqueeze(0))
    diag_mask = torch.eye(N, dtype=torch.bool, device=device)
    
    with torch.no_grad():
        item_sim = torch.matmul(i_batch_norm, i_batch_norm.T)
        too_similar_mask = (item_sim > hnm_threshold) & ~diag_mask
    
    ignore_mask = same_item_mask | too_similar_mask
    
    # 3. [í•µì‹¬] í•˜ë“œ ë„¤ê±°í‹°ë¸Œ 'ì„ íƒ' (Mining)
    # LogQë¥¼ ë°°ì œí•˜ê³  ì˜¤ì§ 'ìœ ì‚¬ë„'ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ Kê°œë¥¼ ë½‘ìŠµë‹ˆë‹¤.
    mining_logits = (cos_sim / temperature).detach().clone()
    mining_logits.masked_fill_(ignore_mask, float('-inf'))
    
    # ê°€ìš©í•œ ë„¤ê±°í‹°ë¸Œ ê°œìˆ˜ ë‚´ì—ì„œ K ì„¤ì •
    available_negs = (~ignore_mask).sum(dim=1)
    num_k = max(1, min(int((N - 1) * top_k_percent), available_negs.min().item()))
    
    _, top_k_indices = torch.topk(mining_logits, k=num_k, dim=1)
    
    # 4. [ë³´ì •] ìµœì¢… ë¡œì§“ êµ¬ì„± ë° LogQ ì ìš©
    # ì„ íƒì€ ìœ ì‚¬ë„ë¡œ í–ˆì§€ë§Œ, Lossë¥¼ ê³„ì‚°í•  ë•ŒëŠ” ì¸ê¸°ë„ í¸í–¥ì„ ì œê±°í•©ë‹ˆë‹¤.
    logits = cos_sim / temperature
    if lambda_logq > 0.0:
        batch_log_q = log_q_tensor[target_ids]
        # Google ìˆ˜ì‹: s/temp - lambda * logQ
        logits = logits - (batch_log_q.view(1, -1) * lambda_logq)

    # 5. ìµœì¢… Lossìš© ë¡œì§“ ìˆ˜ì§‘
    pos_logits = torch.diagonal(logits).unsqueeze(1)
    hard_neg_logits = torch.gather(logits, 1, top_k_indices)
    
    final_logits = torch.cat([pos_logits, hard_neg_logits], dim=1)
    labels = torch.zeros(N, dtype=torch.long, device=device)
    
    loss = F.cross_entropy(final_logits, labels)
    
    # 6. 'ë§¤ìš´ë§›' í†µê³„ (ë³´ì • ì „ ìˆœìˆ˜ ìœ ì‚¬ë„ ê¸°ì¤€)
    with torch.no_grad():
        hard_hn_sims = torch.gather(cos_sim, 1, top_k_indices)
        avg_hn_sim = hard_hn_sims.mean().item()

    return loss, {"avg_hn_similarity": avg_hn_sim, "num_active_hard_negs": num_k}


def inbatch_mixed_hnm_loss_with_stats(
    user_emb, item_tower_emb, target_ids, log_q_tensor, 
    top_k_percent=0.01, random_sample_size=100, 
    hnm_threshold=0.90, temperature=0.1, lambda_logq=0.7
):
    """
    Mixed Strategy: Hard Negatives (Top-K) + Random Negatives (M)
    """
    N = user_emb.size(0)
    device = user_emb.device
    
    # 1. ì •ê·œí™” ë° ìœ ì‚¬ë„ ê³„ì‚°
    u_norm = F.normalize(user_emb, p=2, dim=1)
    i_batch_norm = F.normalize(item_tower_emb[target_ids], p=2, dim=1)
    cos_sim = torch.matmul(u_norm, i_batch_norm.T) 

    # 2. ë§ˆìŠ¤í‚¹ (False Negative & Too Similar)
    same_item_mask = torch.eq(target_ids.unsqueeze(1), target_ids.unsqueeze(0))
    diag_mask = torch.eye(N, dtype=torch.bool, device=device)
    
    with torch.no_grad():
        item_sim = torch.matmul(i_batch_norm, i_batch_norm.T)
        too_similar_mask = (item_sim > hnm_threshold) & ~diag_mask
    
    ignore_mask = same_item_mask | too_similar_mask
    
    # 3. [Mining] Hard Negative Selection (Top-K)
    mining_logits = (cos_sim / temperature).detach().clone()
    mining_logits.masked_fill_(ignore_mask, float('-inf'))
    
    num_k = max(1, int((N - 1) * top_k_percent))
    _, top_k_indices = torch.topk(mining_logits, k=num_k, dim=1)
    
    # 4. [Mining] Random Negative Selection
    # í•˜ë“œ ë„¤ê±°í‹°ë¸Œê°€ ì•„ë‹Œ ë‚˜ë¨¸ì§€ ì¤‘ì—ì„œ ëœë¤í•˜ê²Œ ì¶”ì¶œ
    # êµ¬í˜„ í¸ì˜ìƒ ì „ì²´ ë°°ì¹˜ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë½‘ë˜, ë§ˆìŠ¤í‚¹ëœ ê²ƒë“¤ì€ ì´í›„ Lossì—ì„œ ì œì™¸ë¨
    random_indices = torch.randint(0, N, (N, random_sample_size), device=device)

    # 5. [Correction] ìµœì¢… ë¡œì§“ êµ¬ì„± (LogQ ì ìš©)
    logits = cos_sim / temperature
    if lambda_logq > 0.0:
        batch_log_q = log_q_tensor[target_ids]
        logits = logits - (batch_log_q.view(1, -1) * lambda_logq)

    # 6. ë¡œì§“ ìˆ˜ì§‘ (Positive + Hard + Random)
    pos_logits = torch.diagonal(logits).unsqueeze(1)
    hard_neg_logits = torch.gather(logits, 1, top_k_indices)
    random_neg_logits = torch.gather(logits, 1, random_indices)
    
    # [ì¤‘ìš”] Random ìƒ˜í”Œ ì¤‘ í˜¹ì‹œë‚˜ Positiveë‚˜ Too Similarê°€ ì„ì˜€ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì•„ì£¼ ë‚®ì€ ê°’ìœ¼ë¡œ ì²˜ë¦¬
    # (íš¨ìœ¨ì„ ìœ„í•´ ì™„ì „ ì œì™¸ ëŒ€ì‹  í˜ë„í‹° ë¶€ì—¬)
    random_mask = torch.gather(ignore_mask, 1, random_indices)
    random_neg_logits.masked_fill_(random_mask, -1e9)

    final_logits = torch.cat([pos_logits, hard_neg_logits, random_neg_logits], dim=1)
    labels = torch.zeros(N, dtype=torch.long, device=device)
    
    loss = F.cross_entropy(final_logits, labels)
    
    # í†µê³„ ê³„ì‚°
    with torch.no_grad():
        hard_hn_sims = torch.gather(cos_sim, 1, top_k_indices)
        avg_hn_sim = hard_hn_sims.mean().item()

    return loss, {"avg_hn_similarity": avg_hn_sim, "num_hard": num_k, "num_random": random_sample_size}


def full_batch_hard_emphasis_loss(
    user_emb, item_tower_emb, target_ids, log_q_tensor, 
    top_k_percent=0.01, hard_margin=0.2, 
    hnm_threshold=0.90, temperature=0.1, lambda_logq=1.0
):
    """
    Full-Batch HNM: 
    1) ì „ì²´ ë°°ì¹˜(N-1)ë¥¼ ë„¤ê±°í‹°ë¸Œë¡œ ì‚¬ìš©í•˜ì—¬ Global Structure ìœ ì§€
    2) í•˜ë“œ ë„¤ê±°í‹°ë¸Œì— Marginì„ ì¶”ê°€í•˜ì—¬ ì •ë°€ë„(Hard Emphasis) ê°•í™”
    """
    N = user_emb.size(0)
    device = user_emb.device
    
    # 1. ì •ê·œí™” ë° ìœ ì‚¬ë„ ê³„ì‚°
    u_norm = F.normalize(user_emb, p=2, dim=1)
    i_batch_norm = F.normalize(item_tower_emb[target_ids], p=2, dim=1)
    cos_sim = torch.matmul(u_norm, i_batch_norm.T) 

    # 2. ë§ˆìŠ¤í‚¹ (False Negative & Too Similar)
    same_item_mask = torch.eq(target_ids.unsqueeze(1), target_ids.unsqueeze(0))
    diag_mask = torch.eye(N, dtype=torch.bool, device=device)
    ignore_mask = same_item_mask | ((torch.matmul(i_batch_norm, i_batch_norm.T) > hnm_threshold) & ~diag_mask)
    
    # 3. [Mining] í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìœ„ì¹˜ ì°¾ê¸° (Top-K)
    with torch.no_grad():
        mining_sim = cos_sim.detach().clone()
        mining_sim.masked_fill_(ignore_mask, float('-inf'))
        num_k = max(1, int((N - 1) * top_k_percent))
        # ê° í–‰(ìœ ì €)ë³„ë¡œ í•˜ë“œ ë„¤ê±°í‹°ë¸Œì˜ 'ìœ„ì¹˜(ì¸ë±ìŠ¤)'ë¥¼ í™•ë³´
        _, top_k_indices = torch.topk(mining_sim, k=num_k, dim=1)

    # 4. [Correction] ì „ì²´ ë¡œì§“ êµ¬ì„± ë° LogQ ì ìš©
    logits = cos_sim / temperature
    if lambda_logq > 0.0:
        batch_log_q = log_q_tensor[target_ids]
        logits = logits - (batch_log_q.view(1, -1) * lambda_logq)

    # 5. [Hard Emphasis] í•˜ë“œ ë„¤ê±°í‹°ë¸Œì— Margin ì¶”ê°€
    # í•˜ë“œ ë„¤ê±°í‹°ë¸Œë“¤ì˜ ë¡œì§“ì— ë§ˆì§„ì„ ë”í•´, ëª¨ë¸ì´ ì–˜ë„¤ë¥¼ 'ì‹¤ì œë³´ë‹¤ ë” ê°€ê¹ë‹¤'ê³  ì°©ê°í•˜ê²Œ ë§Œë“¦
    # ê²°ê³¼ì ìœ¼ë¡œ ë” ê°•í•œ í˜ìœ¼ë¡œ ë°€ì–´ë‚´ê²Œ ë¨
    emphasis_mask = torch.zeros_like(logits, dtype=torch.bool)
    emphasis_mask.scatter_(1, top_k_indices, True)
    
    # í•˜ë“œ ë„¤ê±°í‹°ë¸Œ ìœ„ì¹˜ì—ë§Œ ë§ˆì§„ ì¶”ê°€ (ì´ê²Œ 'ì½• ì§‘ì–´ íŒ¨ëŠ”' í•µì‹¬)
    logits = logits + (emphasis_mask.float() * (hard_margin / temperature))

    # 6. [Final Masking] ì–µìš¸í•œ ì˜¤ë‹µ(False Negatives) ì°¨ë‹¨
    # ìê¸° ìì‹ (Positive)ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ê²¹ì¹˜ëŠ” ì•„ì´í…œë“¤ ì œê±°
    false_neg_mask = same_item_mask & ~diag_mask
    logits.masked_fill_(false_neg_mask, float('-inf'))

    # 7. Loss ê³„ì‚° (N x N ì „ì²´ ì‚¬ìš©)
    labels = torch.arange(N, device=device)
    loss = F.cross_entropy(logits, labels)
    
    # í†µê³„
    with torch.no_grad():
        hard_hn_sims = torch.gather(cos_sim, 1, top_k_indices)
        avg_hn_sim = hard_hn_sims.mean().item()

    return loss, {"avg_hn_similarity": avg_hn_sim, "num_hard": num_k}



def inbatch_corrected_logq_loss(
    user_emb, item_tower_emb, target_ids, user_ids, log_q_tensor, # ğŸ’¡ user_ids ì¶”ê°€
    temperature=0.1, lambda_logq=1.0
):
    N = user_emb.size(0)
    
    # 1. ë°°ì¹˜ ë‚´ ì •ë‹µ ì•„ì´í…œ ì„ë² ë”© ì¶”ì¶œ
    batch_item_emb = item_tower_emb[target_ids]
    
    # 2. In-Batch Logits ê³„ì‚°
    logits = torch.matmul(user_emb, batch_item_emb.T)
    logits.div_(temperature)

    # 3. LogQ í¸í–¥ ë³´ì •
    if lambda_logq > 0.0:
        batch_log_q = log_q_tensor[target_ids]
        logits = logits - (batch_log_q.view(1, -1) * lambda_logq)

    # 4. ë§ˆìŠ¤í‚¹ (False Negatives ì°¨ë‹¨)
    # (A) ì•„ì´í…œ IDê°€ ìš°ì—°íˆ ê°™ì€ ê²½ìš°
    same_item_mask = torch.eq(target_ids.unsqueeze(1), target_ids.unsqueeze(0))
    # (B) ğŸ’¡ [ì¶”ê°€] ë™ì¼ ìœ ì €ì˜ ë‹¤ë¥¸ íƒ€ì„ìŠ¤í… íƒ€ê²Ÿì¸ ê²½ìš° (A->B ì˜ˆì¸¡í•  ë•Œ, A->C ì˜ˆì¸¡ íƒ€ê²Ÿì´ ë„¤ê±°í‹°ë¸Œê°€ ë˜ëŠ” ê²ƒ ë°©ì§€)
    same_user_mask = torch.eq(user_ids.unsqueeze(1), user_ids.unsqueeze(0))
    
    # ëŒ€ê°ì„ (ìì‹ ì˜ ì§„ì§œ ì •ë‹µ)ì€ ìœ ì§€
    diag_mask = torch.eye(N, dtype=torch.bool, device=user_emb.device)
    
    # ìµœì¢…ì ìœ¼ë¡œ ì–µìš¸í•œ ì˜¤ë‹µë“¤ì„ ê±¸ëŸ¬ë‚´ëŠ” ë§ˆìŠ¤í¬ (ê°™ì€ ì•„ì´í…œì´ê±°ë‚˜ OR ê°™ì€ ìœ ì €ì´ê±°ë‚˜)
    false_neg_mask = (same_item_mask | same_user_mask) & ~diag_mask
    
    # -infë¡œ ë®ì–´ì”Œì›Œ ë„¤ê±°í‹°ë¸Œ ì—°ì‚°ì—ì„œ ì™„ì „íˆ ë°°ì œ
    logits.masked_fill_(false_neg_mask, float('-inf'))
    
    # 5. ìµœì¢… CrossEntropyLoss ê³„ì‚°
    labels = torch.arange(N, device=user_emb.device)
    return F.cross_entropy(logits, labels)