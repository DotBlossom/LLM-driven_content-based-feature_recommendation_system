import torch
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence

import os
BASE_DIR = r"D:\trainDataset\localprops"
RAW_FILE_PATH = os.path.join(BASE_DIR, "transactions_train_filtered.json")

# ê²°ê³¼ ì €ì¥ ê²½ë¡œ (Parquet + JSON)
USER_FEAT_PATH_PQ = os.path.join(BASE_DIR, "features_user.parquet")
USER_FEAT_PATH_JS = os.path.join(BASE_DIR, "features_user.json")

ITEM_FEAT_PATH_PQ = os.path.join(BASE_DIR, "features_item.parquet")
ITEM_FEAT_PATH_JS = os.path.join(BASE_DIR, "features_item.json")

SEQ_DATA_PATH_PQ = os.path.join(BASE_DIR, "features_sequence.parquet")
SEQ_DATA_PATH_JS = os.path.join(BASE_DIR, "features_sequence.json")

# ì „ì²´ íˆìŠ¤í† ë¦¬ ì €ì¥ ê²½ë¡œ
WEEKLY_HISTORY_PATH = os.path.join(BASE_DIR, "history_weekly_sales.parquet")
MONTHLY_HISTORY_PATH = os.path.join(BASE_DIR, "history_monthly_sales.parquet")

class FeatureProcessor:
    def __init__(self, user_path, item_path, seq_path):
        print("ğŸ”„ Loading & Scaling Features...")
        
        # 1. Load Parquet
        self.users = pd.read_parquet(user_path).set_index('customer_id')
        self.items = pd.read_parquet(item_path).set_index('article_id')
        self.seqs = pd.read_parquet(seq_path).set_index('customer_id')
        
        # -------------------------------------------------------
        # 2. Prepare Scalers (ì „ì²˜ë¦¬ëœ Logê°’ë“¤ì„ ì •ê·œí™”)
        # -------------------------------------------------------
        self.user_scaler = StandardScaler()
        self.item_scaler = StandardScaler()
        
        # User Dense Columns
        self.u_dense_cols = ['user_avg_price_log', 'total_cnt_log', 'recency_log']
        # Item Dense Columns (Advanced Features í¬í•¨)
        self.i_dense_cols = [
            'pop_1w_log', 'pop_1m_log', 
            'velocity_1w', 'velocity_1m', 
            'days_since_release_log', 'avg_item_price_log'
        ]
        
        # -------------------------------------------------------
        # 3. Apply Scaling & Store
        # -------------------------------------------------------
        # (ì¤‘ìš”) Cross Feature ê³„ì‚°ì„ ìœ„í•´ Raw(Log) ê°’ì€ ë”°ë¡œ ë³´ê´€í•´ì•¼ í•¨
        # Python DictionaryëŠ” ëŠë¦¬ë¯€ë¡œ DataFrame ìƒíƒœë¡œ ìœ ì§€í•˜ë˜, 
        # get_item_tensor í˜¸ì¶œ ì‹œ Scaled ê°’ì„ ë°˜í™˜í•˜ë„ë¡ ë¯¸ë¦¬ ê³„ì‚°í•´ë‘ .
        
        # User Scaling
        self.users_scaled = self.users.copy()
        self.users_scaled[self.u_dense_cols] = self.user_scaler.fit_transform(self.users[self.u_dense_cols])
        
        # Item Scaling (raw_probabilityëŠ” ìŠ¤ì¼€ì¼ë§ ì œì™¸!)
        self.items_scaled = self.items.copy()
        self.items_scaled[self.i_dense_cols] = self.item_scaler.fit_transform(self.items[self.i_dense_cols])
        
        print(f"âœ… User Features: {len(self.users)}, Item Features: {len(self.items)}")

    def get_user_tensor(self, user_ids):
        """User Tower Input: Scaled Dense Features"""
        # locìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
        batch_data = self.users_scaled.loc[user_ids]
        
        # Dense Features
        dense = torch.tensor(batch_data[self.u_dense_cols].values, dtype=torch.float32)
        
        # Categorical (Preferred Channel: 1,2 -> 0,1)
        cat = torch.tensor(batch_data['preferred_channel'].values - 1, dtype=torch.long)
        
        return dense, cat

    def get_item_tensor(self, item_ids):
        """GDCN Input: Scaled Dense Features (1w, 1m, velocity, release...)"""
        batch_data = self.items_scaled.loc[item_ids]
        return torch.tensor(batch_data[self.i_dense_cols].values, dtype=torch.float32)

    def get_raw_probability(self, item_ids):
        """User Tower Lossìš© (LogQ Correction)"""
        return torch.tensor(self.items.loc[item_ids]['raw_probability'].values, dtype=torch.float32)

    def get_cross_features(self, user_ids, item_ids):
        """
        Cross Features ê³„ì‚°
        (ì¤‘ìš”) ìŠ¤ì¼€ì¼ë§ ëœ ê°’ì´ ì•„ë‹ˆë¼, ì›ë³¸ Log ê°’ì„ ì¨ì•¼ ë¬¼ë¦¬ì  ì˜ë¯¸ê°€ ë§ìŒ!
        """
        # self.users, self.itemsëŠ” ìŠ¤ì¼€ì¼ë§ ì „ ì›ë³¸(Log applied)
        u_raw = self.users.loc[user_ids]
        i_raw = self.items.loc[item_ids]
        
        # 1. Price Gap: Item Price - User Avg Price (ë‘˜ ë‹¤ Log ìƒíƒœ)
        price_gap = i_raw['avg_item_price_log'].values - u_raw['user_avg_price_log'].values
        
        # 2. Trend Interaction: Item Velocity * User Activity
        # í™œë™ì ì¸ ìœ ì €(cnt high)ê°€ ê°€ì†ë„(velocity) ë†’ì€ ì•„ì´í…œì— ë°˜ì‘
        # velocityëŠ” ìŠ¤ì¼€ì¼ë§ ì „ì—ë„ -1~5 ë²”ìœ„ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        trend_interaction_1w = i_raw['velocity_1w'].values * u_raw['total_cnt_log'].values
        trend_interaction_1m = i_raw['velocity_1m'].values * u_raw['total_cnt_log'].values
        
        # (B, 3)
        cross_feats = np.stack([price_gap, trend_interaction_1w, trend_interaction_1m], axis=1)
        return torch.tensor(cross_feats, dtype=torch.float32)

# ==========================================
# 3. Dataset Classes
# ==========================================
class UserTowerDataset(Dataset):
    def __init__(self, user_ids, processor, max_seq_len=50):
        self.user_ids = user_ids
        self.processor = processor
        self.max_len = max_seq_len
        
    def __len__(self):
        return len(self.user_ids)
    
    def __getitem__(self, idx):
        u_id = self.user_ids[idx]
        
        # 1. User Features
        u_dense, u_cat = self.processor.get_user_tensor([u_id])
        
        # 2. Sequence
        try:
            seq_row = self.processor.seqs.loc[u_id]
            seq_ids = seq_row['sequence_ids'][-self.max_len:]
            seq_deltas = seq_row['sequence_deltas'][-self.max_len:]
        except KeyError: # ì‹œí€€ìŠ¤ ì—†ëŠ” ìœ ì € ì˜ˆì™¸ì²˜ë¦¬
            seq_ids, seq_deltas = [], []
            
        return {
            'user_dense': u_dense.squeeze(0),
            'user_cat': u_cat.squeeze(0),
            'seq_ids': torch.tensor(seq_ids, dtype=torch.long),
            'seq_deltas': torch.tensor(seq_deltas, dtype=torch.long)
        }

class RerankerDataset(Dataset):
    def __init__(self, interactions_df, processor, max_seq_len=50):
        self.data = interactions_df
        self.processor = processor
        self.max_len = max_seq_len
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        u_id = str(row['user_id'])
        i_id = str(row['item_id'])
        label = row['label']
        
        # 1. User & Item Tensors (Scaled)
        u_dense, u_cat = self.processor.get_user_tensor([u_id])
        i_dense = self.processor.get_item_tensor([i_id])
        
        # 2. Cross Features (Calculated from Raw Log)
        cross_feats = self.processor.get_cross_features([u_id], [i_id])
        
        # 3. GDCN Input Concatenation
        # User(3) + Item(6) + Cross(3) = 12 Dense Features
        gdcn_dense = torch.cat([u_dense.squeeze(0), i_dense.squeeze(0), cross_feats.squeeze(0)], dim=0)
        
        # 4. Sequence (Attentionìš©)
        try:
            seq_row = self.processor.seqs.loc[u_id]
            seq_ids = torch.tensor(seq_row['sequence_ids'][-self.max_len:], dtype=torch.long)
        except KeyError:
            seq_ids = torch.tensor([], dtype=torch.long)

        return {
            'gdcn_dense': gdcn_dense,
            'user_cat': u_cat.squeeze(0),
            'seq_ids': seq_ids,
            'target_item_id': torch.tensor(int(i_id) if i_id.isdigit() else 0), # ID Mapping í•„ìš”
            'label': torch.tensor(label, dtype=torch.float32)
        }

# Collate Function
def reranker_collate_fn(batch):
    dense = torch.stack([b['gdcn_dense'] for b in batch])
    cat = torch.stack([b['user_cat'] for b in batch])
    label = torch.stack([b['label'] for b in batch])
    target_item = torch.stack([b['target_item_id'] for b in batch])
    
    seq_ids = pad_sequence([b['seq_ids'] for b in batch], batch_first=True, padding_value=0)
    seq_mask = (seq_ids != 0).long()
    
    return dense, cat, seq_ids, seq_mask, target_item, label

# ==========================================
# Main Execution Check
# ==========================================
if __name__ == "__main__":
    # Test Loading
    try:
        processor = FeatureProcessor(
            USER_FEAT_PATH_PQ, ITEM_FEAT_PATH_PQ, SEQ_DATA_PATH_PQ
        )
        
        # Mock Data
        mock_interactions = pd.DataFrame({
            'user_id': processor.users.index[:5],
            'item_id': processor.items.index[:5],
            'label': [1, 0, 1, 0, 1]
        })
        
        ds = RerankerDataset(mock_interactions, processor)
        loader = torch.utils.data.DataLoader(ds, batch_size=2, collate_fn=reranker_collate_fn)
        
        for batch in loader:
            dense, cat, seq, mask, target, lbl = batch
            print("\nâœ… Reranker Batch Check:")
            print(f" - Dense Input Shape: {dense.shape} (Batch, Features)")
            print(f" - Sequence Shape: {seq.shape}")
            break
            
    except Exception as e:
        print(f"âš ï¸ Error during test: {e}")
    
    
    
    '''
    
    
    from torch.utils.data import Dataset
from torch.nn.utils.rnn import pad_sequence

class UserTowerDataset(Dataset):
    def __init__(self, user_ids, processor, max_seq_len=50):
        self.user_ids = user_ids # í•™ìŠµí•  ìœ ì € ë¦¬ìŠ¤íŠ¸
        self.processor = processor
        self.max_len = max_seq_len
        
    def __len__(self):
        return len(self.user_ids)
    
    def __getitem__(self, idx):
        u_id = self.user_ids[idx]
        
        # 1. Dense & Cat Features (Pre-computed)
        # processor ë‚´ë¶€ì ìœ¼ë¡œ locì„ ì“°ì§€ë§Œ, ì‹¤ì œë¡  array indexingì´ ë” ë¹ ë¦„
        # ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ processor ë©”ì†Œë“œ í˜¸ì¶œ
        user_dense, user_cat = self.processor.get_user_tensor([u_id])
        
        # 2. Sequence Data
        seq_row = self.processor.seqs.loc[u_id]
        seq_ids = seq_row['sequence_ids'][-self.max_len:]
        seq_deltas = seq_row['sequence_deltas'][-self.max_len:]
        
        # Tensor ë³€í™˜
        seq_ids_tensor = torch.tensor(seq_ids, dtype=torch.long)
        seq_deltas_tensor = torch.tensor(seq_deltas, dtype=torch.long)
        
        return {
            'user_dense': user_dense.squeeze(0), # (2,)
            'user_cat': user_cat.squeeze(0),     # (1,)
            'seq_ids': seq_ids_tensor,           # (L,)
            'seq_deltas': seq_deltas_tensor      # (L,)
        }

# Collate Fn: ë°°ì¹˜ ë‹¨ìœ„ íŒ¨ë”© ì²˜ë¦¬
def user_tower_collate_fn(batch):
    user_dense = torch.stack([b['user_dense'] for b in batch])
    user_cat = torch.stack([b['user_cat'] for b in batch])
    
    # Sequence Padding (ë’¤ì— 0 ì±„ì›€)
    seq_ids = pad_sequence([b['seq_ids'] for b in batch], batch_first=True, padding_value=0)
    seq_deltas = pad_sequence([b['seq_deltas'] for b in batch], batch_first=True, padding_value=0)
    
    # Mask ìƒì„± (Padding ë¶€ë¶„ì€ 0, ì‹¤ì œ ë°ì´í„°ëŠ” 1)
    seq_mask = (seq_ids != 0).long()
    
    return user_dense, user_cat, seq_ids, seq_deltas, seq_mask
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    '''
    
    '''
    class RerankerDataset(Dataset):
    def __init__(self, interactions_df, processor, max_seq_len=50):
        """
        interactions_df: [user_id, item_id, label, retrieval_score(Optional)]
        """
        self.data = interactions_df
        self.processor = processor
        self.max_len = max_seq_len
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        u_id = row['user_id']
        i_id = row['item_id']
        label = row['label']
        
        # 1. User Features (Dense, Cat, Sequence) - Towerì™€ ë™ì¼
        u_dense, u_cat = self.processor.get_user_tensor([u_id])
        
        seq_row = self.processor.seqs.loc[u_id]
        seq_ids = torch.tensor(seq_row['sequence_ids'][-self.max_len:], dtype=torch.long)
        # Re-rankerì—ì„œëŠ” ë³´í†µ Deltaê¹Œì§€ëŠ” ì•ˆ ì“°ê±°ë‚˜, ì“°ë”ë¼ë„ Attention Mask ìš©ë„ë¡œ ì”€
        
        # 2. Item Features (Dense)
        i_dense = self.processor.get_item_tensor([i_id]) # Velocity, Steady, Price
        
        # 3. Cross Features (Price Gap, Trend Interaction) - â˜… í•µì‹¬
        cross_feats = self.processor.get_cross_features([u_id], [i_id])
        
        # 4. Retrieval Score (Two-Towerì—ì„œ ë‚˜ì˜¨ ì ìˆ˜, ìˆë‹¤ë©´)
        # ret_score = torch.tensor([row['retrieval_score']], dtype=torch.float32)
        
        # 5. GDCNìš© Dense Vector í†µí•© (User Dense + Item Dense + Cross)
        # (B, 2) + (B, 4) + (B, 2) -> (B, 8)
        gdcn_dense_input = torch.cat([u_dense, i_dense, cross_feats], dim=1)
        
        return {
            'gdcn_dense': gdcn_dense_input.squeeze(0), # MLP/CrossNet ì…ë ¥
            'user_cat': u_cat.squeeze(0),              # Embedding ì…ë ¥
            'seq_ids': seq_ids,                        # DIN Attention ì…ë ¥
            'target_item_id': torch.tensor(int(i_id)), # DIN Attention Query
            'label': torch.tensor(label, dtype=torch.float32)
        }

def reranker_collate_fn(batch):
    dense = torch.stack([b['gdcn_dense'] for b in batch])
    cat = torch.stack([b['user_cat'] for b in batch])
    label = torch.stack([b['label'] for b in batch])
    target_item = torch.stack([b['target_item_id'] for b in batch])
    
    seq_ids = pad_sequence([b['seq_ids'] for b in batch], batch_first=True, padding_value=0)
    seq_mask = (seq_ids != 0).long()
    
    return dense, cat, seq_ids, seq_mask, target_item, label
    
    
    '''
    
    '''
    if __name__ == "__main__":
    # 1. Processor ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ë¡œë”©)
    processor = FeatureProcessor(
        user_path="features_user.parquet", 
        item_path="features_item.parquet", 
        seq_path="features_sequence.parquet"
    )
    
    # 2. User Tower í•™ìŠµìš©
    train_users = ["u1", "u2", "u3"] # ì‹¤ì œ ID ë¦¬ìŠ¤íŠ¸
    tower_ds = UserTowerDataset(train_users, processor)
    tower_loader = torch.utils.data.DataLoader(tower_ds, batch_size=32, collate_fn=user_tower_collate_fn)
    
    # 3. Re-ranker í•™ìŠµìš© (Positive + Negative Samples)
    # ì‹¤ì œë¡œëŠ” Retrieval ê²°ê³¼ë‚˜ Random Negativeë¡œ ìƒì„±ëœ DF í•„ìš”
    interaction_data = pd.DataFrame({
        'user_id': ['u1', 'u1', 'u2'],
        'item_id': ['i100', 'i200', 'i100'], # i200ì€ Negative ê°€ì •
        'label': [1, 0, 1]
    })
    
    rerank_ds = RerankerDataset(interaction_data, processor)
    rerank_loader = torch.utils.data.DataLoader(rerank_ds, batch_size=32, collate_fn=reranker_collate_fn)
    
    # Test Output
    for batch in rerank_loader:
        dense, cat, seq, mask, target, lbl = batch
        print("GDCN Input Dense Shape:", dense.shape) # (32, 8) -> 8ê°œ í”¼ì²˜ê°€ í•©ì³ì§
        print("Cross Features included (Price Gap etc.)")
        break
    
    '''