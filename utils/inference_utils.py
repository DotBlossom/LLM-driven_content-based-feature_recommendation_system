import torch
import os
from tqdm import tqdm
from sqlalchemy import select
from torch.utils.data import DataLoader, Dataset

from database import ProductInferenceInput, TrainingItem
from utils.dependencies import get_global_batch_size, get_global_encoder

# =========================================================
# 1. ê³µí†µ ì „ì²˜ë¦¬ í•¨ìˆ˜ (í•™ìŠµ/ì¶”ë¡  ì–‘ìª½ì—ì„œ importí•˜ì—¬ ì‚¬ìš©)
# =========================================================
def parse_db_row(row) -> TrainingItem:
    """
    DB Row(Dictionary)ë¥¼ ë°›ì•„ í•™ìŠµ/ì¶”ë¡ ì— ì‚¬ìš©í•  TrainingItem ê°ì²´ë¡œ ë³€í™˜.
    ì´ í•¨ìˆ˜ í•˜ë‚˜ë¡œ Feature Flatteningê³¼ Name Taggingì„ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    # 1. Feature Data ë³µì‚¬
    raw_feats = dict(row['feature_data'])
    
    # 2. Reinforced Feature Flattening
    if 'reinforced_feature' in raw_feats:
        re_dict = raw_feats['reinforced_feature']
        if isinstance(re_dict, dict):
            for key, val in re_dict.items():
                # Key í¬ë§·íŒ…: "MAT" -> "[MAT]"
                vocab_key = key if key.startswith("[") and key.endswith("]") else f"[{key}]"
                raw_feats[vocab_key] = val

    # 3. Name Tagging Logic
    base_name = row['product_name']
    product_type = raw_feats.get('product_type_name', "").strip()
    
    final_name = ""
    if base_name:
        if product_type:
            final_name = f"{base_name} (Category: {product_type})"
        else:
            final_name = base_name
    else:
        # Fallback: íƒ€ì…ëª… + ì™¸í˜•
        appearance = raw_feats.get('graphical_appearance_name', "").strip()
        final_name = f"{product_type} {appearance}".strip()
        if not final_name:
            final_name = "Unknown Product"

    return TrainingItem(
        product_id=str(row['product_id']), 
        feature_data=raw_feats, 
        product_name=final_name
    )

# =========================================================
# 2. Inference Dataset & Utils
# =========================================================
class InferenceDataset(Dataset):
    def __init__(self, products):
        self.products = products
        
    def __len__(self):
        return len(self.products)

    def __getitem__(self, idx):
        return self.products[idx]

import os
import torch
from torch.utils.data import DataLoader, Dataset
from sqlalchemy import select
from tqdm import tqdm

# ... (parse_db_row, InferenceDataset ë“±ì˜ ìœ„ìª½ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€) ...

def generate_and_save_item_vectors(
    db_session, 
    save_dir="models", 
    safe_mode=False, 
    checkpoint_path: str = None  # ğŸ‘ˆ [New] ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì¸ì ì¶”ê°€
):
    """
    checkpoint_path: íŠ¹ì • .pth íŒŒì¼ì„ ì§€ì •í•˜ë©´ ê·¸ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
                     Noneì´ë©´ í˜„ì¬ get_global_encoder()ì— ë¡œë“œëœ ìƒíƒœ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    save_tensor_path = os.path.join(save_dir, "pretrained_item_matrix.pt")
    save_ids_path = os.path.join(save_dir, "item_ids.pt") 
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ Target Device: {device}")

    # 1. ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°€ì ¸ì˜¤ê¸°
    try:
        # ê»ë°ê¸°(ì•„í‚¤í…ì²˜)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. 
        # (ë§Œì•½ get_global_encoderê°€ ì‹±ê¸€í†¤ì´ë¼ë„, load_state_dictë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë°”ê¾¸ë©´ ì˜í–¥ì´ ê°ˆ ìˆ˜ ìˆìœ¼ë‹ˆ
        #  ì•ˆì „í•˜ê²Œ í•˜ë ¤ë©´ ìƒˆë¡œ ìƒì„±í•˜ëŠ” ê²ƒì´ ì¢‹ìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ê°€ì ¸ì™€ì„œ ë®ì–´ì”Œì›ë‹ˆë‹¤.)
        model = get_global_encoder()
        
        # ğŸŒŸ [í•µì‹¬] ì§€ì •ëœ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆìœ¼ë©´ ê°€ì¤‘ì¹˜ ë¡œë“œ
        if checkpoint_path:
            if os.path.exists(checkpoint_path):
                print(f"â™»ï¸ Loading weights from Checkpoint: {checkpoint_path}")
                # map_locationìœ¼ë¡œ ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„± í™•ë³´
                state_dict = torch.load(checkpoint_path, map_location=device)
                
                # ëª¨ë¸ì— ê°€ì¤‘ì¹˜ ë®ì–´ì”Œìš°ê¸° (strict=FalseëŠ” í˜¹ì‹œ ëª¨ë¥¼ ë¯¸ì„¸í•œ í‚¤ ë¶ˆì¼ì¹˜ ë¬´ì‹œìš©, ë³´í†µì€ True ê¶Œì¥)
                model.load_state_dict(state_dict, strict=True)
                print("âœ… Successfully loaded checkpoint weights!")
            else:
                print(f"âŒ [Error] Checkpoint path not found: {checkpoint_path}")
                return None, None
        else:
            print("âš ï¸ No checkpoint_path provided. Using current model weights.")

        model = model.to(device)
        model.eval() # ì¶”ë¡  ëª¨ë“œ í•„ìˆ˜

    except Exception as e:
        print(f"âŒ Model Setup Failed: {e}")
        return None, None

    # 2. DB ë°ì´í„° ë¡œë“œ
    print("ğŸš€ Fetching ALL products from DB for Inference...")
    stmt = select(
        ProductInferenceInput.product_id, 
        ProductInferenceInput.feature_data, 
        ProductInferenceInput.product_name 
    )
    result = db_session.execute(stmt).mappings().all()
    
    if not result:
        print("âŒ No products found in DB.")
        return None, None

    inference_items = [parse_db_row(row) for row in result]
    # ID ìˆœìœ¼ë¡œ ì •ë ¬ (ë‚˜ì¤‘ì— ì°¾ê¸° ì‰½ê²Œ)
    inference_items.sort(key=lambda x: x.product_id)
    ordered_ids = [item.product_id for item in inference_items]
    
    print(f"âœ… Prepared {len(inference_items)} items for vector extraction.")

    # 3. DataLoader ì„¤ì •
    dataset = InferenceDataset(inference_items)
    
    # Collator ê°€ì ¸ì˜¤ê¸° (ì „ì—­ í•¨ìˆ˜ í˜¹ì€ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤)
    from item_tower import SimCSECollator
    collator_instance = SimCSECollator()

    def inference_collate_fn(batch):
        # is_first_view=True ì˜µì…˜ìœ¼ë¡œ 1ê°œì˜ ë·°ë§Œ ìƒì„±
        return collator_instance.process_batch_items(batch, is_first_view=True)

    batch_size = get_global_batch_size()
    dataloader = DataLoader(
        dataset, 
        # ì•ˆì „ ëª¨ë“œë©´ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì„
        batch_size=batch_size * 4 if not safe_mode else batch_size, 
        shuffle=False, 
        collate_fn=inference_collate_fn,
        num_workers=0
    )

    # 4. Inference (Full GPU or Safe CPU)
    all_vectors = []
    
    print(f"âš¡ Starting Vector Extraction (Mode: {'Safe/CPU-bound' if safe_mode else 'Fast/GPU-bound'})...")
    
    try:
        with torch.no_grad():
            for batch_inputs in tqdm(dataloader):
                # batch_inputsëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ [input_ids, masks, ...]
                inputs = [t.to(device) for t in batch_inputs]
                
                # Forward
                vectors = model(*inputs) # (Batch, 128)
                
                if safe_mode:
                    # [ì•ˆì „ ëª¨ë“œ] ì¦‰ì‹œ CPUë¡œ ë‚´ë¦¼ (VRAM ì ˆì•½)
                    all_vectors.append(vectors.cpu())
                else:
                    # [ê³ ì† ëª¨ë“œ] GPUì— ë‘ 
                    all_vectors.append(vectors)
        
        # 5. Merge & Save
        print("ğŸ§© Merging tensors...")
        if safe_mode:
            final_tensor = torch.cat(all_vectors, dim=0)
        else:
            final_tensor = torch.cat(all_vectors, dim=0).cpu() # ë§ˆì§€ë§‰ì— í•œ ë²ˆì— ë‚´ë¦¼

    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("ğŸš¨ [OOM Error] GPU Memory Full! Retry with 'safe_mode=True'.")
            torch.cuda.empty_cache()
            return None, None
        raise e

    # 6. íŒŒì¼ ì €ì¥
    os.makedirs(save_dir, exist_ok=True)
    
    # í…ì„œ ì €ì¥
    torch.save(final_tensor, save_tensor_path)
    # ID ë¦¬ìŠ¤íŠ¸ ì €ì¥
    torch.save(ordered_ids, save_ids_path)

    print(f"ğŸ’¾ Saved Vectors: {final_tensor.shape} -> {save_tensor_path}")
    print(f"ğŸ’¾ Saved IDs: {len(ordered_ids)} -> {save_ids_path}")
    
    return final_tensor, ordered_ids