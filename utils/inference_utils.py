# inference_utils.py (ë˜ëŠ” ì ì ˆí•œ ìœ„ì¹˜)

from typing import Any, Dict
from pydantic import BaseModel
from torch.utils.data import Dataset, DataLoader
from database import ProductInferenceInput # DB ìŠ¤í‚¤ë§ˆ
from main import get_global_encoder, get_global_batch_size # ì˜ì¡´ì„± ì£¼ì… í•¨ìˆ˜ ì„í¬íŠ¸
from utils import vocab # vocab ê´€ë ¨ ìœ í‹¸
import torch
import os
from tqdm import tqdm
from sqlalchemy import select

# 1. ì¶”ë¡ ìš© ë°ì´í„°ì…‹ (No Dropout, No Corruption)
class InferenceDataset(Dataset):
    def __init__(self, products):
        self.products = products # List[TrainingItem]
        
    def __len__(self):
        return len(self.products)

    def __getitem__(self, idx):
        # í•™ìŠµ ë•Œì™€ ë‹¬ë¦¬ ë°ì´í„°ë¥¼ ë³µì œí•˜ê±°ë‚˜ ë³€í˜•í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë°˜í™˜
        return self.products[idx]

class TrainingItem(BaseModel):
    product_id: str
    feature_data: Dict[str, Any] # DBì—ì„œ ê¸ì–´ì˜¨ Raw JSON
    product_name: str            # Text Embeddingìš©

# í•„ìš”ì— ë§ëŠ” ë²”ì£¼(train, valid, test, real)ì— ë§ê²Œ Item set ê´€ë¦¬. í›„ case ë§ê²Œ loading
# 2. ë²¡í„° ì¶”ì¶œ í•¨ìˆ˜
def generate_and_save_item_vectors(db_session, save_path="models/pretrained_item_matrix.pt"):
    """
    ì „ì—­ ë©”ëª¨ë¦¬ì— ë¡œë“œëœ Item Towerë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ì•„ì´í…œ ë²¡í„°ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # ---------------------------------------------------------
    # A. ì „ì—­ ëª¨ë¸ ë° ì„¤ì • ê°€ì ¸ì˜¤ê¸° (Dependency Injection)
    # ---------------------------------------------------------
    try:
        model = get_global_encoder()     # ì´ë¯¸ ë¡œë“œëœ HybridItemTower ì¸ìŠ¤í„´ìŠ¤
        batch_size = get_global_batch_size()
        device = next(model.parameters()).device # ëª¨ë¸ì´ ìˆëŠ” ì¥ì¹˜(cuda/cpu) í™•ì¸
        print(f"âœ… Loaded Global Encoder on {device}")
    except Exception as e:
        print(f"âŒ Global Model Load Failed: {e}")
        return

    # ---------------------------------------------------------
    # B. DBì—ì„œ ì „ì²´ ì•„ì´í…œ ë¡œë“œ
    # Queryì˜ ëŒ€ìƒì´ ë˜ëŠ” Loaderë¥¼ ë¯¸ë¦¬ ì„¤ì • 
    # ---------------------------------------------------------
    print("ğŸš€ Fetching ALL products from DB for Inference...")
    stmt = select(
        ProductInferenceInput.product_id, 
        ProductInferenceInput.feature_data, 
        ProductInferenceInput.product_name 
    )
    result = db_session.execute(stmt).mappings().all()
    
    if not result:
        print("âŒ No items found in DB.")
        return

    # ë°ì´í„° ë³€í™˜ (TrainingItem ê°ì²´ë¡œ)
    # (í•™ìŠµ ì½”ë“œì˜ ì „ì²˜ë¦¬ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€ - RE Flattening, Name Tagging ë“±)
    inference_items = []
    for row in result:
        raw_feats = dict(row['feature_data'])
        
        # [ì¤‘ìš”] í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬ ë¡œì§ ë³µì‚¬ (í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ëŠ”ê²Œ ì¢‹ìŒ)
        if 'reinforced_feature' in raw_feats:
            re_dict = raw_feats['reinforced_feature']
            if isinstance(re_dict, dict):
                for key, val in re_dict.items():
                    k = key if key.startswith("[") else f"[{key}]"
                    raw_feats[k] = val
        
        # Name Tagging
        base_name = row['product_name'] or ""
        p_type = raw_feats.get('product_type_name', "").strip()
        final_name = f"{base_name} (Category: {p_type})" if base_name and p_type else base_name
        
        inference_items.append(TrainingItem(
            product_id=str(row['product_id']),
            feature_data=raw_feats,
            product_name=final_name
        ))

    print(f"âœ… Prepared {len(inference_items)} items for inference.")

    # ---------------------------------------------------------
    # C. DataLoader ì¤€ë¹„
    # ---------------------------------------------------------
    dataset = InferenceDataset(inference_items)
    
    # ê¸°ì¡´ SimCSECollator í™œìš©
    # ì£¼ì˜: SimCSECollatorëŠ” í´ë˜ìŠ¤ì´ë¯€ë¡œ ì¸ìŠ¤í„´ìŠ¤í™” í•„ìš”
    from item_tower import SimCSECollator # (Collatorê°€ ì •ì˜ëœ íŒŒì¼ì—ì„œ ì„í¬íŠ¸)
    collator_instance = SimCSECollator() 

    # ì»¤ìŠ¤í…€ Collate í•¨ìˆ˜: ë‹¨ì¼ ë·°(View)ë§Œ ì²˜ë¦¬í•˜ë„ë¡ ë˜í•‘
    def inference_collate_fn(batch):
        # batchëŠ” List[TrainingItem]
        # SimCSECollatorì˜ process_batch_items ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ
        return collator_instance.process_batch_items(batch, is_first_view=True)

    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size * 2, # ì¶”ë¡ ì€ ì—­ì „íŒŒê°€ ì—†ìœ¼ë¯€ë¡œ ë°°ì¹˜ë¥¼ 2ë°°ë¡œ í‚¤ì›Œë„ ë¨
        shuffle=False,             # ìˆœì„œëŒ€ë¡œ ë½‘ì•„ì•¼ ID ë§¤í•‘ì´ ì‰¬ì›€
        collate_fn=inference_collate_fn,
        num_workers=0
    )

    # ---------------------------------------------------------
    # D. Inference Loop
    # ---------------------------------------------------------
    all_vectors = []
    all_product_ids = [] # ë‚˜ì¤‘ì— ID ë§¤í•‘ì„ ìœ„í•´ ì €ì¥
    
    model.eval() # ğŸš¨ í•„ìˆ˜: Dropout ë¹„í™œì„±í™”

    print("âš¡ Starting Vector Extraction...")
    
    with torch.no_grad(): # ğŸš¨ í•„ìˆ˜: Gradient ê³„ì‚° ë„ê¸° (ì†ë„/ë©”ëª¨ë¦¬ ìµœì í™”)
        for batch_inputs in tqdm(dataloader):
            # batch_inputsëŠ” (std, re_ids, re_mask, txt_ids, txt_mask) íŠœí”Œ
            inputs = [t.to(device) for t in batch_inputs]
            
            # ëª¨ë¸ í†µê³¼ (HybridItemTower.forward)
            # ê²°ê³¼: (Batch, 128)
            vectors = model(*inputs)
            
            # CPUë¡œ ë‚´ë ¤ì„œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            all_vectors.append(vectors.cpu())
            
            # í˜„ì¬ ë°°ì¹˜ì˜ Product ID ì¶”ì  (í•„ìš” ì‹œ)
            # (DataLoaderì˜ batch ìˆœì„œì™€ dataset ìˆœì„œê°€ ê°™ìœ¼ë¯€ë¡œ ë³„ë„ ì²˜ë¦¬ ì—†ì–´ë„ ë¨)

    # ---------------------------------------------------------
    # E. ë³‘í•© ë° ì €ì¥
    # ---------------------------------------------------------
    # (Num_Items, 128)
    final_tensor = torch.cat(all_vectors, dim=0)
    
    # í´ë” í™•ë³´
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    torch.save(final_tensor, save_path)
    print(f"ğŸ’¾ Lookup Table Saved: {save_path}")
    print(f"   - Shape: {final_tensor.shape}")
    print(f"   - Device: {final_tensor.device} (Should be cpu)")
    
    return final_tensor