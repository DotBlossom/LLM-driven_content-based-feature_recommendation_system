import json
import os
import torch
import numpy as np
from sentence_transformers import SentenceTransformer, util
from tqdm import tqdm

# ==========================================
# 1. ì„¤ì • ë° ê²½ë¡œ
# ==========================================
BASE_DIR = r"C:\Users\candyform\Desktop\inferenceCode\localprops"
DATA_DIR = os.path.join(BASE_DIR, "results")

# ìˆœì„œê°€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •í•˜ëŠ” ë‘ íŒŒì¼
FILE_A_PATH = os.path.join(BASE_DIR, "articles_detail_desc.json")      # ì›ë³¸ ë¬¸ì¥
FILE_B_PATH = os.path.join(DATA_DIR, "final_ordered_result.json")     # ì •ë ¬ëœ ê²°ê³¼ê°’

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'


BATCH_SIZE = 512 

# ==========================================
# 2. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
# ==========================================
def load_and_extract_text():
    print("Step 1: íŒŒì¼ ë¡œë”© ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
    
    # File A (Sentences) ë¡œë“œ
    with open(FILE_A_PATH, 'r', encoding='utf-8') as f:
        data_a = json.load(f)
    
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ (dictë©´ 'text' or 'caption', ì•„ë‹ˆë©´ str)
    list_a = []
    for item in data_a:
        if isinstance(item, dict):
            list_a.append(item.get("text", "") or item.get("caption", ""))
        else:
            list_a.append(str(item))

    # File B (Results) ë¡œë“œ
    with open(FILE_B_PATH, 'r', encoding='utf-8') as f:
        data_b = json.load(f)
        
    # í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì†ì„±ê°’ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©)
    list_b = []
    for item in data_b:
        # text, score ê°™ì€ ë©”íƒ€ë°ì´í„° ì œì™¸í•˜ê³  ì†ì„±ê°’ë§Œ ëª¨ìŒ
        tokens = []
        for k, v in item.items():
            if k not in ['text', 'similarity_score', 'key_correct']:
                if isinstance(v, list):
                    tokens.extend([str(x) for x in v])
        list_b.append(" ".join(tokens))

    # ê°œìˆ˜ ì²´í¬
    if len(list_a) != len(list_b):
        print(f"âš ï¸ ê²½ê³ : ê°œìˆ˜ ë¶ˆì¼ì¹˜! (A: {len(list_a)}, B: {len(list_b)})")
        min_len = min(len(list_a), len(list_b))
        list_a = list_a[:min_len]
        list_b = list_b[:min_len]
        print(f"   -> {min_len}ê°œ ê¸°ì¤€ìœ¼ë¡œ 1:1 ë¹„êµë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    else:
        print(f"âœ… ê°œìˆ˜ ì¼ì¹˜ í™•ì¸: {len(list_a)}ê±´")
        
    return list_a, list_b

# ==========================================
# 3. ê³ ì† ë°°ì¹˜ ë¹„êµ ë¡œì§
# ==========================================
def run_fast_verification():
    # ë°ì´í„° ì¤€ë¹„
    texts_a, texts_b = load_and_extract_text()
    
    print(f"Step 2: ëª¨ë¸ ë¡œë”© ({DEVICE})...")
    model = SentenceTransformer('all-MiniLM-L6-v2', device=DEVICE)
    
    all_scores = []
    total_len = len(texts_a)
    
    print(f"Step 3: ë°°ì¹˜ ë‹¨ìœ„ GPU ê²€ì¦ ì‹œì‘ (Batch: {BATCH_SIZE})...")
    
    # tqdmìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
    for i in tqdm(range(0, total_len, BATCH_SIZE), desc="Verifying"):
        end_i = min(i + BATCH_SIZE, total_len)
        
        batch_a = texts_a[i:end_i]
        batch_b = texts_b[i:end_i]
        
        # 1. ì„ë² ë”© (Encoding) - GPUë¡œ ë°”ë¡œ í…ì„œ ìƒì„±
        emb_a = model.encode(batch_a, convert_to_tensor=True, show_progress_bar=False)
        emb_b = model.encode(batch_b, convert_to_tensor=True, show_progress_bar=False)
        
        # 2. 1:1 ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (Pairwise Cosine Similarity)
        # pairwise_cos_sim
        scores = util.pairwise_cos_sim(emb_a, emb_b)
        
        # 3. ê²°ê³¼ ìˆ˜ì§‘ (GPU -> CPU)
        all_scores.extend(scores.cpu().tolist())

    # ==========================================
    # 4. ê²°ê³¼ ë¦¬í¬íŠ¸
    # ==========================================
    scores_np = np.array(all_scores)
    
    print("\n" + "="*50)
    print("ğŸš€ FAST VERIFICATION REPORT")
    print("="*50)
    print(f"Total Pairs : {len(scores_np):,}")
    print(f"Average Sim : {np.mean(scores_np):.4f}")
    print(f"Median  Sim : {np.median(scores_np):.4f}")
    print(f"Min Score   : {np.min(scores_np):.4f}")
    print("-" * 50)
    
    # ì ìˆ˜ëŒ€ë³„ ë¶„í¬
    count_high = np.sum(scores_np >= 0.7)
    count_mid = np.sum((scores_np >= 0.5) & (scores_np < 0.7))
    count_low = np.sum(scores_np < 0.5)
    
    print(f"âœ… High Match (>= 0.7) : {count_high:,} ({count_high/len(scores_np)*100:.1f}%)")
    print(f"âš ï¸ Mid Match  (0.5~0.7): {count_mid:,} ({count_mid/len(scores_np)*100:.1f}%)")
    print(f"âŒ Low Match  (< 0.5)  : {count_low:,} ({count_low/len(scores_np)*100:.1f}%)")
    print("="*50)

    # (ì˜µì…˜) ë¬¸ì œê°€ ë˜ëŠ” ì¸ë±ìŠ¤, lowerbound ì§ì ‘ ëˆˆìœ¼ë¡œ ë³´ê³  íŒë‹¨
    if count_low > 0:
        print("\nğŸ” ë¶ˆì¼ì¹˜ ì˜ì‹¬ ìƒìœ„ 3ê°œ (Low Score Examples):")
        worst_indices = np.argsort(scores_np)[:3]
        for idx in worst_indices:
            
            print(f"[Row {idx}] Score: {scores_np[idx]:.4f}")
            print(f"  A: {texts_a[idx][:100]}...") # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
            print(f"  B: {texts_b[idx][:100]}...")
            print("-" * 30)

if __name__ == "__main__":
    
    run_fast_verification()