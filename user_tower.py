import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from tqdm import tqdm
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# -------------------------------------------------------------------------
# 0. Global Configuration
# -------------------------------------------------------------------------
EMBED_DIM = 128
MAX_SEQ_LEN = 50
DROPOUT = 0.1
GNN_DIM = 64
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ê²½ë¡œ ì„¤ì • (preprocess_final.py ê²°ê³¼ë¬¼ ìœ„ì¹˜)
BASE_DIR = r"D:\trainDataset\localprops"
MODEL_DIR = os.path.join(BASE_DIR, "models")

ITEM_FEAT_PATH_PQ = os.path.join(BASE_DIR, "features_item.parquet")
USER_FEAT_PATH_PQ = os.path.join(BASE_DIR, "features_user.parquet")
SEQ_DATA_PATH_PQ = os.path.join(BASE_DIR, "features_sequence.parquet")

# Pre-trained Weights Paths
GNN_PATH = os.path.join(MODEL_DIR, "1simgcl_trained.pth")
ITEM_MATRIX_PATH = os.path.join(MODEL_DIR, "1pretrained_item_matrix.pt")

# -------------------------------------------------------------------------
# 1. Feature Processor & Dataset (Data Loading)
# -------------------------------------------------------------------------
class FeatureProcessor:
    def __init__(self, user_path, item_path, seq_path):
        print("ğŸ”„ Loading & Scaling Features...")
        
        # Load Parquet
        self.users = pd.read_parquet(user_path).set_index('customer_id')
        self.items = pd.read_parquet(item_path).set_index('article_id')
        self.seqs = pd.read_parquet(seq_path).set_index('customer_id')
        
        # User ID Mapping (String -> Int for Embedding Lookup)
        self.user_ids = self.users.index.tolist()
        self.user2id = {uid: i for i, uid in enumerate(self.user_ids)}
        
        # Item ID Mapping (String -> Int)
        self.item_ids = self.items.index.tolist()
        self.item2id = {iid: i for i, iid in enumerate(self.item_ids)}
        
        # Scalers
        self.user_scaler = StandardScaler()
        self.u_dense_cols = ['user_avg_price_log', 'total_cnt_log', 'recency_log']
        
        # Scaling Apply
        self.users_scaled = self.users.copy()
        self.users_scaled[self.u_dense_cols] = self.user_scaler.fit_transform(self.users[self.u_dense_cols])
        
        print("âœ… Features processed successfully.")

    def get_user_tensor(self, user_id):
        # Dense Features (3 dims)
        dense_vals = self.users_scaled.loc[user_id, self.u_dense_cols].values
        dense = torch.tensor(dense_vals, dtype=torch.float32)
        
        # Cat Feature (Preferred Channel: 1,2 -> 0,1)
        cat_val = self.users_scaled.loc[user_id, 'preferred_channel']
        cat = torch.tensor(int(cat_val) - 1, dtype=torch.long)
        
        return dense, cat

    def get_logq_probs(self, device):
        """ LogQ Correctionì„ ìœ„í•œ Log Probability Tensor ìƒì„± """
        # raw_probability ì»¬ëŸ¼ì„ ê°€ì ¸ì™€ì„œ item2id ìˆœì„œëŒ€ë¡œ ì •ë ¬
        probs = np.zeros(len(self.item_ids), dtype=np.float32)
        
        # article_idê°€ ì¸ë±ìŠ¤ì´ë¯€ë¡œ ìˆœíšŒí•˜ë©° ë§¤í•‘
        # (ë” ë¹ ë¥¸ ë°©ë²•: reindex ì‚¬ìš©)
        sorted_probs = self.items['raw_probability'].reindex(self.item_ids).fillna(0).values
        
        return torch.tensor(sorted_probs, dtype=torch.float32).to(device)

class UserTowerDataset(Dataset):
    def __init__(self, processor, max_seq_len=50):
        self.processor = processor
        self.user_ids = processor.user_ids # List of Strings
        self.max_len = max_seq_len
        
    def __len__(self):
        return len(self.user_ids)
    
    def __getitem__(self, idx):
        u_id_str = self.user_ids[idx]
        u_idx_int = idx # processor.user_ids ìˆœì„œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
        # 1. User Features
        u_dense, u_cat = self.processor.get_user_tensor(u_id_str)
        
        # 2. Sequence Data
        seq_ids = []
        seq_deltas = []
        
        if u_id_str in self.processor.seqs.index:
            seq_row = self.processor.seqs.loc[u_id_str]
            # String Item IDs -> Integer IDs ë³€í™˜
            raw_seq_ids = seq_row['sequence_ids'][-self.max_len:]
            seq_ids = [self.processor.item2id.get(i, 0) for i in raw_seq_ids]
            seq_deltas = seq_row['sequence_deltas'][-self.max_len:]
        
        return {
            'user_idx': torch.tensor(u_idx_int, dtype=torch.long),
            'user_dense': u_dense,       # (3,)
            'user_cat': u_cat,           # (1,)
            'seq_ids': torch.tensor(seq_ids, dtype=torch.long),
            'seq_deltas': torch.tensor(seq_deltas, dtype=torch.long)
        }

def user_tower_collate_fn(batch):
    user_idx = torch.stack([b['user_idx'] for b in batch])
    user_dense = torch.stack([b['user_dense'] for b in batch])
    user_cat = torch.stack([b['user_cat'] for b in batch])
    
    # Padding (Padding Value = 0)
    seq_ids = pad_sequence([b['seq_ids'] for b in batch], batch_first=True, padding_value=0)
    seq_deltas = pad_sequence([b['seq_deltas'] for b in batch], batch_first=True, padding_value=0)
    
    # Mask (0 is padding)
    seq_mask = (seq_ids != 0).long()
    
    return user_idx, user_dense, user_cat, seq_ids, seq_deltas, seq_mask

# -------------------------------------------------------------------------
# 2. Model Architecture (Updated)
# -------------------------------------------------------------------------
class ContextGatingFusion(nn.Module):
    def __init__(self, dim=128):
        super().__init__()
        # ì…ë ¥: GNN(128) + Seq(128) + Meta(128) = 384
        self.gate_mlp = nn.Sequential(
            nn.Linear(dim * 3, dim * 3), 
            nn.ReLU(),
            nn.Linear(dim * 3, dim * 3), 
            nn.Sigmoid() 
        )
        self.output_proj = nn.Sequential(
            nn.Linear(dim, dim),
            nn.LayerNorm(dim)
        )

    def forward(self, v_gnn, v_seq, v_meta):
        combined = torch.cat([v_gnn, v_seq, v_meta], dim=1) 
        all_gates = self.gate_mlp(combined) 
        
        dim = v_gnn.shape[1]
        g_gnn = all_gates[:, :dim]
        g_seq = all_gates[:, dim:2*dim]
        g_meta = all_gates[:, 2*dim:]
        
        v_fused = (v_gnn * g_gnn) + (v_seq * g_seq) + (v_meta * g_meta)
        return self.output_proj(v_fused)

class HybridUserTower(nn.Module):
    def __init__(self, 
                 num_users, 
                 num_items,
                 pretrained_gnn_embeddings=None, 
                 pretrained_item_vectors=None):
        super().__init__()
        
        # A. GNN Part
        if pretrained_gnn_embeddings is not None:
            self.gnn_user_emb = nn.Embedding.from_pretrained(pretrained_gnn_embeddings, freeze=False)
            current_gnn_dim = pretrained_gnn_embeddings.shape[1]
        else:
            current_gnn_dim = GNN_DIM
            self.gnn_user_emb = nn.Embedding(num_users, current_gnn_dim)
            nn.init.xavier_normal_(self.gnn_user_emb.weight)

        self.gnn_projector = nn.Sequential(
            nn.Linear(current_gnn_dim, EMBED_DIM),
            nn.LayerNorm(EMBED_DIM),
            nn.GELU()
        )

        # B. Sequential Part
        if pretrained_item_vectors is not None:
            # freeze=True for Transfer Learning Stability
            self.item_content_emb = nn.Embedding.from_pretrained(pretrained_item_vectors, freeze=True)
        else:
            self.item_content_emb = nn.Embedding(num_items, EMBED_DIM)

        # Time Embedding (Bucketized)
        # 0~1000ì¼ê¹Œì§€ì˜ Time Deltaë¥¼ ì„ë² ë”©
        self.time_emb = nn.Embedding(1001, EMBED_DIM) 
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=EMBED_DIM, nhead=4, dim_feedforward=EMBED_DIM*4, dropout=DROPOUT, batch_first=True, norm_first=True
        )
        self.seq_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)

        # C. Meta Part (Updated)
        # Input: Dense(3) + Cat Emb(128)
        self.channel_emb = nn.Embedding(2, 32) # Channel 0 or 1 -> 32 dim
        
        # 3 (Dense) + 32 (Cat) = 35
        self.meta_mlp = nn.Sequential(
            nn.Linear(3 + 32, EMBED_DIM),
            nn.GELU(),
            nn.Linear(EMBED_DIM, EMBED_DIM),
            nn.LayerNorm(EMBED_DIM)
        )

        # D. Fusion
        self.fusion_layer = ContextGatingFusion(dim=EMBED_DIM)

    def forward(self, user_indices, seq_ids, seq_deltas, seq_mask, user_dense, user_cat):
        # 1. GNN Representation
        v_gnn = self.gnn_projector(self.gnn_user_emb(user_indices))

        # 2. Sequential Representation
        seq_emb = self.item_content_emb(seq_ids)
        
        # Time Embedding (Bucketize: 1000ì¼ ë„˜ì–´ê°€ë©´ 1000ìœ¼ë¡œ í´ë¦¬í•‘)
        deltas = seq_deltas.clamp(max=1000)
        time_emb = self.time_emb(deltas)
        
        seq_input = seq_emb + time_emb
        
        # Transformer
        key_padding_mask = (seq_mask == 0)
        seq_out = self.seq_encoder(seq_input, src_key_padding_mask=key_padding_mask)
        
        # Attention Pooling (Last Valid Token or Mean)
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ Masked Mean Pooling
        mask_expanded = seq_mask.unsqueeze(-1)
        sum_seq = (seq_out * mask_expanded).sum(dim=1)
        cnt_seq = mask_expanded.sum(dim=1).clamp(min=1e-9)
        v_seq = sum_seq / cnt_seq

        # 3. Meta Representation
        cat_vec = self.channel_emb(user_cat) # (B, 32)
        meta_input = torch.cat([user_dense, cat_vec], dim=1) # (B, 35)
        v_meta = self.meta_mlp(meta_input)

        # 4. Fusion
        output = self.fusion_layer(v_gnn, v_seq, v_meta)
        
        return F.normalize(output, p=2, dim=1)

# -------------------------------------------------------------------------
# 3. LogQ Loss Function
# -------------------------------------------------------------------------
def logq_correction_loss(user_emb, item_emb, pos_item_ids, item_log_probs, temperature=0.07, lambda_logq=0.5):
    """
    LogQ Correctionì„ ì ìš©í•œ Contrastive Loss (Sampled Softmax)
    """
    # 1. Batch ë‚´ì˜ Positive/Negative Score ê³„ì‚° (In-batch Negative)
    # user_emb: (B, Dim)
    # item_emb: (B, Dim) -> ì—¬ê¸°ì„œ item_embëŠ” Batch ë‚´ ìœ ì €ë“¤ì´ ë‹¤ìŒ ì‹œì ì— êµ¬ë§¤í•œ 'ì •ë‹µ ì•„ì´í…œ'ë“¤ì˜ ì„ë² ë”©
    
    # logits: (B, B) -> ëŒ€ê°ì„ ì´ ì •ë‹µ(Positive), ë‚˜ë¨¸ì§€ëŠ” Negative
    logits = torch.matmul(user_emb, item_emb.T) 
    logits = logits / temperature
    
    # 2. LogQ Correction
    # ë°°ì¹˜ì— í¬í•¨ëœ ì•„ì´í…œë“¤ì˜ ì¸ê¸°ë„ í™•ë¥  ê°€ì ¸ì˜¤ê¸°
    # pos_item_ids: (B,)
    batch_log_probs = torch.log(item_log_probs[pos_item_ids] + 1e-9) # (B,)
    
    # Correction: logitsì—ì„œ log(P)ë¥¼ ëºŒ
    # ì¸ê¸° ì•„ì´í…œì¼ìˆ˜ë¡ Pê°€ í¬ê³  log(P)ê°€ í¼ -> Logitsê°€ ë§ì´ ê¹ì„ (í˜ë„í‹°)
    # Broadcasting: (1, B) í˜•íƒœë¡œ ë¹¼ì¤Œ (ê° ì•„ì´í…œ(Column)ì— ëŒ€í•´ ë³´ì •)
    correction = batch_log_probs.unsqueeze(0) 
    
    corrected_logits = logits - (lambda_logq * correction)
    
    # 3. Cross Entropy
    labels = torch.arange(logits.size(0)).to(logits.device)
    loss = F.cross_entropy(corrected_logits, labels)
    
    return loss

# -------------------------------------------------------------------------
# 4. Main Training Routine
# -------------------------------------------------------------------------
def train_user_tower():
    # 1. Load Data & Processor
    processor = FeatureProcessor(USER_FEAT_PATH_PQ, ITEM_FEAT_PATH_PQ, SEQ_DATA_PATH_PQ)
    
    dataset = UserTowerDataset(processor, MAX_SEQ_LEN)
    dataloader = DataLoader(dataset, batch_size=256, shuffle=True, collate_fn=user_tower_collate_fn, num_workers=0)

    # 2. Load Assets (GNN, Item Matrix)
    # ì‹¤ì œë¡œëŠ” load_pretrained_assets() í•¨ìˆ˜ ì‚¬ìš©. ì—¬ê¸°ì„œëŠ” ê°€ì •.
    # item_tensor: (Num_Items, 128)
    item_tensor = torch.load(ITEM_MATRIX_PATH, map_location='cpu') if os.path.exists(ITEM_MATRIX_PATH) else torch.randn(len(processor.item_ids), 128)
    
    # 3. Model Init
    model = HybridUserTower(
        num_users=len(processor.user_ids),
        num_items=len(processor.item_ids),
        pretrained_gnn_embeddings=None, # GNN íŒŒì¼ ìˆìœ¼ë©´ ë¡œë“œí•´ì„œ ë„£ê¸°
        pretrained_item_vectors=item_tensor
    ).to(DEVICE)

    # 4. Setup Optimization
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    
    # LogQìš© í™•ë¥  í…ì„œ ë¡œë“œ
    item_log_probs = processor.get_logq_probs(DEVICE)
    
    # Target Item Lookup (ì •ë‹µ ë¹„êµìš©, Gradient X)
    target_lookup = nn.Embedding.from_pretrained(item_tensor, freeze=True).to(DEVICE)

    print(f"\nğŸš€ Start Training User Tower (LogQ Corrected)...")
    
    for epoch in range(5):
        model.train()
        total_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
        
        for batch in pbar:
            # Unpack Batch
            u_idx, u_dense, u_cat, seq_ids, seq_deltas, seq_mask = [x.to(DEVICE) for x in batch]
            
            optimizer.zero_grad()
            
            # Forward
            user_vec = model(u_idx, seq_ids, seq_deltas, seq_mask, u_dense, u_cat)
            
            # Target (Next Item Prediction) - ì—¬ê¸°ì„œëŠ” ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ë‹¤ìŒ ì•„ì´í…œì„ ì˜ˆì¸¡í•´ì•¼ í•¨.
            # í•˜ì§€ë§Œ í˜„ì¬ ë°ì´í„°ì…‹ì—” 'Target Item'ì´ ëª…ì‹œì ìœ¼ë¡œ ì—†ìŒ (User Seqë§Œ ìˆìŒ).
            # [ìˆ˜ì •] í•™ìŠµì„ ìœ„í•´ì„  'ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ì•„ì´í…œ'ì„ ì •ë‹µ(Target)ìœ¼ë¡œ ì“°ê³ , 
            # ì…ë ¥ì„ 'ë§ˆì§€ë§‰ ì œì™¸(t-1)'ê¹Œì§€ë¡œ í•˜ëŠ” Self-Supervised ë°©ì‹ì„ ì“°ê±°ë‚˜,
            # ë°ì´í„°ì…‹ì— Target Item ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•¨.
            
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ì‹œí€€ìŠ¤ì˜ 'ë§ˆì§€ë§‰ ì•„ì´í…œ'ì„ ì •ë‹µìœ¼ë¡œ ê°„ì£¼í•˜ê³ , ì…ë ¥ì—ì„œ ë§ˆìŠ¤í‚¹í•˜ëŠ” ë°©ì‹ ì‚¬ìš©
            # ì‹¤ì œë¡œëŠ” Dataset __getitem__ ì—ì„œ target_item_idë¥¼ ë±‰ì–´ì£¼ëŠ” ê²Œ ë§ìŒ.
            # ì½”ë“œê°€ ê¸¸ì–´ì§€ë¯€ë¡œ, í˜„ì¬ ë°°ì¹˜ì˜ seq_idsì˜ ë§ˆì§€ë§‰ ê°’ì„ Targetìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤.
            
            # Target: ì‹œí€€ìŠ¤ì˜ ì‹¤ì œ ë§ˆì§€ë§‰ ì•„ì´í…œ
            # (ì£¼ì˜: íŒ¨ë”©ì´ 0ì´ë¯€ë¡œ 0ì´ ì•„ë‹Œ ë§ˆì§€ë§‰ ê°’ì„ ì°¾ì•„ì•¼ í•¨. ê°„ë‹¨íˆ seq_idsì˜ ì²«ë²ˆì§¸(ìµœê·¼)ê°€ ë§ˆì§€ë§‰ì´ë¼ ê°€ì •)
            # preprocess ë¡œì§ìƒ seq_ids[-1]ì´ ê°€ì¥ ìµœê·¼ì„.
            
            target_item_ids = seq_ids[:, -1] # ê°€ì¥ ìµœê·¼ ì•„ì´í…œ
            target_item_vec = target_lookup(target_item_ids) # (B, 128)
            target_item_vec = F.normalize(target_item_vec, p=2, dim=1)
            
            # Loss Calculation (LogQ)
            loss = logq_correction_loss(
                user_vec, target_item_vec, target_item_ids, item_log_probs, lambda_logq=0.5
            )
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({'loss': f"{loss.item():.4f}"})
            
        print(f"âœ… Epoch {epoch+1} Done. Avg Loss: {total_loss/len(dataloader):.4f}")
        
    # Save
    torch.save(model.state_dict(), os.path.join(MODEL_DIR, "user_tower_logq.pth"))

if __name__ == "__main__":
    os.makedirs(MODEL_DIR, exist_ok=True)
    train_user_tower()








'''
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import numpy as np

# -------------------------------------------------------------------------
# 0. Global Configuration
# -------------------------------------------------------------------------
EMBED_DIM = 128          # Item Towerì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤
MAX_SEQ_LEN = 50         # ìœ ì €ì˜ êµ¬ë§¤ ì´ë ¥ ìµœëŒ€ ê¸¸ì´
NUM_META_FEATURES = 4    # ë‚˜ì´, ì„±ë³„ ë“±
DROPOUT = 0.1
GNN_DIM = 64
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • (í˜„ì¬ íŒŒì¼ ê¸°ì¤€ relative path)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "models")

GNN_PATH = os.path.join(MODEL_DIR, "1simgcl_trained.pth")
ITEM_MATRIX_PATH = os.path.join(MODEL_DIR, "1pretrained_item_matrix.pt")

# -------------------------------------------------------------------------
# 1. Modules (Fusion & User Tower)
# -------------------------------------------------------------------------

class ContextGatingFusion(nn.Module):
    """
    [SE-Block Style Fusion]
    GNN(Global), Seq(Current), Meta(Static) 3ê°€ì§€ ì‹ í˜¸ë¥¼ 
    ìƒí™©ì— ë§ê²Œ ë™ì ìœ¼ë¡œ ì„ëŠ” ëª¨ë“ˆ
    """
    def __init__(self, dim=128):
        super().__init__()
        # ì…ë ¥: 3ê°œ ë²¡í„° ì—°ê²° (128 * 3) -> ì¶œë ¥: 3ê°œ ê²Œì´íŠ¸
        self.gate_mlp = nn.Sequential(
            nn.Linear(dim * 3, dim * 3), 
            nn.ReLU(),
            nn.Linear(dim * 3, dim * 3), 
            nn.Sigmoid() 
        )
        # ìµœì¢… ì •ì œ
        self.output_proj = nn.Sequential(
            nn.Linear(dim, dim),
            nn.LayerNorm(dim)
        )

    def forward(self, v_gnn, v_seq, v_meta):
        # 1. Concatenation (B, 384)
        combined = torch.cat([v_gnn, v_seq, v_meta], dim=1) 
        
        # 2. Calculate Channel-wise Gates
        all_gates = self.gate_mlp(combined) 
        
        # 3. Split Gates
        dim = v_gnn.shape[1]
        g_gnn = all_gates[:, :dim]
        g_seq = all_gates[:, dim:2*dim]
        g_meta = all_gates[:, 2*dim:]
        
        # 4. Gated Sum (Element-wise Multiplication)
        v_fused = (v_gnn * g_gnn) + (v_seq * g_seq) + (v_meta * g_meta)
        
        # 5. Final Projection
        return self.output_proj(v_fused)

class HybridUserTower(nn.Module):
    def __init__(self, 
                 num_users: int, 
                 pretrained_gnn_embeddings: torch.Tensor = None, 
                 pretrained_item_vectors: torch.Tensor = None,   
                 freeze_item_emb: bool = True):
        super().__init__()
        
        # A. GNN Part
        if pretrained_gnn_embeddings is not None:
            # GNN í•™ìŠµ ê²°ê³¼ ë¡œë“œ (Num_Users, 64)
            self.gnn_user_emb = nn.Embedding.from_pretrained(pretrained_gnn_embeddings, freeze=False)
            current_gnn_dim = pretrained_gnn_embeddings.shape[1]
        else:
            # Fallback (í…ŒìŠ¤íŠ¸ìš©)
            print("âš ï¸ [Warning] Initializing GNN Embedding Randomly.")
            current_gnn_dim = GNN_DIM
            self.gnn_user_emb = nn.Embedding(num_users, current_gnn_dim)
            nn.init.xavier_normal_(self.gnn_user_emb.weight)

        self.gnn_projector = nn.Sequential(
            nn.Linear(current_gnn_dim, EMBED_DIM),
            nn.LayerNorm(EMBED_DIM),
            nn.GELU()
        )

        # B. Sequential Part
        # Item Tower ê²°ê³¼ ë¡œë“œ (Num_Items, 128)
        if pretrained_item_vectors is not None:
            self.item_content_emb = nn.Embedding.from_pretrained(pretrained_item_vectors, freeze=freeze_item_emb)
        else:
            print("âš ï¸ [Warning] Initializing Item Embedding Randomly.")
            self.item_content_emb = nn.Embedding(10000, EMBED_DIM) # Dummy size

        self.position_emb = nn.Embedding(MAX_SEQ_LEN, EMBED_DIM)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=EMBED_DIM, 
            nhead=4, 
            dim_feedforward=EMBED_DIM*4, 
            dropout=DROPOUT,
            batch_first=True,
            norm_first=True
        )
        self.seq_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)

        # C. Meta Part
        self.meta_mlp = nn.Sequential(
            nn.Linear(NUM_META_FEATURES, EMBED_DIM // 2),
            nn.GELU(),
            nn.Linear(EMBED_DIM // 2, EMBED_DIM),
            nn.LayerNorm(EMBED_DIM)
        )

        # D. Fusion
        self.fusion_layer = ContextGatingFusion(dim=EMBED_DIM)

    def forward(self, user_indices, history_item_ids, history_mask, meta_features):
        # 1. GNN
        v_gnn = self.gnn_projector(self.gnn_user_emb(user_indices))

        # 2. Sequential
        seq_emb = self.item_content_emb(history_item_ids)
        
        # Positional Encoding
        B, L = history_item_ids.shape
        positions = torch.arange(L, device=history_item_ids.device).unsqueeze(0)
        seq_input = seq_emb + self.position_emb(positions)

        # Transformer (Padding Masking)
        # maskê°€ 1ì´ë©´ Valid, 0ì´ë©´ Padding -> key_padding_maskëŠ” Trueê°€ Masking
        # ë”°ë¼ì„œ 1-mask í˜¹ì€ ~mask.bool() ì‚¬ìš©
        key_padding_mask = (history_mask == 0)
        seq_out = self.seq_encoder(seq_input, src_key_padding_mask=key_padding_mask)
        
        # Masked Mean Pooling
        mask_expanded = history_mask.unsqueeze(-1)
        sum_seq = (seq_out * mask_expanded).sum(dim=1)
        cnt_seq = mask_expanded.sum(dim=1).clamp(min=1e-9)
        v_seq = sum_seq / cnt_seq

        # 3. Meta
        v_meta = self.meta_mlp(meta_features)

        # 4. Fusion
        output = self.fusion_layer(v_gnn, v_seq, v_meta)
        
        return F.normalize(output, p=2, dim=1)

# -------------------------------------------------------------------------
# 2. Asset Loader Helper
# -------------------------------------------------------------------------
def load_pretrained_assets():
    """ GNN ê°€ì¤‘ì¹˜ì™€ Item Vector í…ì„œë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤. """
    print("\nğŸ“¦ Loading Pre-trained Assets...")
    
    gnn_tensor = None
    item_tensor = None
    
    # 1. GNN Load
    if os.path.exists(GNN_PATH):
        print(f"   - Found GNN Checkpoint: {GNN_PATH}")
        ckpt = torch.load(GNN_PATH, map_location='cpu')
        print(f"     âœ… Loaded GNN Tensor: {ckpt.shape}")

    else:
        print(f"   âŒ GNN Checkpoint not found at {GNN_PATH}")

    # 2. Item Matrix Load
    if os.path.exists(ITEM_MATRIX_PATH):
        print(f"   - Found Item Matrix: {ITEM_MATRIX_PATH}")
        item_tensor = torch.load(ITEM_MATRIX_PATH, map_location='cpu')
        print(f"     âœ… Loaded Item Tensor: {item_tensor.shape}")
    else:
        print(f"   âŒ Item Matrix not found at {ITEM_MATRIX_PATH}")
        
    return gnn_tensor, item_tensor

# -------------------------------------------------------------------------
# 3. Dataset
# -------------------------------------------------------------------------
class UserSeqDataset(Dataset):
    def __init__(self, user_ids, history_seqs, target_items, meta_data, max_len=50):
        self.user_ids = user_ids
        self.history_seqs = history_seqs
        self.target_items = target_items
        self.meta_data = meta_data
        self.max_len = max_len

    def __len__(self):
        return len(self.user_ids)

    def __getitem__(self, idx):
        seq = self.history_seqs[idx]
        seq_len = len(seq)
        
        if seq_len >= self.max_len:
            seq = seq[-self.max_len:]
            mask = [1] * self.max_len
        else:
            pad_len = self.max_len - seq_len
            seq = seq + [0] * pad_len # 0 is Padding ID
            mask = [1] * seq_len + [0] * pad_len

        return {
            "user_idx": torch.tensor(self.user_ids[idx], dtype=torch.long),
            "history_ids": torch.tensor(seq, dtype=torch.long),
            "history_mask": torch.tensor(mask, dtype=torch.long),
            "meta": torch.tensor(self.meta_data[idx], dtype=torch.float),
            "target_item_id": torch.tensor(self.target_items[idx], dtype=torch.long)
        }

# -------------------------------------------------------------------------
# 4. Training Loop
# -------------------------------------------------------------------------
def train_user_tower():
    # A. ë°ì´í„° ì¤€ë¹„ (Dummy Data for Demo)
    # ì‹¤ì œë¡œëŠ” DBì—ì„œ ì½ì–´ì™€ì•¼ í•©ë‹ˆë‹¤.
    print("\nğŸ› ï¸ Preparing Data...")
    num_dummy_users = 100
    dummy_user_ids = list(range(num_dummy_users))
    dummy_history = [np.random.randint(1, 1000, size=np.random.randint(5, 30)).tolist() for _ in range(num_dummy_users)]
    dummy_targets = np.random.randint(1, 1000, size=num_dummy_users).tolist()
    dummy_meta = np.random.randn(num_dummy_users, NUM_META_FEATURES).astype(np.float32)

    dataset = UserSeqDataset(dummy_user_ids, dummy_history, dummy_targets, dummy_meta, MAX_SEQ_LEN)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    # B. ëª¨ë¸ ì´ˆê¸°í™”
    gnn_emb, item_emb = load_pretrained_assets()
    
    # GNN í…ì„œê°€ ì—†ìœ¼ë©´ ìœ ì € ìˆ˜ë¼ë„ ë§ì¶°ì„œ ë”ë¯¸ ìƒì„± (ì—ëŸ¬ ë°©ì§€)
    real_num_users = gnn_emb.shape[0] if gnn_emb is not None else 1000
    
    model = HybridUserTower(
        num_users=real_num_users,
        pretrained_gnn_embeddings=gnn_emb,
        pretrained_item_vectors=item_emb,
        freeze_item_emb=True
    ).to(DEVICE)

    # C. í•™ìŠµ ì„¤ì •
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss()
    
    # Target Item Lookupì„ ìœ„í•œ ê³ ì • ì„ë² ë”© ë ˆì´ì–´ (í•™ìŠµ X)
    # User Tower ë‚´ë¶€ì˜ item_content_embì™€ ê°™ì€ ê°’ì„ ê³µìœ í•˜ì§€ë§Œ, ìš©ë„ê°€ ë‹¤ë¦„ (ì •ë‹µ ë¹„êµìš©)
    if item_emb is not None:
        target_lookup = nn.Embedding.from_pretrained(item_emb, freeze=True).to(DEVICE)
    else:
        # Fallback
        target_lookup = nn.Embedding(10000, 128).to(DEVICE)

    # D. Training Loop
    EPOCHS = 5
    print(f"\nğŸš€ Start Training on {DEVICE}...")
    
    for epoch in range(EPOCHS):
        total_loss = 0
        steps = 0
        model.train()
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        for batch in pbar:
            optimizer.zero_grad()
            
            # Input
            u_idx = batch['user_idx'].to(DEVICE)
            h_ids = batch['history_ids'].to(DEVICE)
            h_mask = batch['history_mask'].to(DEVICE)
            meta = batch['meta'].to(DEVICE)
            t_ids = batch['target_item_id'].to(DEVICE)
            
            # 1. User Representation
            user_vec = model(u_idx, h_ids, h_mask, meta) # (B, 128)
            
            # 2. Target Item Representation
            # "ì´ ìœ ì €ê°€ ì‹¤ì œë¡œ ì‚° ê·¸ ì•„ì´í…œ"ì˜ ë¯¸ë¦¬ ê³„ì‚°ëœ ë²¡í„°ë¥¼ ê°€ì ¸ì˜´
            target_vec = target_lookup(t_ids)
            target_vec = F.normalize(target_vec, p=2, dim=1) # (B, 128)
            
            # 3. In-batch Contrastive Loss
            # Score: (B, B) -> ëŒ€ê°ì„ ì´ Positive Pair
            scores = torch.matmul(user_vec, target_vec.T) 
            scores = scores / 0.07 # Temperature
            
            labels = torch.arange(scores.size(0)).to(DEVICE)
            
            loss = criterion(scores, labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            steps += 1
            pbar.set_postfix({'loss': f"{loss.item():.4f}"})
            
        print(f"   âœ… Epoch {epoch+1} Avg Loss: {total_loss/steps:.4f}")

    print("\nğŸ’¾ Training Finished. Saving User Tower...")
    torch.save(model.state_dict(), os.path.join(MODEL_DIR, "user_tower_final.pth"))

if __name__ == "__main__":
    # í´ë”ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë‚˜ë¯€ë¡œ ìƒì„±
    os.makedirs(MODEL_DIR, exist_ok=True)
    train_user_tower()

# -------------------------------------------------------------------------
# 3. Usage Example
# -------------------------------------------------------------------------
if __name__ == "__main__":
    # Mock Data
    NUM_USERS = 1000
    NUM_ITEMS = 5000
    EMBED_DIM = 128
    
    # 1. Pre-trained Vectors (ê°€ì •)
    # Item Towerì—ì„œ ì „ì²´ ì•„ì´í…œì„ Inferenceí•´ì„œ ë§Œë“  (5000, 128) í–‰ë ¬
    pretrained_item_vecs = torch.randn(NUM_ITEMS, EMBED_DIM) 
    pretrained_gnn_emb = torch.randn(NUM_USERS, EMBED_DIM)
    
    # 2. Dataset Preparation
    # ìœ ì € 0ë²ˆì´ [1, 2, 3]ì„ ìƒ€ê³ , ë‹¤ìŒì— 4ë²ˆì„ ìƒ€ë‹¤.
    user_ids = [0, 1, 2] * 100
    history_seqs = [[1, 2, 3], [10, 20], [100, 101, 102, 103]] * 100
    target_items = [4, 21, 104] * 100
    meta_data = torch.randn(300, 4) # (N, 4)
    
    dataset = UserSeqDataset(user_ids, history_seqs, target_items, meta_data)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # 3. Instantiate Model
    # ì•ì„œ ì •ì˜í•œ HybridUserTower í´ë˜ìŠ¤
    model = HybridUserTower(
        num_users=NUM_USERS,
        pretrained_gnn_embeddings=pretrained_gnn_emb,
        pretrained_item_vectors=pretrained_item_vecs,
        freeze_item_emb=True # Item Embedding LayerëŠ” ê³ ì • (í•™ìŠµ X)
    )
    
    # 4. Train
    trained_model = train_user_tower(
        model, 
        pretrained_item_vecs, 
        loader, 
        epochs=3
    )
'''