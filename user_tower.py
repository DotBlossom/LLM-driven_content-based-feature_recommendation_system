import torch
import torch.nn as nn
import torch.nn.functional as F

# -------------------------------------------------------------------------
# Global Config
# -------------------------------------------------------------------------
EMBED_DIM = 128          # Item Towerì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤
MAX_SEQ_LEN = 50         # ìœ ì €ì˜ êµ¬ë§¤ ì´ë ¥ ìµœëŒ€ ê¸¸ì´
NUM_META_FEATURES = 4    # ì˜ˆ: ë‚˜ì´(1) + ì„±ë³„(1) + ê´€ì‹¬ë„(1) + ê°€ê²©ë¯¼ê°ë„(1)(ì •ê·œí™” í‰ê· )
DROPOUT = 0.1
GNN_DIM = 64
class HybridUserTower(nn.Module):
    def __init__(self, 
                 num_users: int, 
                 pretrained_gnn_embeddings: torch.Tensor = None, # SimGCLì—ì„œ í•™ìŠµëœ User Emb
                 pretrained_item_vectors: torch.Tensor = None,   # Item Towerë¡œ ë¯¸ë¦¬ ë½‘ì•„ë‘” Item Vector Matrix
                 freeze_item_emb: bool = True):
        super().__init__()
        
        # ======================================================
        # 1. GNN Part (Collaborative Signal)
        # ======================================================
        # SimGCLì—ì„œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œ (ID ê¸°ë°˜)
        if pretrained_gnn_embeddings is not None:
            # Input: (Num_Users, 64)
            self.gnn_user_emb = nn.Embedding.from_pretrained(pretrained_gnn_embeddings, freeze=False)
            current_gnn_dim = pretrained_gnn_embeddings.shape[1] # Should be 64
        else:
            # Fallback for testing
            current_gnn_dim = GNN_DIM
            self.gnn_user_emb = nn.Embedding(num_users, current_gnn_dim)
            nn.init.xavier_normal_(self.gnn_user_emb.weight)


        self.gnn_projector = nn.Sequential(
            nn.Linear(current_gnn_dim, EMBED_DIM), # 64 -> 128
            nn.LayerNorm(EMBED_DIM),
            nn.GELU()
        )
        # ======================================================
        # 2. Sequential Part (SesRec / Content Signal)
        # ======================================================
        # Item Towerì˜ 128ì°¨ì› ë²¡í„°ë¥¼ Lookup Tableë¡œ ì‚¬ìš©
        # (num_items, 128) í–‰ë ¬ì´ ë“¤ì–´ì™€ì•¼ í•¨
        if pretrained_item_vectors is not None:
            self.item_content_emb = nn.Embedding.from_pretrained(pretrained_item_vectors, freeze=freeze_item_emb)
        else:
            # Fallback (ì¼ë°˜ í•™ìŠµìš©)
            self.item_content_emb = nn.Embedding(10000, EMBED_DIM)

        # Positional Embedding (ìˆœì„œ ì •ë³´)
        self.position_emb = nn.Embedding(MAX_SEQ_LEN, EMBED_DIM)
        
        # Transformer Encoder (SASRec Style)
        # ìœ ì €ì˜ êµ¬ë§¤ íˆìŠ¤í† ë¦¬ë¥¼ ìš”ì•½
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=EMBED_DIM, 
            nhead=4, 
            dim_feedforward=EMBED_DIM*4, 
            dropout=DROPOUT,
            batch_first=True,
            norm_first=True # Pre-LN ê¶Œì¥
        )
        self.seq_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)

        # ======================================================
        # 3. Meta Part (User Demographics)
        # ======================================================
        self.meta_mlp = nn.Sequential(
            nn.Linear(NUM_META_FEATURES, EMBED_DIM // 2),
            nn.GELU(),
            nn.Linear(EMBED_DIM // 2, EMBED_DIM),
            nn.LayerNorm(EMBED_DIM)
        )

        # ======================================================
        # 4. Gating Fusion
        # ======================================================
        # GNN(128) + Seq(128) + Meta(128) -> ê°€ì¤‘ì¹˜(Gate) ìƒì„± / ì„ì‹œ
        self.gate_network = nn.Sequential(
            nn.Linear(EMBED_DIM * 3, 64),
            nn.Tanh(),
            nn.Linear(64, 3), # 3ê°œì˜ ì†ŒìŠ¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜
            nn.Softmax(dim=1)
        )
        
        # ìµœì¢… ìœµí•© í›„ Projection
        self.final_proj = nn.Sequential(
            nn.Linear(EMBED_DIM, EMBED_DIM),
            nn.LayerNorm(EMBED_DIM) # ìµœì¢… ì¶œë ¥ ì •ê·œí™”
        )

    def forward(self, 
                user_indices: torch.Tensor,      # (B,)
                history_item_ids: torch.Tensor,  # (B, Seq_Len) - Padding í¬í•¨
                history_mask: torch.Tensor,      # (B, Seq_Len) - 1:Valid, 0:Pad
                meta_features: torch.Tensor      # (B, Meta_Dim)
                ):
        
        # --- A. GNN Vector (Global Interest) ---
        
        # 1. Retrieve 64-dim vector
        v_gnn_raw = self.gnn_user_emb(user_indices) # (B, 64)
        
        # 2. Project to 128-dim
        v_gnn = self.gnn_projector(v_gnn_raw)

        # --- B. Sequential Vector (Current Interest) ---
        # 1. Item Embedding + Positional Embedding
        batch_size, seq_len = history_item_ids.shape
        
        # ì•„ì´í…œ ë²¡í„° (Pre-trained Item Towerì˜ ì§€ì‹)
        seq_emb = self.item_content_emb(history_item_ids) # (B, Seq, 128)
        
        # ìœ„ì¹˜ ë²¡í„°
        positions = torch.arange(seq_len, device=history_item_ids.device).unsqueeze(0)
        pos_emb = self.position_emb(positions) # (1, Seq, 128)
        
        seq_input = seq_emb + pos_emb

        # 2. Transformer Encoding
        # src_key_padding_mask: Trueê°€ Maskingë¨ (PyTorch í‘œì¤€) -> history_maskê°€ 1(ìœ íš¨)ë©´ False(ì•ˆê°€ë¦¼)ì—¬ì•¼ í•¨
        # ë”°ë¼ì„œ ~history_mask.bool() ì‚¬ìš©
        seq_out = self.seq_encoder(seq_input, src_key_padding_mask=~history_mask.bool())
        
        # 3. Pooling (Last Valid Item or Mean)
        # ì—¬ê¸°ì„œëŠ” ê°€ì¥ ìµœê·¼ì— ì‚° ë¬¼ê±´(Last Valid)ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ê°€ì • -> SASRec ë°©ì‹
        # í˜¹ì€ ì „ì²´ ë¬¸ë§¥(Mean) ì‚¬ìš©.
        
        # [ê°„í¸ êµ¬í˜„] Masked Mean Pooling
        mask_expanded = history_mask.unsqueeze(-1) # (B, Seq, 1)
        sum_seq = (seq_out * mask_expanded).sum(dim=1)
        cnt_seq = mask_expanded.sum(dim=1).clamp(min=1e-9)
        v_seq = sum_seq / cnt_seq # (B, 128)

        # --- C. Meta Vector ---
        v_meta = self.meta_mlp(meta_features) # (B, 128)

        # --- D. Gating & Fusion ---
        # 3ê°€ì§€ ë²¡í„°ë¥¼ ì´ì–´ ë¶™ì—¬ì„œ Gate í†µê³¼
        combined = torch.cat([v_gnn, v_seq, v_meta], dim=1) # (B, 384)
        gates = self.gate_network(combined) # (B, 3) -> [w_gnn, w_seq, w_meta]
        
        # ê°€ì¤‘í•© (Weighted Sum)
        v_final = (gates[:, 0:1] * v_gnn) + \
                  (gates[:, 1:2] * v_seq) + \
                  (gates[:, 2:3] * v_meta)
        
        # ìµœì¢… Projection (Retrievalì„ ìœ„í•´)
        output = self.final_proj(v_final)
        
        # ë‚´ì  ê²€ìƒ‰ì„ ìœ„í•´ L2 Normalize (SimCSE Item Towerì™€ í˜¸í™˜ì„± ìœ ì§€)
        return F.normalize(output, p=2, dim=1)
    
    import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np

# -------------------------------------------------------------------------
# 1. Dataset for User Tower Training
# -------------------------------------------------------------------------
class UserSeqDataset(Dataset):
    def __init__(self, 
                 user_ids,         # List[int]
                 history_seqs,     # List[List[int]]: ê³¼ê±° êµ¬ë§¤ ì´ë ¥ (Input)
                 target_items,     # List[int]: ë‹¤ìŒì— ì‹¤ì œë¡œ êµ¬ë§¤í•œ ì•„ì´í…œ (Label)
                 meta_data,        # Tensor: ìœ ì € ë©”íƒ€ ì •ë³´
                 max_len=50):
        
        self.user_ids = user_ids
        self.history_seqs = history_seqs
        self.target_items = target_items
        self.meta_data = meta_data
        self.max_len = max_len

    def __len__(self):
        return len(self.user_ids)

    def __getitem__(self, idx):
        # 1. History Sequence Padding
        seq = self.history_seqs[idx]
        seq_len = len(seq)
        
        if seq_len >= self.max_len:
            seq = seq[-self.max_len:] # ìµœê·¼ê±°ë§Œ
            mask = [1] * self.max_len
        else:
            # Pre-padding (ì•ì„ 0ìœ¼ë¡œ ì±„ì›€) or Post-padding
            pad_len = self.max_len - seq_len
            seq = seq + [0] * pad_len # Post-padding (0 is PAD ID)
            mask = [1] * seq_len + [0] * pad_len

        return {
            "user_idx": torch.tensor(self.user_ids[idx], dtype=torch.long),
            "history_ids": torch.tensor(seq, dtype=torch.long),
            "history_mask": torch.tensor(mask, dtype=torch.long), # Transformerìš© ë§ˆìŠ¤í¬
            "meta": torch.tensor(self.meta_data[idx], dtype=torch.float),
            "target_item_id": torch.tensor(self.target_items[idx], dtype=torch.long)
        }

# -------------------------------------------------------------------------
# 2. Training Loop (In-batch Negatives)
# -------------------------------------------------------------------------
def train_user_tower(
    user_tower_model: nn.Module,
    pretrained_item_matrix: torch.Tensor, # (Num_Items, 128) - Fixed
    dataloader: DataLoader,
    epochs=5,
    lr=1e-4,
    device='cuda'
):
    # 1. Setup
    user_tower_model = user_tower_model.to(device)
    user_tower_model.train()
    
    # Item VectorëŠ” í•™ìŠµë˜ì§€ ì•Šë„ë¡ ê³ ì • (Lookup Tableë¡œ ì‚¬ìš©)
    # Target Itemì˜ ë²¡í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•¨
    item_emb_layer = nn.Embedding.from_pretrained(pretrained_item_matrix, freeze=True).to(device)
    
    optimizer = optim.AdamW(user_tower_model.parameters(), lr=lr)
    
    # Loss: InfoNCE (Contrastive Loss)
    # In-batch Negative Samplingì„ í™œìš©í•œ CrossEntropy
    criterion = nn.CrossEntropyLoss()

    print(f"ğŸš€ Start Training User Tower for {epochs} epochs...")

    for epoch in range(epochs):
        total_loss = 0
        step = 0
        
        for batch in dataloader:
            optimizer.zero_grad()
            
            # --- Move to Device ---
            user_idx = batch['user_idx'].to(device)
            hist_ids = batch['history_ids'].to(device)
            hist_mask = batch['history_mask'].to(device)
            meta = batch['meta'].to(device)
            target_ids = batch['target_item_id'].to(device) # ì •ë‹µ ì•„ì´í…œ ID

            # --- Forward Pass ---
            
            # 1. User Vector ìƒì„± (User Towerê°€ ìš”ë¦¬í•¨)
            # (Batch, 128)
            user_vector = user_tower_model(user_idx, hist_ids, hist_mask, meta)
            
            # 2. Target Item Vector ê°€ì ¸ì˜¤ê¸° (ì´ë¯¸ í•™ìŠµëœ Item Tower ê²°ê³¼ê°’)
            # (Batch, 128)
            target_item_vector = item_emb_layer(target_ids)
            target_item_vector = F.normalize(target_item_vector, p=2, dim=1) # Normalize í•„ìˆ˜
            
            # --- Loss Calculation (In-batch Negatives) ---
            # User(B, D) @ Item(B, D).T -> Score Matrix (B, B)
            # ëŒ€ê°ì„ : Positive (ë‚´ ìœ ì €ê°€ ì‚° ë‚´ ì•„ì´í…œ)
            # ë‚˜ë¨¸ì§€: Negative (ë‚´ ìœ ì €ê°€ ì•ˆ ì‚°, ë‚¨ì´ ì‚° ì•„ì´í…œ)
            scores = torch.matmul(user_vector, target_item_vector.T) # (Batch, Batch)
            
            # Temperature Scaling
            temperature = 0.07
            scores = scores / temperature
            
            # Labels: 0, 1, 2, ... (ëŒ€ê°ì„  ì¸ë±ìŠ¤)
            labels = torch.arange(scores.size(0)).to(device)
            
            loss = criterion(scores, labels)
            
            # --- Backward ---
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            step += 1
            
            if step % 100 == 0:
                print(f"Epoch [{epoch+1}/{epochs}] Step [{step}] Loss: {loss.item():.4f}")

        print(f"==== Epoch {epoch+1} Avg Loss: {total_loss/step:.4f} ====")

    print("âœ… Training Finished.")
    return user_tower_model

# -------------------------------------------------------------------------
# 3. Usage Example
# -------------------------------------------------------------------------
if __name__ == "__main__":
    # Mock Data
    NUM_USERS = 1000
    NUM_ITEMS = 5000
    EMBED_DIM = 128
    
    # 1. Pre-trained Vectors (ê°€ì •)
    # Item Towerì—ì„œ ì „ì²´ ì•„ì´í…œì„ Inferenceí•´ì„œ ë§Œë“  (5000, 128) í–‰ë ¬
    pretrained_item_vecs = torch.randn(NUM_ITEMS, EMBED_DIM) 
    pretrained_gnn_emb = torch.randn(NUM_USERS, EMBED_DIM)
    
    # 2. Dataset Preparation
    # ìœ ì € 0ë²ˆì´ [1, 2, 3]ì„ ìƒ€ê³ , ë‹¤ìŒì— 4ë²ˆì„ ìƒ€ë‹¤.
    user_ids = [0, 1, 2] * 100
    history_seqs = [[1, 2, 3], [10, 20], [100, 101, 102, 103]] * 100
    target_items = [4, 21, 104] * 100
    meta_data = torch.randn(300, 4) # (N, 4)
    
    dataset = UserSeqDataset(user_ids, history_seqs, target_items, meta_data)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    # 3. Instantiate Model
    # ì•ì„œ ì •ì˜í•œ HybridUserTower í´ë˜ìŠ¤
    model = HybridUserTower(
        num_users=NUM_USERS,
        pretrained_gnn_embeddings=pretrained_gnn_emb,
        pretrained_item_vectors=pretrained_item_vecs,
        freeze_item_emb=True # Item Embedding LayerëŠ” ê³ ì • (í•™ìŠµ X)
    )
    
    # 4. Train
    trained_model = train_user_tower(
        model, 
        pretrained_item_vecs, 
        loader, 
        epochs=3
    )