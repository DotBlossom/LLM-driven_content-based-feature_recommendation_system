from sqlalchemy import select
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, AutoModel
from pydantic import BaseModel
from typing import Any, Dict, List
import copy
import random
from tqdm import tqdm
from torch.amp import autocast, GradScaler
from database import ProductInferenceInput
from utils import vocab

import os


# --- Global Configuration ---
EMBED_DIM = 128
OUTPUT_DIM_ENCODER = 128       # Encoder(Representation) ì¶œë ¥
OUTPUT_DIM_PROJECTOR = 128     # Projector(SimCSE Lossìš©) ì¶œë ¥
FASHION_BERT_MODEL = "bert-base-uncased" 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

PAD_ID = vocab.PAD_ID
UNK_ID = vocab.UNK_ID

MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)


# ----------------------------------------------------------------------
# 1. Models: Encoder + Projector + Wrapper
# ----------------------------------------------------------------------

# (A) Encoder: HybridItemTower 
class SEResidualBlock(nn.Module):
    """ Squeeze-and-Excitation Residual Block """
    def __init__(self, dim, dropout=0.2, expansion_factor=4):
        super().__init__()
        
        
        # 1. Feature Transformation (SwiGLU ìŠ¤íƒ€ì¼ë¡œ?)
        self.block = nn.Sequential(
            nn.Linear(dim, dim * expansion_factor),
            nn.LayerNorm(dim * expansion_factor),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * expansion_factor, dim),
            nn.LayerNorm(dim),
        )        
        # 2. SE-Block (Channel Attention, SE-Net êµ¬ì¡° ë°˜ì˜, Gating=Relu íŒŒíŠ¸)
        # ì…ë ¥ ë²¡í„°ì˜ ê° ì°¨ì›(feature)ì— ëŒ€í•´ ì¤‘ìš”ë„(0~1)ë¥¼ ê³„ì‚°
        self.se_block = nn.Sequential(
            nn.Linear(dim, dim // 4),
            nn.ReLU(),
            nn.Linear(dim // 4, dim),
            nn.Sigmoid()
        )
        self.act = nn.GELU()
    
    def forward(self, x):
        residual = x
        out = self.block(x)
        
        # (B) SE-Attention Path
        # MLP ì¶œë ¥ê°’(out)ì— ì¤‘ìš”ë„(weight)ë¥¼ ê³±í•¨
        weight = self.se_block(out)
        out = out * weight
        
        return self.act(residual + out)

class DeepResidualHead(nn.Module):

    def __init__(self, input_dim, output_dim=128):
        super().__init__()
        
        # ì°¨ì› ì •ì˜ 
        mid_dim = input_dim * 2      # 256
        hidden_dim = input_dim * 4   # 512
        
        # 1. Progressive Expansion 
        self.expand_layer1 = nn.Sequential(
            nn.Linear(input_dim, mid_dim),  # step 1 128 -> 256
            nn.LayerNorm(mid_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        self.expand_layer2 = nn.Sequential(
            nn.Linear(mid_dim, hidden_dim), # step 2 256 -> 512
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
        # 2. Deep Interaction 
        self.res_blocks = nn.Sequential(
            SEResidualBlock(hidden_dim, dropout=0.2), 
            SEResidualBlock(hidden_dim, dropout=0.2)  
        )
        
        # 3. Final Projection (Compression)s

        self.final_proj = nn.Linear(hidden_dim, output_dim)
        
        # 4. Global Skip Connection (Input Shortcut) ResNet ì”ì°¨
        self.input_skip = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        # --- [Step 1] Progressive Expansion ---
        m = self.expand_layer1(x) 
        h = self.expand_layer2(m)  
        
        # --- [Step 2] Feature Interaction (SE-Attention) ---
        h = self.res_blocks(h)     
        
        # --- [Step 3] Compression ---
        main_out = self.final_proj(h) 
        
        # --- [Step 4] Global Shortcut ---
        skip_out = self.input_skip(x) 
        
        return main_out + skip_out
    

class HybridItemTower(nn.Module):
    def __init__(self,
                 std_vocab_size: int,
                 num_std_fields: int,
                 embed_dim: int = 128, 
                 output_dim: int = 128):
        super().__init__()

        # A. STD Encoder
        self.std_embedding = nn.Embedding(std_vocab_size, embed_dim, padding_idx=PAD_ID)
        self.std_field_emb = nn.Parameter(torch.randn(1, num_std_fields, embed_dim))

        # B. Fashion-BERT Setup
        print(f"Loading {FASHION_BERT_MODEL} ...")
        self.bert_config = AutoConfig.from_pretrained(FASHION_BERT_MODEL)
        self.bert_model = AutoModel.from_pretrained(FASHION_BERT_MODEL)
        bert_dim = self.bert_config.hidden_size 

        # C. RE Encoder (Embeddings from BERT -> Project)
        self.re_proj = nn.Sequential(
            nn.Linear(bert_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU()
        )
        self.re_field_position = nn.Parameter(torch.randn(1, 9, embed_dim))

        # D. Text Encoder (Product Name)
        self.text_proj = nn.Sequential(
            nn.Linear(bert_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU()
        )

        # E. Fusion
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=4,
            dim_feedforward=embed_dim * 4,
            batch_first=True,
            dropout=0.1,
            activation='gelu',
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=2,
            enable_nested_tensor=False
            )
        self.head = DeepResidualHead(input_dim=embed_dim, output_dim=output_dim)

        self._debug_logged = False
    def _debug_log(self, stage: int, title: str, tensors: Dict[str, torch.Tensor]):
        """
        [ë‚´ë¶€ í•¨ìˆ˜] ìŠ¤í…Œì´ì§€ë³„ë¡œ í…ì„œ ì •ë³´ë¥¼ ê¹”ë”í•˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.
        """
        if self._debug_logged:
            return

        # í—¤ë” ì¶œë ¥ (Stage 0ì¼ ë•Œ)
        if stage == 0:
            print("\n" + "="*60)
            print(f"ğŸ§© [HybridItemTower] Forward Flow Debugging Start")
            print("="*60)

        # ìŠ¤í…Œì´ì§€ íƒ€ì´í‹€
        print(f"ğŸ”¹ [Stage {stage}] {title}")

        # í…ì„œ ì •ë³´ ë¶„ì„ ë° ì¶œë ¥
        if tensors:
            for name, tensor in tensors.items():
                if isinstance(tensor, torch.Tensor):
                    shape_str = str(tuple(tensor.shape))
                    
                    # ê°’ì´ ì‹¤ìˆ˜í˜•ì´ë©´ í†µê³„(í‰ê· , í‘œì¤€í¸ì°¨)ë„ ì¶œë ¥
                    if tensor.dtype in [torch.float32, torch.float16, torch.float64]:
                        mean_val = tensor.mean().item()
                        std_val = tensor.std().item()
                        info = f"Shape: {shape_str} | Mean: {mean_val:.4f} | Std: {std_val:.4f}"
                    else:
                        info = f"Shape: {shape_str} (Type: {tensor.dtype})"
                        
                    print(f"   - {name:<15}: {info}")
                else:
                    print(f"   - {name:<15}: {tensor} (Not a Tensor)")
        
        print("-" * 40)

        # ì¢…ë£Œ ì²˜ë¦¬ (Stage 99ì¼ ë•Œ)
        if stage == 99:
            print("âœ… Debugging Log Finished.")
            print("="*60 + "\n")
            self._debug_logged = True
            
    def forward(self, 
                std_input: torch.Tensor,       
                re_input_ids: torch.Tensor,    
                re_attn_mask: torch.Tensor,    
                text_input_ids: torch.Tensor,  
                text_attn_mask: torch.Tensor): 
        
        B = std_input.shape[0]


        # 1. STD
        std_emb = self.std_embedding(std_input) 
        std_emb = std_emb + self.std_field_emb  

        self._debug_log(1, "STD Embedding", {"std_emb": std_emb})


        # 2. RE (Using Fashion-BERT Word Embeddings Only)
        flat_re_ids = re_input_ids.view(-1, re_input_ids.size(-1))
        with torch.no_grad(): 
             word_embs = self.bert_model.embeddings(input_ids=flat_re_ids) 
        
        re_feats = self.re_proj(word_embs) 
        
        # íŒ¨ë”© ì œì™¸ Pooling
        flat_mask = re_attn_mask.view(-1, re_attn_mask.size(-1)).unsqueeze(-1) 
        sum_re = torch.sum(re_feats * flat_mask, dim=1)
        count_re = torch.clamp(flat_mask.sum(dim=1), min=1e-9)
        re_vectors = sum_re / count_re 
        
        re_vectors = re_vectors.view(B, 9, -1) 
        re_vectors = re_vectors + self.re_field_position
        
        # [Log] Stage 2: RE Encoding
        self._debug_log(2, "RE Process", {
            "BERT Word Emb": word_embs,
            "Pooled RE Vec": re_vectors
        })

        # 3. Text (Product Name) -> Full BERT Context
        bert_out = self.bert_model(input_ids=text_input_ids, attention_mask=text_attn_mask)
        cls_token = bert_out.last_hidden_state[:, 0, :] # [CLS]
        text_vec = self.text_proj(cls_token).unsqueeze(1) 

        self._debug_log(3, "Text Encoder", {"CLS Vector": text_vec})
        
        # 4. Fusion
    
        combined_seq = torch.cat([std_emb, re_vectors, text_vec], dim=1) 
        self._debug_log(4, "Fusion Prep", {"Combined Seq": combined_seq})
        
        context_out = self.transformer(combined_seq) 
        final_vec = context_out.mean(dim=1) 
        out = self.head(final_vec)
        self._debug_log(99, "Final Projection", {"Output": out})
        
        return out
    
# (B) Projector: OptimizedItemTower for SimCSE
class OptimizedItemTower(nn.Module):
    """
    [Optimization Tower]: Projection Head for Contrastive Learning
    Representation(128) -> Hidden(128) -> Output(128) -> Normalize
    """
    def __init__(self, input_dim=OUTPUT_DIM_ENCODER, output_dim=OUTPUT_DIM_PROJECTOR):
        super().__init__()
        self.layer = nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.LayerNorm(input_dim),
            nn.GELU(),
            nn.Linear(input_dim, output_dim),
        )
        
    def forward(self, x):
        x = self.layer(x)
        return F.normalize(x, p=2, dim=1) # L2 Normalization for Cosine Similarity

# (C) Wrapper: SimCSEModelWrapper (Input Arguments í™•ì¥)
class SimCSEModelWrapper(nn.Module):
    def __init__(self, encoder: nn.Module, projector: nn.Module):
        super().__init__()
        self.encoder = encoder      
        self.projector = projector  

    def forward(self, std, re_ids, re_mask, txt_ids, txt_mask):
        # 1. Encoder (Representation)
        # 5ê°œì˜ ì¸ìë¥¼ Encoderì— ì „ë‹¬
        enc_out = self.encoder(std, re_ids, re_mask, txt_ids, txt_mask)
        
        # 2. Projector (SimCSE Space)
        proj_out = self.projector(enc_out)
        
        return proj_out

# ----------------------------------------------------------------------
# 2. Data Structures & Dataset (Augmentation Logic)
# ----------------------------------------------------------------------

class TrainingItem(BaseModel):
    product_id: str
    feature_data: Dict[str, Any] # DBì—ì„œ ê¸ì–´ì˜¨ Raw JSON
    product_name: str            # Text Embeddingìš©

class SimCSERecSysDataset(Dataset):
    def __init__(self, products: List[TrainingItem], dropout_prob: float):
        self.products = products
        self.dropout_prob = dropout_prob
        
        # Dropout ëŒ€ìƒì´ ë˜ëŠ” Key ê·¸ë£¹ ì •ì˜
        self.std_keys = vocab.get_std_field_keys() # ["product_type_name", ...]
        self.re_keys = vocab.RE_FEATURE_KEYS       # ["[CAT]", "[MAT]", ...]

    def __len__(self):
        return len(self.products)
    
    def _corrupt_data(self, item: TrainingItem) -> TrainingItem:
        # ì›ë³¸ ë°ì´í„° ë³µì‚¬
        new_feature_data = copy.deepcopy(item.feature_data)
        new_name = item.product_name
        

        # í™•ë¥  ì„¤ì •
        KEY_DROP_PROB = self.dropout_prob - 0.1    # í‚¤ ìì²´ë¥¼ ë‚ ë¦´ í™•ë¥  (í†µì§¸ë¡œ ì‚­ì œ)
        VALUE_DROP_PROB = self.dropout_prob  # ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ì˜ ê°’ì„ í•˜ë‚˜ì”© ë‚ ë¦´ í™•ë¥  (ë¶€ë¶„ ì‚­ì œ)
        
        all_keys = list(new_feature_data.keys())
        
        for k in all_keys:
            val = new_feature_data[k]
            
            # (A) ê°’ì´ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì˜ˆ: [MAT]: ['Cotton', 'Poly'])
            if isinstance(val, list):
                # 1. ë¨¼ì € ê°’ì„ ì†ì•„ëƒ„ (Value-level)
                # ì‚´ì•„ë‚¨ì€ ì• ë“¤ë§Œ í•„í„°ë§
                surviving_values = [v for v in val if random.random() > VALUE_DROP_PROB]
                
                # 2. ë§Œì•½ ë‹¤ ì§€ì›Œì ¸ì„œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë˜ë©´ -> í‚¤ ìì²´ë¥¼ ì‚­ì œ
                if not surviving_values:
                    del new_feature_data[k]
                else:
                    new_feature_data[k] = surviving_values
                    
            # (B) ê°’ì´ ë‹¨ì¼ ê°’(ë¬¸ìì—´ ë“±)ì¸ ê²½ìš° (ì˜ˆ: product_type_name)
            else:
                # ê·¸ëƒ¥ í‚¤ ìì²´ë¥¼ ë‚ ë¦¼ (Key-level)
                if random.random() < KEY_DROP_PROB:
                    del new_feature_data[k]

        # =======================================================
        # 2. Text Deletion (ë‹¨ì–´ êµ¬ë© ëš«ê¸°)
        # =======================================================
        if new_name:
            words = new_name.split()
            # ë‹¨ì–´ê°€ 2ê°œ ì´ìƒì´ë©´ í•˜ë‚˜ë¥¼ ì‚­ì œ (ë‚œì´ë„ ì¡°ì ˆ)
            if len(words) > 1: 
                if random.random() < 0.5: 
                    drop_idx = random.randint(0, len(words)-1)
                    del words[drop_idx]
                    new_name = " ".join(words)
            # ë‹¨ì–´ê°€ 1ê°œë¿ì´ë©´ ê°€ë” ì•„ì˜ˆ ì‚­ì œ
            elif len(words) == 1:
                if random.random() < 0.1:
                    new_name = ""

        return TrainingItem(
            product_id=item.product_id,
            feature_data=new_feature_data,
            product_name=new_name
        )

    def _apply_dropout(self, item: TrainingItem) -> TrainingItem:
        """
        Feature Dropout ìˆ˜í–‰: ë”•ì…”ë„ˆë¦¬ì—ì„œ Keyë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì œê±°.
        ì œê±°ëœ KeyëŠ” ë‚˜ì¤‘ì— Collate ë‹¨ê³„ì—ì„œ vocab.get_std_id í˜¸ì¶œ ì‹œ 
        ê°’ì´ ì—†ìœ¼ë¯€ë¡œ ìë™ìœ¼ë¡œ PAD_ID ë˜ëŠ” UNK_IDê°€ ë¨.
        """
        if self.dropout_prob <= 0:
            return item

        # Deep Copy to preserve original
        new_feature_data = copy.deepcopy(item.feature_data)
        new_name = item.product_name
        # 1. STD & RE Keys Dropout
        # feature_data ì•ˆì— flattenedëœ í˜•íƒœë¡œ ìˆë‹¤ê³  ê°€ì • 
        all_keys = list(new_feature_data.keys())
        
        for k in all_keys:
            # ì£¼ìš” Feature Keyì¸ ê²½ìš°ì—ë§Œ ë“œëì•„ì›ƒ ì‹œë„
            if (k in self.std_keys or k in self.re_keys):
                if random.random() < self.dropout_prob:
                    del new_feature_data[k]
        
        # Text Dropout (Optional): ì´ë¦„ ìì²´ë¥¼ ì§€ìš¸ì§€ ë§ì§€ ê²°ì •. 
        TEXT_DROPOUT_PROB = 0.5
        
        if random.random() < TEXT_DROPOUT_PROB:
            # ë¹ˆ ë¬¸ìì—´ë¡œ ë§Œë“¤ë©´ Tokenizerê°€ [CLS], [SEP] + Paddingìœ¼ë¡œ ì²˜ë¦¬
            new_name = ""
            
        return TrainingItem(
            product_id=item.product_id,
            feature_data=new_feature_data,
            product_name=new_name
        )

    def __getitem__(self, idx):
        item = self.products[idx]
        
        view1 = self._corrupt_data(item)
        view2 = self._corrupt_data(item)
        
        return view1, view2

# ----------------------------------------------------------------------
# 3. Collate Function (Tokenizer & Tensor Conversion)
# ----------------------------------------------------------------------

MAX_RE_LEN = 32  
MAX_TXT_LEN = 32
FIELD_PROMPT_MAP = {

    "[CAT]": "Clothing Category",   

    "[MAT]": "Fabric Material",     
    
    "[DET]": "Garment Detail",      
    
    "[FIT]": "Clothing Fit",        
    
    "[FNC]": "Apparel Function",    
    
    "[SPC]": "Product Specification", 
    
    "[COL]": "Garment Color",       
    
    "[CTX]": "Occasion",            
    
    "[LOC]": "Body Part"            
}
class SimCSECollator:
    """
    DataLoaderì—ì„œ ë°°ì¹˜ë¥¼ ë§Œë“¤ ë•Œ í† í¬ë‚˜ì´ì§• ë° í…ì„œí™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤.
    Tokenizerë¥¼ ë§¤ë²ˆ ë¡œë“œí•˜ì§€ ì•Šê¸° ìœ„í•´ í´ë˜ìŠ¤ë¡œ ê°ìŒˆ.
    """
    def __init__(self, tokenizer_path=FASHION_BERT_MODEL):
        print(f"ğŸ”„ Initializing Collator with {tokenizer_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        self.std_keys = vocab.get_std_field_keys()
        self.re_keys = vocab.RE_FEATURE_KEYS
        
        # Global Config  ë‚˜ì¤‘ì— MLopsì— ë‹¤ ì£¼ì…ë³€ìˆ˜í–‰ìœ¼ë¡œ
        self.max_re_len = MAX_RE_LEN
        self.max_txt_len = MAX_TXT_LEN
        

        self.sep = self.tokenizer.sep_token
        
        # data flow ì´ˆê¸° check flag
        self._has_logged_sample = False
    def _serialize_feature_value(self, value: Any) -> str:
        """
        ë¦¬ìŠ¤íŠ¸ë¥¼ [SEP] í† í°ìœ¼ë¡œ êµ¬ë¶„

        """
        if not value:
            return ""
        
        if isinstance(value, list):
            valid_items = [str(v) for v in value if v]
            if not valid_items:
                return ""
            # [SEP] 1 ...
            return f" {self.sep} ".join(valid_items)
            
        return str(value)

    def process_batch_items(self, items: List[TrainingItem], is_first_view: bool = False):
        """Raw Items -> Model Input Tensors ë³€í™˜"""
        
    def process_batch_items(self, items: List[TrainingItem], is_first_view: bool = False):
        batch_std = []
        batch_re_ids = []
        batch_re_masks = []
        batch_txt = [] 

        # [Log] ì²« ë°°ì¹˜ì˜ ì²« ë²ˆì§¸ ì•„ì´í…œì˜ ëª¨ë“  RE í•„ë“œ ìˆ˜ì§‘
        sample_log_re = [] 

        for idx, item in enumerate(items):
            # 1. STD
            std_ids = [vocab.get_std_id(item.feature_data.get(k, "")) for k in self.std_keys]
            batch_std.append(std_ids)

            # 2. RE 
            re_vals = [self._serialize_feature_value(item.feature_data.get(k)) for k in self.re_keys]
            
            curr_re_ids = []
            curr_re_masks = []
            for i, val in enumerate(re_vals):

                final_text = val
                if val:
                    # "[MAT]" -> "Material"
                    key_code = self.re_keys[i] 
                    prompt = FIELD_PROMPT_MAP.get(key_code, key_code) 
                    
                    # í…ìŠ¤íŠ¸ ê²°í•©: "Material: Jersey"
                    final_text = f"{prompt}: {val}"

                enc = self.tokenizer(
                    final_text, # ğŸ‘ˆ ìˆ˜ì •ëœ í…ìŠ¤íŠ¸ ì…ë ¥
                    padding='max_length', 
                    truncation=True, 
                    max_length=self.max_re_len, 
                    add_special_tokens=True
                )
                curr_re_ids.append(enc['input_ids'])
                curr_re_masks.append(enc['attention_mask'])
                

                if idx == 0 and is_first_view and not self._has_logged_sample:
                    if val:
                        key_name = self.re_keys[i]
                        decoded = self.tokenizer.decode(enc['input_ids'], skip_special_tokens=False)

                        sample_log_re.append(f"      - {key_name}: '{final_text}' -> {decoded[:40]}...")

            batch_re_ids.append(curr_re_ids)    
            batch_re_masks.append(curr_re_masks) 
            batch_txt.append(item.product_name)

        # Tensor Stacking
        tensor_std = torch.tensor(batch_std, dtype=torch.long)
        tensor_re_ids = torch.tensor(batch_re_ids, dtype=torch.long)
        tensor_re_mask = torch.tensor(batch_re_masks, dtype=torch.long)

        txt_enc = self.tokenizer(
            batch_txt, 
            padding='max_length', 
            truncation=True, 
            max_length=self.max_txt_len, 
            return_tensors='pt'
        )
        
        # [Log Print]
        if not self._has_logged_sample and is_first_view:
            import sys
            msg = []
            msg.append("\n" + "="*60)
            msg.append(f"ğŸ” [Data Integrity Check] First Batch Sample")
            msg.append(f"   1. Product Name: '{items[0].product_name}'")
            if not items[0].product_name:
                msg.append(f"      âš ï¸ WARNING: Product Name is EMPTY!")
            
            msg.append(f"   2. RE Features Found ({len(sample_log_re)} fields):")
            if sample_log_re:
                msg.extend(sample_log_re)
            else:
                msg.append("      âš ï¸ NO RE FEATURES FOUND (Check Key Matching)")
            
            msg.append("="*60 + "\n")
            
            final_msg = "\n".join(msg)
            try:
                from tqdm import tqdm
                tqdm.write(final_msg)
            except ImportError:
                print(final_msg, flush=True)

            self._has_logged_sample = True

        return tensor_std, tensor_re_ids, tensor_re_mask, txt_enc['input_ids'], txt_enc['attention_mask']

    def __call__(self, batch):
        view1_list = [item[0] for item in batch]
        view2_list = [item[1] for item in batch]
        return self.process_batch_items(view1_list, is_first_view=True), self.process_batch_items(view2_list, is_first_view=False)
    
# ----------------------------------------------------------------------
# 4. Training Loop Implementation
# ----------------------------------------------------------------------

def train_simcse_from_db(    
    encoder: nn.Module,       
    projector: nn.Module,
    db_session, # DB Session ê°ì²´ ì£¼ì… í•„ìš”
    batch_size: int,
    epochs: int,
    lr: float
):
    print("ğŸš€ Fetching data from DB...")
    stmt = select(
        ProductInferenceInput.product_id, 
        ProductInferenceInput.feature_data, 
        ProductInferenceInput.product_name 
    )
    result = db_session.execute(stmt).mappings().all()

    if not result:
        print("âŒ [Error] No data found in DB.")
        return

    # load
    products_list = []
    
    for row in result:
        # DBì˜ ì›ë³¸ ë°ì´í„° (ìˆ˜ì • ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ dictë¡œ ë³µì‚¬)
        raw_feats = dict(row['feature_data'])
        
        # 'reinforced_feature'ê°€ ìˆë‹¤ë©´ êº¼ë‚´ì„œ ì²˜ë¦¬
        if 'reinforced_feature' in raw_feats:
            re_dict = raw_feats['reinforced_feature']
            # mat -> [mat] why? tsf encoderì—ëŠ” ìƒê´€ì—†ê³ , í˜¹ì‹œ bert embeddingì—ì„œ ë” ì˜ ì“°ì¼ê¹Œë´.
            if isinstance(re_dict, dict):
                for key, val in re_dict.items():
   
                    if key.startswith("[") and key.endswith("]"):
                        vocab_key = key
                    else:
                        vocab_key = f"[{key}]"  # "MAT" -> "[MAT]"
                    

                    raw_feats[vocab_key] = val
                    
        # name tagging 
        base_name = row['product_name']
        product_type = raw_feats.get('product_type_name', "").strip() # ì˜ˆ: "Underwear Tights"
        
        final_name = ""

        if base_name:
            # Case A: ì´ë¦„ì´ ìˆëŠ” ê²½ìš° -> "ì›ë˜ì´ë¦„ (Category: íƒ€ì…ëª…)"
            if product_type:
                final_name = f"{base_name} (Category: {product_type})"
            else:
                final_name = base_name
        else:
            # Case B: ì´ë¦„ì´ ì—†ëŠ” ê²½ìš° (Fallback) -> "íƒ€ì…ëª… + ì™¸í˜•"
            appearance = raw_feats.get('graphical_appearance_name', "").strip()
            final_name = f"{product_type} {appearance}".strip()
            
            if not final_name:
                final_name = "Unknown Product"
                
                        
        item = TrainingItem(
                product_id=str(row['product_id']), 
                feature_data=raw_feats, 
                product_name=row['product_name'] if row['product_name'] else ""
            )
        products_list.append(item)
    print(f"âœ… Loaded {len(products_list)} items.")
    # â–¼â–¼â–¼ ë””ë²„ê¹… ì½”ë“œ 
    print("\nğŸ” [DEBUG] Raw DB Data Check (First Item):")
    first_row = result[0]
    print(f"   - Keys in feature_data: {list(first_row['feature_data'].keys())}")
    print(f"   - Full content: {first_row['feature_data']}")
    print("-" * 50 + "\n")
    # â–²â–²â–² ì—¬ê¸°ê¹Œì§€ â–²â–²â–²
    
    
    # 1. Model Setup
    model = SimCSEModelWrapper(encoder, projector).to(DEVICE)
    model.train()
    # ğŸ› ï¸ [AMP] Scaler ì´ˆê¸°í™” (GPU ì‚¬ìš© ì‹œ)
    use_amp = (DEVICE == "cuda")
    scaler = GradScaler(enabled=use_amp)
    
    if use_amp:
        print("âš¡ [AMP] Mixed Precision Training Enabled.")     
    bert_params = []
    other_params = []
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue 
            

        if "bert_model" in name:
            bert_params.append(param)
        else:
            other_params.append(param)
    
    
    # 2. Optimization

    optimizer = AdamW([
        {
            'params': bert_params, 
            'lr': 1e-5  
        },
        {
            'params': other_params, 
            'lr': lr   
        }
    ])
    
    # 3. Dataset & DataLoader
    dataset = SimCSERecSysDataset(products_list, dropout_prob=0.4)
    collator = SimCSECollator() # Initialize Tokenizer once
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True, 
        collate_fn=collator, # Use the class instance
        drop_last=True,
        num_workers=0 # win - ë©€í‹°í”„ë¡œì„¸ì‹± ì‹œ Tokenizer ì´ìŠˆ ì£¼ì˜
    )

    # 4. Scheduler
    total_steps = len(dataloader) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * 0.1),
        num_training_steps=total_steps
    )

    # Loss Function (Contrastive)
    # in-batch negatives í™œìš©
    from torch.nn import CrossEntropyLoss
    loss_func = CrossEntropyLoss()

    print("ğŸ”¥ Starting Training Loop...")
    
    for epoch in range(epochs):
        total_loss = 0
        step = 0
        
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        # Unpack Collator Outputs (5 tensors per view)
        for inputs_v1, inputs_v2 in progress:
            # Move to Device
            inputs_v1 = [t.to(DEVICE) for t in inputs_v1]
            inputs_v2 = [t.to(DEVICE) for t in inputs_v2]
            
            optimizer.zero_grad()
            
            
            with autocast(device_type="cuda", dtype=torch.float16, enabled=use_amp):
                    # Forward
                emb1 = model(*inputs_v1)
                emb2 = model(*inputs_v2)
                    
                    # Loss Calculation
                temperature = 0.05
                sim_matrix = torch.matmul(emb1, emb2.T) / temperature 
                    
                labels = torch.arange(emb1.size(0)).to(DEVICE)
                loss_1 = loss_func(sim_matrix, labels) 
                loss_2 = loss_func(sim_matrix.T, labels) 
                    
                loss = (loss_1 + loss_2) / 2
                
            scaler.scale(loss).backward()  # loss scaling
            scaler.step(optimizer)         # optimizer step with scaler
            scaler.update()                # update scaler factor
                    
            scheduler.step()
                    
            total_loss += loss.item()
            step += 1
            progress.set_postfix({"loss": f"{loss.item():.4f}"})
                
        if step > 0:
            avg_loss = total_loss / step
            print(f"Epoch {epoch+1} Avg Loss: {avg_loss:.4f}")
        else:
            print(f"Epoch {epoch+1}: No batches processed.")
        
        ckpt_name = f"encoder_ep{epoch+1:02d}_loss{avg_loss:.4f}.pth"
        save_path = os.path.join(MODEL_DIR, ckpt_name)
        
        # encoder only 
        torch.save(encoder.state_dict(), save_path)
        print(f"âœ… Saved Checkpoint: {ckpt_name}")
        
  
        # torch.save(projector.state_dict(), os.path.join(MODEL_DIR, f"projector_ep{epoch+1:02d}.pth"))
        
        print("-" * 50)
        # =========================================================

    print("Training Finished.")