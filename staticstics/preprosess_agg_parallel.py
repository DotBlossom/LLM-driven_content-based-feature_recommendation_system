import os
import pandas as pd
import numpy as np
import gc
import json
import ijson
from datetime import timedelta
from pandarallel import pandarallel
from sklearn.preprocessing import StandardScaler

import torch

# ==========================================
# 0. Global Settings
# ==========================================
# 16GB RAM ê¸°ì¤€: Worker 2~3ê°œ ì¶”ì²œ (ì•ˆì „í•˜ê²Œ 2)
WORKER_COUNT = 2
pandarallel.initialize(progress_bar=True, nb_workers=WORKER_COUNT, verbose=1)

BASE_DIR = r"D:\trainDataset\localprops"
RAW_FILE_PATH = os.path.join(BASE_DIR, "transactions_train_filtered.json")
CACHE_FILE_PATH = os.path.join(BASE_DIR, "cached_transactions_1yr.parquet")

path_case = {
    "train" : ["features_user","features_item", "features_sequence", "history_weekly_sales", "history_monthly_sales" ] ,
    "valid" :  ["features_user_val","features_item_val", "features_sequence_val", "history_weekly_sales_val", "history_monthly_sales_val" ]        
}
# f'path_case["valid"][0].parquet'
# Output Paths
USER_FEAT_PATH_PQ = os.path.join(BASE_DIR, "features_user_w_meta.parquet")
USER_FEAT_PATH_JS = os.path.join(BASE_DIR, "features_user_w_meta.json")
USER_FEAT_VAL_PATH_PQ = os.path.join(BASE_DIR, "features_user_w_meta_val.parquet")
USER_FEAT_VAL_PATH_JS = os.path.join(BASE_DIR, "features_user_w_meta_val.json")
ITEM_FEAT_PATH_PQ = os.path.join(BASE_DIR, "features_item.parquet")
ITEM_FEAT_PATH_JS = os.path.join(BASE_DIR, "features_item.json")
SEQ_DATA_PATH_PQ = os.path.join(BASE_DIR, "features_sequence.parquet")
SEQ_DATA_PATH_JS = os.path.join(BASE_DIR, "features_sequence.json")
WEEKLY_HISTORY_PATH = os.path.join(BASE_DIR, "history_weekly_sales.parquet")
MONTHLY_HISTORY_PATH = os.path.join(BASE_DIR, "history_monthly_sales.parquet")
USER_META_PATH = os.path.join(BASE_DIR, "customers.csv")

# Date Config
TRAIN_START_DATE = pd.to_datetime("2019-09-23")
DATASET_MAX_DATE = pd.to_datetime("2020-09-22")
VALID_START_DATE = pd.to_datetime("2020-09-16") 





def make_validation_target_file(full_df, valid_start_date, max_date, save_path):
    """
    ê²€ì¦ ê¸°ê°„ ë™ì•ˆì˜ ì‹¤ì œ êµ¬ë§¤ ë‚´ì—­ì„ ìœ ì €ë³„ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë ¬í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ¯ [Valid Target] Extracting ground truth ({valid_start_date.date()} ~ {max_date.date()})...")
    
    # 1. ê²€ì¦ ê¸°ê°„(ë§ˆì§€ë§‰ 1ì£¼ì¼) ë°ì´í„°ë§Œ í•„í„°ë§
    valid_mask = (full_df['t_dat'] >= valid_start_date) & (full_df['t_dat'] <= max_date)
    valid_target_df = full_df.loc[valid_mask].copy()
    
    if valid_target_df.empty:
        print("âš ï¸ Warning: í•´ë‹¹ ê¸°ê°„ì— êµ¬ë§¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

    # 2. ìœ ì €ë³„ êµ¬ë§¤ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    # - í•œ ìœ ì €ê°€ ì¼ì£¼ì¼ ë™ì•ˆ ì—¬ëŸ¬ ì•„ì´í…œì„ ìƒ€ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ listë¡œ ë¬¶ìŠµë‹ˆë‹¤.
    # - ê²°ê³¼ í˜•íƒœ: customer_id | target_ids (list)
    ground_truth = valid_target_df.groupby('customer_id')['article_id'].apply(list).reset_index()
    ground_truth.columns = ['customer_id', 'target_ids']

    # 3. ì €ì¥ (TARGET_VAL_PATH)
    print(f" ğŸ’¾ Saving Ground Truth to: {save_path}")
    ground_truth.to_parquet(save_path, index=False)
    
    print(f" âœ… Extraction Complete! Total Users in Target: {len(ground_truth)}")
    return ground_truth




# ==========================================
# 1. Utility Functions
# ==========================================
def save_dataframe(df, parquet_path, json_path):
    print(f"   ğŸ’¾ Saving to {parquet_path} ...")
    df.to_parquet(parquet_path, index=False)
    # df.to_json(json_path, orient='records', force_ascii=False) # í•„ìš” ì‹œ ì£¼ì„ í•´ì œ

def load_data():
    """
    ijsonì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ í­ë°œ ì—†ì´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³ ,
    reduce_mem_usage í•¨ìˆ˜ ì—†ì´ ëª…ì‹œì  íƒ€ì… ë³€í™˜ìœ¼ë¡œ ìµœì í™”í•©ë‹ˆë‹¤.
    """
    # 1. Cache Hit
    if os.path.exists(CACHE_FILE_PATH):
        print(f"\nğŸš€ [Cache Hit] {CACHE_FILE_PATH}")
        df = pd.read_parquet(CACHE_FILE_PATH)
        
    # 2. Cache Miss (Streaming Load)
    else:
        print(f"\nğŸ¢ [Cache Miss] Streaming load with ijson...")
        
        chunk_list = []
        chunk_size = 100000
        buffer = []
        
        with open(RAW_FILE_PATH, 'rb') as f:
            parser = ijson.items(f, 'item')
            
            for obj in parser:
                buffer.append(obj)
                
                if len(buffer) >= chunk_size:
                    temp_df = pd.DataFrame(buffer)
                    
                    # [Clean Optimization] ëª…ì‹œì  íƒ€ì… ë³€í™˜ (í•¨ìˆ˜ ëŒ€ì‹  ì§ì ‘ ì§€ì •)
                    temp_df['t_dat'] = pd.to_datetime(temp_df['t_dat'])
                    temp_df['article_id'] = temp_df['article_id'].astype(str)
                    temp_df['customer_id'] = temp_df['customer_id'].astype(str)
                    # ìˆ«ìí˜•ì€ float32/int8ë¡œ ì¦‰ì‹œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
                    temp_df['price'] = temp_df['price'].astype(np.float32)
                    temp_df['sales_channel_id'] = temp_df['sales_channel_id'].astype(np.int8)
                    
                    # í•„í„°ë§
                    mask = (temp_df['t_dat'] >= TRAIN_START_DATE) & (temp_df['t_dat'] <= DATASET_MAX_DATE)
                    temp_df = temp_df.loc[mask]
                    
                    if not temp_df.empty:
                        chunk_list.append(temp_df)
                    buffer = []

            # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
            if buffer:
                temp_df = pd.DataFrame(buffer)
                temp_df['t_dat'] = pd.to_datetime(temp_df['t_dat'])
                temp_df['article_id'] = temp_df['article_id'].astype(str)
                temp_df['customer_id'] = temp_df['customer_id'].astype(str)
                temp_df['price'] = temp_df['price'].astype(np.float32)
                temp_df['sales_channel_id'] = temp_df['sales_channel_id'].astype(np.int8)
                
                mask = (temp_df['t_dat'] >= TRAIN_START_DATE) & (temp_df['t_dat'] <= DATASET_MAX_DATE)
                temp_df = temp_df.loc[mask]
                
                if not temp_df.empty:
                    chunk_list.append(temp_df)

        if not chunk_list:
             raise ValueError("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: JSON íŒŒì¼ ë‚´ìš©ì´ë‚˜ ë‚ ì§œ ë²”ìœ„ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
             
        print("Merging chunks...")
        df = pd.concat(chunk_list, ignore_index=True)
        del chunk_list, buffer
        gc.collect()
        
        print("Sorting...")
        df = df.sort_values(by=['customer_id', 't_dat']).reset_index(drop=True)
            
        print(f"Saving cache to {CACHE_FILE_PATH}...")
        df.to_parquet(CACHE_FILE_PATH, index=False)

    train_df = df[df['t_dat'] < VALID_START_DATE].copy()
    print(f" -> Loaded: {len(df)} rows, Train: {len(train_df)} rows")
    return df, train_df

# ==========================================
# 2. Item Features (Cleaned)
# ==========================================
def make_item_features(train_df):
    print("\nğŸ“¦ [Item Stats] Calculating...")

    # A. Raw Probability
    total_tx = len(train_df)
    item_counts = train_df['article_id'].value_counts()
    # float32 ëª…ì‹œ
    item_feats = pd.DataFrame({'raw_probability': (item_counts / total_tx).astype(np.float32)})
    item_feats.index.name = 'article_id'

    # B. Pivot Sales
    train_df['week_start'] = train_df['t_dat'] - pd.to_timedelta(train_df['t_dat'].dt.dayofweek, unit='D')
    weekly_sales = train_df.groupby(['article_id', 'week_start']).size().unstack(fill_value=0).sort_index(axis=1)
    
    # Archive
    print("   ğŸ’¾ Archiving Weekly History...")
    weekly_save = weekly_sales.copy()
    weekly_save.columns = weekly_save.columns.astype(str)
    weekly_save.reset_index().to_parquet(WEEKLY_HISTORY_PATH)
    del weekly_save; gc.collect()

    # C. Popularity & Velocity (Vectorized)
    last_4w = weekly_sales.iloc[:, -4:]
    prev_4w = weekly_sales.iloc[:, -8:-4]
    
    # Log1p ì ìš© (float32 ë³€í™˜ ë¶ˆí•„ìš”, pivot ê²°ê³¼ê°€ ì´ë¯¸ ìˆ«ìì„)
    item_feats['pop_1w_log'] = np.log1p(weekly_sales.iloc[:, -1].astype(np.float32))
    item_feats['pop_1m_log'] = np.log1p(last_4w.sum(axis=1).astype(np.float32))

    # Velocity
    s_curr_w = weekly_sales.iloc[:, -1]
    s_prev_w = weekly_sales.iloc[:, -2] if weekly_sales.shape[1] > 1 else 0
    item_feats['velocity_1w'] = ((s_curr_w - s_prev_w) / (s_prev_w + 1)).clip(-1, 5).astype(np.float32)

    s_curr_m = last_4w.sum(axis=1)
    s_prev_m = prev_4w.sum(axis=1) if len(prev_4w) > 0 else 0
    item_feats['velocity_1m'] = ((s_curr_m - s_prev_m) / (s_prev_m + 1)).clip(-1, 5).astype(np.float32)

    # Steady Score
    recent_12w = weekly_sales.iloc[:, -12:]
    mean_12w = recent_12w.mean(axis=1)
    std_12w = recent_12w.std(axis=1)
    item_feats['steady_score_log'] = np.log1p(mean_12w / (std_12w + 1e-9)).astype(np.float32)

    del weekly_sales; gc.collect()

    # Price
    item_feats['avg_item_price_log'] = np.log1p(train_df.groupby('article_id')['price'].mean().astype(np.float32))

    # D. Cold-Start Imputation
    first_sale = train_df.groupby('article_id')['t_dat'].min()
    max_date = train_df['t_dat'].max()
    
    # Days Calculation
    days_since = (max_date - first_sale).dt.days
    item_feats['days_since_release_log'] = np.log1p(days_since).astype(np.float32)
    
    # Imputation
    is_new = days_since < 14
    cols_to_impute = ['pop_1w_log', 'pop_1m_log', 'velocity_1w', 'velocity_1m']
    
    for col in cols_to_impute:
        if col in item_feats.columns:
            avg_val = item_feats[col].mean()
            item_feats.loc[is_new, col] = avg_val

    # E. Save
    item_feats = item_feats.reset_index()
    final_cols = ['article_id', 'raw_probability'] + cols_to_impute + ['steady_score_log', 'avg_item_price_log', 'days_since_release_log']
    final_df = item_feats[final_cols].fillna(0)
    
    save_dataframe(final_df, ITEM_FEAT_PATH_PQ, ITEM_FEAT_PATH_JS)
    return final_df
'''
# ==========================================
# 3. User Features (Cleaned)
# ==========================================
def make_user_features(train_df):
    print("\nğŸ‘¤ [User Stats] Calculating...")
    user_stats = train_df.groupby('customer_id').agg({
        'price': ['mean', 'count'],
        'article_id': 'nunique',
        't_dat': 'max',
        'sales_channel_id': 'mean'
    })
    user_stats.columns = ['user_avg_price', 'total_cnt', 'unique_item_cnt', 'last_purchase_date', 'channel_avg']
    user_stats = user_stats.reset_index()

    # íŒŒìƒ ë³€ìˆ˜ (ëª…ì‹œì  float32 ë³€í™˜ìœ¼ë¡œ ì—ëŸ¬ ë°©ì§€)
    user_stats['user_avg_price_log'] = np.log1p(user_stats['user_avg_price'].astype(np.float32))
    user_stats['total_cnt_log'] = np.log1p(user_stats['total_cnt'].astype(np.float32))
    user_stats['repurchase_ratio'] = (1 - (user_stats['unique_item_cnt'] / user_stats['total_cnt'])).astype(np.float32)
    
    max_date = train_df['t_dat'].max()
    days_diff = (max_date - user_stats['last_purchase_date']).dt.days
    user_stats['recency_log'] = np.log1p(days_diff.astype(np.float32))
    
    user_stats['preferred_channel'] = np.where(user_stats['channel_avg'] > 1.5, 2, 1).astype(np.int8)
    
    final_cols = ['customer_id', 'user_avg_price_log', 'total_cnt_log', 'repurchase_ratio', 'recency_log', 'preferred_channel']
    final_df = user_stats[final_cols].fillna(0)
    
    save_dataframe(final_df, USER_FEAT_PATH_PQ, USER_FEAT_PATH_JS)
    return final_df

'''
import os
import gc
import numpy as np
import pandas as pd

def make_user_features(train_df, target_val_path):
    print("\nğŸ‘¤ [User Stats] Calculating Enhanced Features (with Bucketing & Scaling)...")




    print("\nğŸ¯ [Validation User Stats] Preparing point-in-time features...")

    # 1. í‰ê°€ ëŒ€ìƒ ìœ ì € ID ì¶”ì¶œ (ì •ë‹µì§€ê°€ ìˆëŠ” ìœ ì €ë“¤)
    target_val = pd.read_parquet(target_val_path)
    val_user_set = set(target_val['customer_id'].unique())
    print(f" -> Found {len(val_user_set):,} target users for validation.")

    # 2. 9/15 ì´ì „ ê±°ë˜ ì¤‘ 'í‰ê°€ ëŒ€ìƒ ìœ ì €'ì˜ ê¸°ë¡ë§Œ ì¶”ì¶œ
    # (ì´ë¯¸ full_dfê°€ 9/15 ì´ì „ ë°ì´í„°ë¼ë©´ ë‚ ì§œ í•„í„°ëŠ” ìƒëµ ê°€ëŠ¥)
    val_train_df = train_df[train_df['customer_id'].isin(val_user_set)].copy()
    print(f" -> Using {len(val_train_df):,} transaction records for feature calculation.")



    # ==========================================
    # 1. Basic Interaction Stats
    # ==========================================
    train_df['day_of_week'] = train_df['t_dat'].dt.dayofweek
    train_df['is_weekend'] = (train_df['day_of_week'] >= 5).astype(np.int8)
    train_df['month_id'] = train_df['t_dat'].dt.to_period('M')

    user_agg = train_df.groupby('customer_id').agg({
        'price': ['mean', 'std', 'last'],  
        'article_id': ['count', 'nunique'], 
        't_dat': 'max',                     
        'sales_channel_id': 'mean',         
        'is_weekend': 'mean',               
        'month_id': 'nunique'               
    })
    
    user_agg.columns = [
        'user_avg_price', 'price_std', 'last_price',
        'total_cnt', 'unique_item_cnt',
        'last_purchase_date', 'channel_avg', 
        'weekend_ratio', 'active_months'
    ]
    user_agg = user_agg.reset_index()

    # ==========================================
    # 2. Derived Features & Bucketing/Scaling
    # ==========================================
    print("   âš™ï¸ Generating Derived Features & Bucketing...")
    
    # (1) ê²°ì¸¡ì¹˜ ë° íŒŒìƒ ë³€ìˆ˜ ì²˜ë¦¬
    user_agg['price_std'] = user_agg['price_std'].fillna(0).astype(np.float32)
    user_agg['last_price_diff'] = (user_agg['last_price'] - user_agg['user_avg_price']).astype(np.float32)
    user_agg['repurchase_ratio'] = (1.0 - (user_agg['unique_item_cnt'] / user_agg['total_cnt'])).astype(np.float32)
    
    max_date = train_df['t_dat'].max()
    user_agg['recency_days'] = (max_date - user_agg['last_purchase_date']).dt.days.astype(np.float32)
    
    user_agg['preferred_channel'] = np.where(user_agg['channel_avg'] > 1.5, 2, 1).astype(np.int8)
    user_agg['active_months'] = user_agg['active_months'].astype(np.int16)

    # (2) Bucketing (Quantile ê¸°ë°˜ 10êµ¬ê°„ ë¶„í•  -> Categorical IDë¡œ ë³€í™˜)
    # ì¤‘ë³µê°’ì´ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ duplicates='drop' ì ìš©
    user_agg['user_avg_price_bucket'] = pd.qcut(user_agg['user_avg_price'], q=10, labels=False, duplicates='drop').astype(np.int8) + 1
    user_agg['total_cnt_bucket'] = pd.qcut(user_agg['total_cnt'], q=10, labels=False, duplicates='drop').astype(np.int8) + 1
    user_agg['recency_bucket'] = pd.qcut(user_agg['recency_days'], q=10, labels=False, duplicates='drop').astype(np.int8) + 1

    # (3) Continuous Features Scaling (Standardization: í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
    # ëª¨ë¸ì˜ Continuous MLPì— ë“¤ì–´ê°ˆ ë³€ìˆ˜ë“¤
    cont_cols = ['price_std', 'last_price_diff', 'repurchase_ratio', 'weekend_ratio']
    for col in cont_cols:
        col_mean = user_agg[col].mean()
        col_std = user_agg[col].std() + 1e-9 # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        user_agg[f'{col}_scaled'] = ((user_agg[col] - col_mean) / col_std).astype(np.float32)

    # ==========================================
    # 3. Customer Metadata Integration (ê³ ê° ì •ë³´ ë³‘í•©)
    # ==========================================
    print("   ğŸ‘¥ Loading & Merging Customer Metadata...")
    customers_df = pd.read_csv(USER_META_PATH)
    
    if 'postal_code' in customers_df.columns:
        customers_df = customers_df.drop(columns=['postal_code'])
        
    age_median = customers_df['age'].median()
    customers_df['age'] = customers_df['age'].fillna(age_median)
    # Age Bucketing (10êµ¬ê°„)
    customers_df['age_bucket'] = pd.qcut(customers_df['age'], q=10, labels=False, duplicates='drop').astype(np.int8) + 1
    
    customers_df['FN'] = customers_df['FN'].fillna(0).astype(np.int8)
    customers_df['Active'] = customers_df['Active'].fillna(0).astype(np.int8)
    
    customers_df['club_member_status'] = customers_df['club_member_status'].fillna('OTHER').astype(str).str.upper()
    status_map = {'ACTIVE': 1, 'PRE-CREATE': 2}
    customers_df['club_member_status_idx'] = customers_df['club_member_status'].map(status_map).fillna(0).astype(np.int8)
    
    customers_df['fashion_news_frequency'] = customers_df['fashion_news_frequency'].fillna('NONE').astype(str).str.upper()
    news_map = {'REGULARLY': 1}
    customers_df['fashion_news_frequency_idx'] = customers_df['fashion_news_frequency'].map(news_map).fillna(0).astype(np.int8)

    meta_cols = ['customer_id', 'age_bucket', 'FN', 'Active', 'club_member_status_idx', 'fashion_news_frequency_idx']
    customers_meta = customers_df[meta_cols]

    final_df = pd.merge(user_agg, customers_meta, on='customer_id', how='left')

    # ë³‘í•© í›„ ê²°ì¸¡ì¹˜ ë°©ì–´
    final_df['age_bucket'] = final_df['age_bucket'].fillna(0).astype(np.int8)
    for col in ['FN', 'Active', 'club_member_status_idx', 'fashion_news_frequency_idx']:
        final_df[col] = final_df[col].fillna(0).astype(np.int8)

    # ==========================================
    # 4. Final Selection & Save
    # ==========================================
    # ì €ì¥í•  ìµœì¢… ì»¬ëŸ¼ (Bucket IDs 4ê°œ, Scaled Cont 4ê°œ, Categorical IDs 5ê°œ)
    final_cols = [
        'customer_id', 
        'user_avg_price_bucket', 'total_cnt_bucket', 'recency_bucket', 'age_bucket', # Bucket IDs
        'price_std_scaled', 'last_price_diff_scaled', 'repurchase_ratio_scaled', 'weekend_ratio_scaled', # Scaled Cont
        'preferred_channel', 'active_months', 'FN', 'Active', 'club_member_status_idx', 'fashion_news_frequency_idx' # Categoricals
    ]
    
    final_df = final_df[final_cols].fillna(0)
    print("\nğŸ” [Check] Generated User Features (Top 5):")
    print(final_df.head(5).T.to_string())
    save_dataframe(final_df, USER_FEAT_VAL_PATH_PQ, USER_FEAT_VAL_PATH_JS)
    del user_agg, customers_df, customers_meta; gc.collect()
    print("   âœ… User features successfully calculated and saved!")
    
    return final_df
# ==========================================
# 4. Sequences (Cleaned)
# ==========================================
def process_sequence_row(group):
    import pandas as pd # â˜… ì—¬ê¸°ì— import ì¶”ê°€ (Windows ë©€í‹°í”„ë¡œì„¸ì‹± í•„ìˆ˜)
    
    # ì •ë ¬ (sort_valuesëŠ” ì–•ì€ ë³µì‚¬ì´ë¯€ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    group = group.sort_values('days_int')
    
    # Valuesë§Œ ì¶”ì¶œí•˜ì—¬ Numpy Arrayë¡œ ì²˜ë¦¬ (ë¹ ë¦„)
    article_ids = group['article_id'].values
    days_ints = group['days_int'].values
    
    if len(article_ids) > 50:
        article_ids = article_ids[-50:]
        days_ints = days_ints[-50:]
        
    # Vectorized subtraction
    time_deltas = days_ints[-1] - days_ints
    
    # Parquet ì €ì¥ì„ ìœ„í•´ list ë³€í™˜
    return pd.Series({
        'sequence_ids': list(article_ids),
        'sequence_deltas': list(time_deltas)
    })
def make_cleaned_sequences(full_df, processor, save_path):
    """
    ì •ì œëœ full_dfë¥¼ ë°”íƒ•ìœ¼ë¡œ 0ë²ˆ ë…¸ì´ì¦ˆê°€ ì—†ëŠ” ì‹œí€€ìŠ¤ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*50)
    print("ğŸ§¹ [Step 1] Filtering Transactions with Valid Items Only...")
    
    # ğŸŒŸ [í•µì‹¬] FeatureProcessorì— ë“±ë¡ëœ 7ë§Œ ê°œ ì•„ì´í…œë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    valid_item_set = set(processor.item_ids)
    initial_rows = len(full_df)
    
    # ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” ì•„ì´í…œ ê±°ë˜ë¥¼ ì—¬ê¸°ì„œ ì‚­ì œ (0ë²ˆ ì›ì²œ ë´‰ì‡„)
    full_df = full_df[full_df['article_id'].isin(valid_item_set)].copy()
    
    print(f" -> Removed {initial_rows - len(full_df):,} noise records.")
    print(f" -> Remaining Records: {len(full_df):,}")

    print("\nğŸ”— [Step 2] Building Sequences with Parallel Processing...")
    
    # ë‚ ì§œ ì •ìˆ˜ ë³€í™˜
    full_df['days_int'] = ((full_df['t_dat'] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')).astype(np.int32)
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
    mini_df = full_df[['customer_id', 'article_id', 'days_int']].copy()
    del full_df
    gc.collect()
    
    # ë³‘ë ¬ ì²˜ë¦¬ ì ìš©
    grouped = mini_df.groupby('customer_id')
    seq_df = grouped.parallel_apply(process_sequence_row)
    
    # ê²°ê³¼ ì €ì¥
    seq_df = seq_df.reset_index()
    seq_df.to_parquet(save_path, index=False)
    
    print(f" âœ… [Success] Cleaned sequence file saved to: {save_path}")
    print("="*50 + "\n")
    return seq_df
class FeatureProcessor:
    def __init__(self, user_path, item_path, seq_path):
        self.users = pd.read_parquet(user_path).set_index('customer_id')
        self.items = pd.read_parquet(item_path).set_index('article_id')
        self.seqs = pd.read_parquet(seq_path).set_index('customer_id')
        self.user_ids = self.users.index.tolist()
        self.user2id = {uid: i + 1 for i, uid in enumerate(self.user_ids)}
        self.item_ids = self.items.index.tolist()
        self.item2id = {iid: i + 1 for i, iid in enumerate(self.item_ids)}
        self.user_scaler = StandardScaler()
        self.u_dense_cols = ['user_avg_price_log', 'total_cnt_log', 'recency_log']
        self.users_scaled = self.users.copy()
        self.users_scaled[self.u_dense_cols] = self.user_scaler.fit_transform(self.users[self.u_dense_cols])

    def get_user_tensor(self, user_id):
        dense = torch.tensor(self.users_scaled.loc[user_id, self.u_dense_cols].values, dtype=torch.float32)
        cat = torch.tensor(int(self.users_scaled.loc[user_id, 'preferred_channel']) - 1, dtype=torch.long)
        return dense, cat

    def get_logq_probs(self, device):
        sorted_probs = self.items['raw_probability'].reindex(self.item_ids).fillna(0).values
        return torch.tensor(sorted_probs, dtype=torch.float32).to(device)




def deep_inspect_missing_items(full_df, processor):
    print("\nğŸ” [Deep Inspection] Identifying the source of 107k Zeros...")
    
    # 1. FeatureProcessorì— ë“±ë¡ëœ ìœ íš¨ ì•„ì´í…œ ID ì…‹
    valid_items = set(processor.item_ids)
    
    # 2. ê±°ë˜ ë°ì´í„°(full_df)ì—ì„œ ë“±ë¡ë˜ì§€ ì•Šì€ ì•„ì´í…œ ì°¾ê¸°
    is_invalid = ~full_df['article_id'].isin(valid_items)
    invalid_transactions = full_df[is_invalid]
    
    missing_count = len(invalid_transactions)
    unique_missing_items = invalid_transactions['article_id'].nunique()
    
    print(f" - Total Transactions with Missing Items: {missing_count:,}ê±´")
    print(f" - Unique Missing Item IDs: {unique_missing_items:,}ì¢…ë¥˜")
    
    if missing_count > 0:
        print("\nğŸ“Š [Top 10 Missing Items] ì´ ì•„ì´í…œë“¤ì´ 0ë²ˆì˜ ì£¼ë²”ì…ë‹ˆë‹¤:")
        print(invalid_transactions['article_id'].value_counts().head(10))
        
        # 3. ì¡°ì¹˜ ì œì•ˆ
        print("\nğŸ’¡ [Recommendation]")
        print(f" - ì´ {missing_count}ê±´ì˜ ë°ì´í„°ëŠ” í•™ìŠµ ì‹œ target_id=0ì„ ë§Œë“­ë‹ˆë‹¤.")
        print(f" - Recall í–¥ìƒì„ ìœ„í•´ full_dfì—ì„œ ìœ„ ì•„ì´í…œë“¤ì„ ì œê±°(drop)í•˜ê³  í•™ìŠµí•˜ì„¸ìš”.")
    else:
        print("âœ… All items in full_df are correctly mapped to Processor!")
        
        
        
import pandas as pd
import numpy as np
import os

def make_validation_user_features(full_df, target_val_path, save_path):
    """
    full_df: 9/15 ì´ì „ê¹Œì§€ì˜ ëª¨ë“  ê±°ë˜ ê¸°ë¡ (ì •ì œëœ ê²ƒ)
    target_val_path: features_target_val.parquet ê²½ë¡œ
    save_path: ì €ì¥í•  ê²½ë¡œ (USER_VAL_FEAT_PATH)
    """
    print("\nğŸ¯ [Validation User Stats] Preparing point-in-time features...")

    # 1. í‰ê°€ ëŒ€ìƒ ìœ ì € ID ì¶”ì¶œ (ì •ë‹µì§€ê°€ ìˆëŠ” ìœ ì €ë“¤)
    target_val = pd.read_parquet(target_val_path)
    val_user_set = set(target_val['customer_id'].unique())
    print(f" -> Found {len(val_user_set):,} target users for validation.")

    # 2. 9/15 ì´ì „ ê±°ë˜ ì¤‘ 'í‰ê°€ ëŒ€ìƒ ìœ ì €'ì˜ ê¸°ë¡ë§Œ ì¶”ì¶œ
    # (ì´ë¯¸ full_dfê°€ 9/15 ì´ì „ ë°ì´í„°ë¼ë©´ ë‚ ì§œ í•„í„°ëŠ” ìƒëµ ê°€ëŠ¥)
    val_train_df = full_df[full_df['customer_id'].isin(val_user_set)].copy()
    print(f" -> Using {len(val_train_df):,} transaction records for feature calculation.")

    # 3. ê¸°ì¡´ ë¡œì§ ê·¸ëŒ€ë¡œ ì‹¤í–‰ (make_user_featuresì˜ ë‚´ë¶€ ë¡œì§)
    print("ğŸ‘¤ Calculating user stats for validation...")
    user_stats = val_train_df.groupby('customer_id').agg({
        'price': ['mean', 'count'],
        'article_id': 'nunique',
        't_dat': 'max',
        'sales_channel_id': 'mean'
    })
    user_stats.columns = ['user_avg_price', 'total_cnt', 'unique_item_cnt', 'last_purchase_date', 'channel_avg']
    user_stats = user_stats.reset_index()

    # íŒŒìƒ ë³€ìˆ˜ ê³„ì‚°
    user_stats['user_avg_price_log'] = np.log1p(user_stats['user_avg_price'].astype(np.float32))
    user_stats['total_cnt_log'] = np.log1p(user_stats['total_cnt'].astype(np.float32))
    user_stats['repurchase_ratio'] = (1 - (user_stats['unique_item_cnt'] / user_stats['total_cnt'])).astype(np.float32)
    
    max_date = val_train_df['t_dat'].max()
    days_diff = (max_date - user_stats['last_purchase_date']).dt.days
    user_stats['recency_log'] = np.log1p(days_diff.astype(np.float32))
    
    user_stats['preferred_channel'] = np.where(user_stats['channel_avg'] > 1.5, 2, 1).astype(np.int8)
    
    final_cols = ['customer_id', 'user_avg_price_log', 'total_cnt_log', 'repurchase_ratio', 'recency_log', 'preferred_channel']
    final_df = user_stats[final_cols].fillna(0)
    
    # 4. ì €ì¥ (í‰ê°€ ì „ìš© ê²½ë¡œë¡œ ì €ì¥)
    final_df.to_parquet(save_path, index=False)
    print(f" âœ¨ [Success] Validation user features saved to: {save_path}")
    return final_df

        
def make_validation_sequences(full_df, target_val_path, save_path, processor):
    """
    full_df: 9/15 ì´ì „ì˜ ê±°ë˜ ê¸°ë¡ (train_dfë¥¼ ë„£ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤)
    target_val_path: features_target_val.parquet (ì •ë‹µì§€)
    save_path: features_sequence_val.parquet
    """
    print("\nğŸ”— [Validation Sequences] Creating point-in-time sequences for target users...")
    
    # 1. 7ë§Œ ê°œ ëª…ë‹¨ì— ìˆëŠ” ì•„ì´í…œë§Œ í•„í„°ë§ (0ë²ˆ ë…¸ì´ì¦ˆ ì›ì²œ ë´‰ì‡„)
    valid_item_set = set(processor.item_ids)
    initial_len = len(full_df)
    
    # ë³€ìˆ˜ëª…ì„ full_dfë¡œ í†µì¼í•˜ê±°ë‚˜, ê¹”ë”í•˜ê²Œ ì—¬ê¸°ì„œë¶€í„° dfë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    df = full_df[full_df['article_id'].isin(valid_item_set)].copy() 
    
    deleted = initial_len - len(df)
    if deleted > 0:
        print(f"ğŸ§¹ ì‹œí€€ìŠ¤ ìƒì„± ì „ {deleted:,}ê±´ì˜ ë¯¸ë“±ë¡ ì•„ì´í…œì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.")

    # 2. í‰ê°€ ëŒ€ìƒ 6.5ë§Œ ëª… ìœ ì € ID ì¶”ì¶œ
    target_val = pd.read_parquet(target_val_path)
    val_user_set = set(target_val['customer_id'].unique())
    print(f" -> Found {len(val_user_set):,} target users for validation.")

    # 3. ì •ì œëœ ë°ì´í„°ì—ì„œ 'í‰ê°€ ëŒ€ìƒ ìœ ì €'ì˜ ê¸°ë¡ë§Œ ì¶”ì¶œ
    val_train_df = df[df['customer_id'].isin(val_user_set)].copy()
    
    if val_train_df.empty:
        print("âš ï¸ Warning: í•„í„°ë§ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ID ë§¤í•‘ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

    # 4. ì‹œí€€ìŠ¤ ìƒì„± ë¡œì§ ì‹¤í–‰
    print(f" -> Processing sequences for {len(val_user_set):,} users...")
    
    # ë‚ ì§œ ì •ìˆ˜ ë³€í™˜
    val_train_df['days_int'] = ((val_train_df['t_dat'] - pd.Timestamp("1970-01-01")) // pd.Timedelta('1D')).astype(np.int32)
    
    grouped = val_train_df.groupby('customer_id')
    # ë³‘ë ¬ ì²˜ë¦¬ ì ìš© (process_sequence_row í˜¸ì¶œ)
    seq_df = grouped.parallel_apply(process_sequence_row) 
    
    seq_df = seq_df.reset_index()
    seq_df.to_parquet(save_path, index=False)
    
    print(f" âœ¨ [Success] Validation sequences saved to: {save_path}")
    return seq_df

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np




def check_sequence_distribution(train_seq_path, valid_seq_path):
    print("ğŸ“ˆ [Data Audit] Comparing Sequence Length Distributions...")
    
    # 1. ë°ì´í„° ë¡œë“œ
    train_seq = pd.read_parquet(train_seq_path)
    valid_seq = pd.read_parquet(valid_seq_path)
    
    # 2. ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°
    train_lens = train_seq['sequence_ids'].apply(len)
    valid_lens = valid_seq['sequence_ids'].apply(len)
    
    # 3. ê¸°ì´ˆ í†µê³„ëŸ‰ ë¹„êµ í…Œì´ë¸” ìƒì„±
    stats = pd.DataFrame({
        'Dataset': ['Train (All)', 'Valid (Target Users)'],
        'Count': [len(train_lens), len(valid_lens)],
        'Mean': [train_lens.mean(), valid_lens.mean()],
        'Median': [train_lens.median(), valid_lens.median()],
        'Std': [train_lens.std(), valid_lens.std()],
        'Min': [train_lens.min(), valid_lens.min()],
        'Max': [train_lens.max(), valid_lens.max()]
    })
    
    print("\n[Check 1] Descriptive Statistics:")
    print(stats.to_string(index=False))

    # 4. ì‹œê°í™” (Distribution Plot)
    plt.figure(figsize=(12, 5))
    
    sns.histplot(train_lens, color='skyblue', label='Train', kde=True, stat="probability", bins=30)
    sns.histplot(valid_lens, color='orange', label='Valid', kde=True, stat="probability", bins=30)
    
    plt.title('Sequence Length Distribution Comparison')
    plt.xlabel('Sequence Length (Number of Items)')
    plt.ylabel('Density (Probability)')
    plt.legend()
    plt.grid(axis='y', alpha=0.3)
    plt.show()

    # 5. ë¶„ì„ ì˜ê²¬ ì¶œë ¥
    diff = abs(train_lens.mean() - valid_lens.mean())
    if diff < 5:
        print(f"\nâœ… SUCCESS: ë¶„í¬ê°€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤. (í‰ê·  ì°¨ì´: {diff:.2f})")
    else:
        print(f"\nâš ï¸ WARNING: ë¶„í¬ ì°¨ì´ê°€ í½ë‹ˆë‹¤. (í‰ê·  ì°¨ì´: {diff:.2f})")
        print(" -> Valid ìœ ì €ë“¤ì´ ìƒëŒ€ì ìœ¼ë¡œ í—¤ë¹„ ìœ ì €ì´ê±°ë‚˜ ë¼ì´íŠ¸ ìœ ì €ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ì‹¤í–‰
# check_sequence_distribution(SEQ_DATA_PATH_PQ, SEQ_VAL_DATA_PATH)
 
import pandas as pd
import numpy as np

def final_sanity_check(seq_val_path, target_val_path):
    print("ğŸ” [Final Guardrail] Verifying Validation Data Integrity...")
    
    # 1. ë°ì´í„° ë¡œë“œ
    seq_val = pd.read_parquet(seq_val_path)
    target_val = pd.read_parquet(target_val_path)
    
    # 2. ìœ ì € ìˆ˜ ì¼ì¹˜ í™•ì¸
    target_users = set(target_val['customer_id'].unique())
    seq_users = set(seq_val['customer_id'].unique())
    
    missing_in_seq = target_users - seq_users
    
    print(f"\n[Check 1] User Count Consistency")
    print(f" - Target Ground Truth Users: {len(target_users):,}ëª…")
    print(f" - Sequence Data Users: {len(seq_users):,}ëª…")
    
    if len(missing_in_seq) == 0:
        print(" âœ… SUCCESS: ëª¨ë“  íƒ€ê²Ÿ ìœ ì €ì˜ ì‹œí€€ìŠ¤ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.")
    else:
        print(f" âš ï¸ WARNING: {len(missing_in_seq):,}ëª…ì˜ ìœ ì € ì‹œí€€ìŠ¤ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(" (ì‚¬ìœ : í•´ë‹¹ ìœ ì €ë“¤ì´ 9/15 ì´ì „ì— êµ¬ë§¤í•œ ê¸°ë¡ì´ ì „í˜€ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")

    # 3. 0ë²ˆ(Unknown Item) í¬í•¨ ì—¬ë¶€ í™•ì¸
    # sequence_ids ë¦¬ìŠ¤íŠ¸ ì•ˆì— 0ì´ í•˜ë‚˜ë¼ë„ ìˆëŠ”ì§€ ì „ìˆ˜ì¡°ì‚¬
    contains_zero = seq_val['sequence_ids'].apply(lambda x: 0 in x).sum()
    
    print(f"\n[Check 2] Zero-ID (Noise) Check")
    if contains_zero == 0:
        print(" âœ… SUCCESS: ì‹œí€€ìŠ¤ ë‚´ì— '0'ë²ˆ(ì•Œ ìˆ˜ ì—†ëŠ” ì•„ì´í…œ)ì´ ì „í˜€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f" âŒ ERROR: {contains_zero:,}ê°œì˜ í–‰ì—ì„œ '0'ë²ˆì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤! í•„í„°ë§ ë¡œì§ì„ ì¬ì ê²€í•˜ì„¸ìš”.")

    # 4. ì‹œí€€ìŠ¤ ê¸¸ì´ ì ì ˆì„± í™•ì¸
    avg_len = seq_val['sequence_ids'].apply(len).mean()
    print(f"\n[Check 3] Sequence Quality")
    print(f" - Average Sequence Length: {avg_len:.2f}")
    if avg_len >= 15:
        print(" âœ… SUCCESS: í’ë¶€í•œ ë§¥ë½ ì •ë³´ê°€ í™•ë³´ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(" â„¹ï¸ INFO: í‰ê·  ì‹œí€€ìŠ¤ê°€ ë‹¤ì†Œ ì§§ìŠµë‹ˆë‹¤.")

    print("\n" + "="*50)
    if len(missing_in_seq) == 0 and contains_zero == 0:
        print("ğŸš€ ALL SYSTEMS GO! ì´ì œ Phase 2.5 í•™ìŠµì„ ì‹œì‘í•˜ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤.")
    else:
        print("ğŸ› ï¸ ìœ„ ê²½ê³ /ì—ëŸ¬ ì‚¬í•­ì„ í™•ì¸ í›„ ì§„í–‰ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ì„¸ìš”.")
    print("="*50)

# ì‹¤í–‰
# final_sanity_check(SEQ_VAL_DATA_PATH, TARGET_VAL_PATH)   

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_and_visualize_user_stats(parquet_path):
    """
    ì €ì¥ëœ user_features parquet íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ 
    ê¸°ì´ˆ í†µê³„ëŸ‰ ì¶œë ¥ ë° ë¶„í¬ ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ“‚ Loading data from: {parquet_path}")
    df = pd.read_parquet(parquet_path)
    
    # 1. ê¸°ì´ˆ í†µê³„ëŸ‰ (ìµœëŒ€, ìµœì†Œ, í‰ê· , 4ë¶„ìœ„ìˆ˜) ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“Š [1. Summary Statistics]")
    print("="*50)
    # ê°€ë…ì„±ì„ ìœ„í•´ ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ í‘œì‹œ
    pd.set_option('display.float_format', lambda x: '%.4f' % x)
    stats_df = df.describe().T[['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]
    print(stats_df)
    
    # ë¶„ì„í•  ì»¬ëŸ¼ ì •ì˜
    # customer_id ì œì™¸, preferred_channelì€ ë²”ì£¼í˜•ìœ¼ë¡œ ì·¨ê¸‰
    numeric_cols = ['user_avg_price_log', 'total_cnt_log', 'repurchase_ratio', 'recency_log']
    cat_col = 'preferred_channel'
    
    # ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •
    sns.set(style="whitegrid")
    
    # 2. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ ì‹œê°í™” (Histogram + Boxplot)
    print("\n" + "="*50)
    print("ğŸ“ˆ [2. Numeric Features Distribution]")
    print("="*50)
    
    fig, axes = plt.subplots(len(numeric_cols), 2, figsize=(16, 5 * len(numeric_cols)))
    
    for i, col in enumerate(numeric_cols):
        # (1) íˆìŠ¤í† ê·¸ë¨ & KDE (ë¶„í¬ ëª¨ì–‘ í™•ì¸)
        sns.histplot(df[col], kde=True, ax=axes[i, 0], color='skyblue', bins=50)
        axes[i, 0].set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')
        axes[i, 0].set_xlabel('')
        
        # í‰ê· ê³¼ ì¤‘ì•™ê°’ ì„  í‘œì‹œ
        axes[i, 0].axvline(df[col].mean(), color='red', linestyle='--', label='Mean')
        axes[i, 0].axvline(df[col].median(), color='green', linestyle='-', label='Median')
        axes[i, 0].legend()
        
        # (2) ë°•ìŠ¤í”Œë¡¯ (ì´ìƒì¹˜ í™•ì¸)
        sns.boxplot(x=df[col], ax=axes[i, 1], color='lightgreen')
        axes[i, 1].set_title(f'Boxplot of {col}', fontsize=14, fontweight='bold')
        axes[i, 1].set_xlabel('')
        
    plt.tight_layout()
    plt.show()
    
    # 3. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ (Channel)
    print("\n" + "="*50)
    print("ğŸ“Š [3. Categorical Feature Balance]")
    print("="*50)
    
    plt.figure(figsize=(8, 5))
    ax = sns.countplot(x=cat_col, data=df, palette='viridis')
    plt.title(f'Count of {cat_col} (1: Offline/Mixed, 2: Online)', fontsize=14, fontweight='bold')
    
    # ë°” ìœ„ì— ì¹´ìš´íŠ¸ ìˆ«ì í‘œì‹œ
    for p in ax.patches:
        ax.annotate(f'{int(p.get_height()):,}', (p.get_x() + p.get_width() / 2., p.get_height()), 
                    ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')
    plt.show()
    
    # 4. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ (ë³€ìˆ˜ ê°„ ë‹¤ì¤‘ê³µì„ ì„± ì²´í¬)
    print("\n" + "="*50)
    print("ğŸ”¥ [4. Correlation Heatmap]")
    print("="*50)
    
    plt.figure(figsize=(10, 8))
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ + ì±„ë„ë§Œ í¬í•¨
    corr_matrix = df[numeric_cols + [cat_col]].corr()
    
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool)) # ìƒë‹¨ ì‚¼ê°í˜• ê°€ë¦¬ê¸°
    sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', mask=mask, vmin=-1, vmax=1)
    plt.title('Feature Correlation Matrix', fontsize=16)
    plt.show()

# ==========================================
# ì‹¤í–‰ ì˜ˆì‹œ
# ==========================================
# ì €ì¥í–ˆë˜ ê²½ë¡œë¥¼ ê·¸ëŒ€ë¡œ ë„£ì–´ì£¼ì„¸ìš”
# USER_VAL_FEAT_PATH = "D:/trainDataset/localprops/features_user_val.parquet" (ì˜ˆì‹œ)
# analyze_and_visualize_user_stats(USER_VAL_FEAT_PATH)
# ==========================================
# Main Execution
# ==========================================
if __name__ == "__main__":
    # 1. Load Data
    full_df, train_df = load_data()
    gc.collect()
    
    
    # 2. ì„¤ì •ëœ ê²½ë¡œ ë° ë‚ ì§œ ì‚¬ìš©
    VALID_START_DATE = pd.to_datetime("2020-09-16")
    DATASET_MAX_DATE = pd.to_datetime("2020-09-22")
    TARGET_VAL_PATH = os.path.join(BASE_DIR, "features_target_val.parquet")


    #processor = FeatureProcessor(USER_FEAT_PATH_PQ, ITEM_FEAT_PATH_PQ, SEQ_DATA_PATH_PQ)

    #CLEANED_SEQ_PATH = os.path.join(BASE_DIR, "features_sequence_cleaned.parquet")
    #new_seq_df = make_cleaned_sequences(full_df, processor, CLEANED_SEQ_PATH)
    

    #USER_VAL_FEAT_PATH = os.path.join(BASE_DIR, "features_user_val.parquet")
    #val_user_features = make_validation_user_features(full_df, TARGET_VAL_PATH, USER_VAL_FEAT_PATH)

    #SEQ_VAL_DATA_PATH = os.path.join(BASE_DIR, "features_sequence_val.parquet")
    #make_validation_sequences(full_df, TARGET_VAL_PATH, SEQ_VAL_DATA_PATH)
    train_only_df = full_df[full_df['t_dat'] < VALID_START_DATE].copy()
    
    
    
    TARGET_VAL_PATH = os.path.join(BASE_DIR, "features_target_val.parquet")
    
    make_user_features(train_only_df,TARGET_VAL_PATH)
    
    
    # 2. Validationìš© ì •ë‹µì§€ ë¨¼ì € ìƒì„± (ì´ê±´ full_dfê°€ í•„ìš”í•¨)
    TARGET_VAL_PATH = os.path.join(BASE_DIR, "features_target_val.parquet")
    #make_validation_target_file(full_df, VALID_START_DATE, DATASET_MAX_DATE, TARGET_VAL_PATH)

    # 3. ğŸŒŸ [í•µì‹¬ ìˆ˜ì •] Validationìš© í”¼ì²˜ ìƒì„± ì‹œ ë°˜ë“œì‹œ 'train_df'ë¥¼ ë„£ìœ¼ì„¸ìš”!
    #USER_VAL_FEAT_PATH = os.path.join(BASE_DIR, "features_user_val.parquet")
    # full_df ëŒ€ì‹  train_dfë¥¼ ë„£ì–´ì•¼ 9/16 ì´í›„ ë°ì´í„°ê°€ ì¹¨ë²”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    #val_user_features = make_validation_user_features(train_df, TARGET_VAL_PATH, USER_VAL_FEAT_PATH)

    # 4. ğŸŒŸ [í•µì‹¬ ìˆ˜ì •] Validationìš© ì‹œí€€ìŠ¤ ìƒì„± ì‹œì—ë„ 'train_df'ë¥¼ ë„£ìœ¼ì„¸ìš”!
    #SEQ_VAL_DATA_PATH = os.path.join(BASE_DIR, "features_sequence_val.parquet")
    # full_df ëŒ€ì‹  train_dfë¥¼ ë„£ì–´ì•¼ ì‹œí€€ìŠ¤ ë‚´ì— 'ë¯¸ë˜ì˜ ì •ë‹µ'ì´ í¬í•¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    #make_validation_sequences(train_df, TARGET_VAL_PATH, SEQ_VAL_DATA_PATH,processor)
    
    
    #final_sanity_check(SEQ_VAL_DATA_PATH, TARGET_VAL_PATH)   


    USER_VAL_FEAT_PATH = "D:/trainDataset/localprops/features_user_val.parquet"
    #analyze_and_visualize_user_stats(USER_FEAT_PATH_PQ)


    #check_sequence_distribution(SEQ_DATA_PATH_PQ, SEQ_VAL_DATA_PATH)
    '''
    # 2. Item Stats
    make_item_features(train_df)
    del train_df; gc.collect()
    
    # 3. User Stats (Train Only)
    train_only_df = full_df[full_df['t_dat'] < VALID_START_DATE].copy()
    make_user_features(train_only_df)
    del train_only_df; gc.collect()
    
    # 4. Sequences (Train Only)
    train_seq_df = full_df[full_df['t_dat'] < VALID_START_DATE].copy()
    del full_df; gc.collect() # full_df ì‚­ì œí•˜ì—¬ ë©”ëª¨ë¦¬ ìµœëŒ€ë¡œ í™•ë³´
    
    make_sequences(train_seq_df)
    
    
    
    
    
    
    
    
    # 3. í•¨ìˆ˜ í˜¸ì¶œ
    make_validation_target_file(
        full_df=full_df, 
        valid_start_date=VALID_START_DATE, 
        max_date=DATASET_MAX_DATE, 
        save_path=TARGET_VAL_PATH
    )

    '''
