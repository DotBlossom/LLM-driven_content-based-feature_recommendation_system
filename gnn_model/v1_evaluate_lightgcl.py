import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import pandas as pd
import numpy as np
import os
from tqdm import tqdm
import numpy as np
import scipy.sparse as sp
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))

project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.append(project_root)
    
from tower_code.inference_utils import ITEM_FEAT_PATH_PQ, SEQ_DATA_PATH_PQ, SEQ_VAL_DATA_PATH, TARGET_VAL_PATH, USER_FEAT_PATH_PQ, USER_VAL_FEAT_PATH, FeatureProcessor

# =========================================================
# 1. Inferenceìš© ê»ë°ê¸° ëª¨ë¸
# =========================================================
class LightGCL_InferenceWrapper(nn.Module):
    def __init__(self, num_users, num_items, emb_dim=64):
        super().__init__()
        # Padding(0) í¬í•¨
        self.embedding_user = nn.Embedding(num_users, emb_dim, padding_idx=0)
        self.embedding_item = nn.Embedding(num_items, emb_dim, padding_idx=0)
        
    def forward(self, u_idx):
        return self.embedding_user(u_idx)

def build_sparse_graph(user_ids, item_ids, train_df, device):
    """
    Train DataFrame(Parquet)ì„ ì½ì–´ LightGCL í•™ìŠµ ë•Œì™€ ë™ì¼í•œ 
    Normalized Adjacency Matrixë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("âš¡ Building Graph Adjacency Matrix...")
    
    n_users = len(user_ids) + 1 # 0ë²ˆ padding ê³ ë ¤ (Processor ê¸°ì¤€)
    n_items = len(item_ids) + 1
    
    # 1. ID ë§¤í•‘ ì¤€ë¹„ (Processorì˜ ID ì²´ê³„ ì‚¬ìš©)
    # user_ids, item_idsëŠ” processor.user_ids, processor.item_ids
    u_mapper = {uid: i+1 for i, uid in enumerate(user_ids)}
    i_mapper = {iid: i+1 for i, iid in enumerate(item_ids)}
    
    # 2. ì»¬ëŸ¼ëª… ìë™ ê°ì§€ (sequence_ids ì¶”ê°€)
    u_col = 'customer_id' if 'customer_id' in train_df.columns else train_df.columns[0]
    
    # 'sequence_ids'ë¥¼ ìš°ì„ ìˆœìœ„ë¡œ ë‘ 
    possible_item_cols = ['sequence_ids', 'article_id', 'item_id', 'product_id', 'article_ids']
    i_col = next((col for col in possible_item_cols if col in train_df.columns), None)
    
    if i_col is None:
        raise KeyError(f"âŒ Item column not found! Available: {train_df.columns}")
    
    print(f"   -> Using columns: User='{u_col}', Item='{i_col}'")

    # 3. ì—£ì§€(Edge) ì¶”ì¶œ
    src = []
    dst = []
    
    valid_interactions = 0
    
    # DataFrame ìˆœíšŒ
    for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc="   -> Mapping Edges"):
        u_val = row[u_col]
        i_val = row[i_col] # ì´ê²Œ ë¦¬ìŠ¤íŠ¸ì¼ í™•ë¥  99% (sequence_ids)
        
        # ìœ ì €ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê±´ë„ˆëœ€
        if u_val not in u_mapper:
            continue
            
        u_idx = u_mapper[u_val]
        
        # [í•µì‹¬] ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(sequence_ids) ì²˜ë¦¬
        if isinstance(i_val, (list, np.ndarray)):
            for item in i_val:
                if item in i_mapper:
                    src.append(u_idx)
                    dst.append(i_mapper[item])
                    valid_interactions += 1
        # ë‹¨ì¼ ê°’ ì²˜ë¦¬ (í˜¹ì‹œ ëª¨ë¥¼ ëŒ€ë¹„)
        else:
            if i_val in i_mapper:
                src.append(u_idx)
                dst.append(i_mapper[i_val])
                valid_interactions += 1
            
    print(f"   -> Valid Edges: {valid_interactions}")
    
    if valid_interactions == 0:
        raise ValueError("âŒ No valid interactions found! Check ID matching.")

    # 4. Sparse Matrix ìƒì„± (Train ì½”ë“œ ë¡œì§ ì¤€ìˆ˜)
    # User-Item Interaction Matrix R
    # shape: (n_users, n_items)
    # src(user indices), dst(item indices)
    
    # ì¤‘ë³µ ì œê±° (Userê°€ ê°™ì€ ì•„ì´í…œì„ ì—¬ëŸ¬ ë²ˆ ìƒ€ì„ ìˆ˜ ìˆìŒ -> Graph EdgeëŠ” 1ê°œë¡œ ì·¨ê¸‰)
    # coo_matrix ìƒì„± ì‹œ ì¤‘ë³µëœ ì¢Œí‘œëŠ” ê°’ì´ ë”í•´ì§€ë¯€ë¡œ, ì¼ë‹¨ ë§Œë“¤ê³  1ë¡œ ë§Œë“¦
    R = sp.coo_matrix((np.ones(len(src)), (src, dst)), shape=(n_users, n_items))
    # 0ë³´ë‹¤ í° ê°’ì€ 1ë¡œ (Interaction ì—¬ë¶€ë§Œ ì¤‘ìš”)
    R.data = np.ones_like(R.data) 

    # 5. Adjacency Matrix A ìƒì„±
    # [ 0, R ]
    # [ R.T, 0 ]
    # Training ì½”ë“œì—ì„œëŠ” sp.coo_matrixë¡œ ì§ì ‘ ì¢Œí‘œë¥¼ í•©ì³ì„œ ë§Œë“¤ì—ˆì§€ë§Œ,
    # ì—¬ê¸°ì„œëŠ” Rì„ ê¸°ë°˜ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    
    R = R.tocoo()
    
    # User Node: 0 ~ n_users-1
    # Item Node: n_users ~ n_users + n_items - 1 (Offset ì ìš©)
    user_nodes = R.row
    item_nodes = R.col + n_users
    
    # ìƒë‹¨ ìš°ì¸¡ (User -> Item)
    row_idx = np.concatenate([user_nodes, item_nodes])
    col_idx = np.concatenate([item_nodes, user_nodes])
    data = np.ones(len(row_idx), dtype=np.float32)
    
    num_nodes = n_users + n_items
    adj_mat = sp.coo_matrix((data, (row_idx, col_idx)), shape=(num_nodes, num_nodes))
    
    # 6. Normalization (Train ì½”ë“œì™€ ì™„ë²½ ë™ì¼ ë¡œì§)
    # D^-0.5 * A * D^-0.5
    rowsum = np.array(adj_mat.sum(axis=1)).flatten()
    d_inv = np.power(rowsum, -0.5)
    d_inv[np.isinf(d_inv)] = 0.
    d_mat = sp.diags(d_inv)
    
    norm_adj = d_mat.dot(adj_mat).dot(d_mat)
    norm_adj = norm_adj.tocoo()
    
    # 7. Torch Sparse Tensor ë³€í™˜
    indices = torch.from_numpy(np.vstack((norm_adj.row, norm_adj.col)).astype(np.int64))
    values = torch.from_numpy(norm_adj.data).float()
    shape = torch.Size(norm_adj.shape)
    
    adj_tensor = torch.sparse_coo_tensor(indices, values, shape).coalesce().to(device)
    
    return adj_tensor

def compute_final_embeddings(model, adj_tensor, n_layers=2):
    """
    ì €ì¥ëœ Weight(Layer 0)ë¥¼ ê·¸ë˜í”„ì— í†µê³¼ì‹œì¼œ Final Embeddingì„ ê³„ì‚°
    """
    print(f"\nğŸŒŠ Propagating Embeddings (Layers: {n_layers})...")
    model.eval()
    with torch.no_grad():
        # 1. ì´ˆê¸° ì„ë² ë”© ê²°í•© (User + Item)
        ego_embeddings = torch.cat([
            model.embedding_user.weight, 
            model.embedding_item.weight
        ], dim=0)
        
        all_embeddings = [ego_embeddings]
        
        # 2. ë ˆì´ì–´ ì „íŒŒ (Graph Convolution)
        for k in range(n_layers):
            # Sparse Matrix Multiplication (Message Passing)
            ego_embeddings = torch.sparse.mm(adj_tensor, ego_embeddings)
            all_embeddings.append(ego_embeddings)
            print(f"   -> Layer {k+1} done.")
            
        # 3. ë ˆì´ì–´ í‰ê·  (Mean Aggregation)
        # stack -> (Layers, Nodes, Dim) -> mean(dim=0)
        final_embeddings = torch.stack(all_embeddings, dim=0).mean(dim=0)
        
        # 4. ë‹¤ì‹œ User/Itemìœ¼ë¡œ ë¶„ë¦¬
        num_users = model.embedding_user.num_embeddings
        num_items = model.embedding_item.num_embeddings
        
        final_user_emb, final_item_emb = torch.split(final_embeddings, [num_users, num_items])
        
        return final_user_emb, final_item_emb
# =========================================================
# 2. ëª¨ë¸ ë¡œë“œ ë° ì •ë ¬ (Alignment) - ê°€ì¥ ì¤‘ìš” â­
# =========================================================
def load_and_align_model(model, processor, checkpoint_path, maps_path, device):
    """
    í•™ìŠµëœ .pth(ê³¼ê±° ID ìˆœì„œ)ë¥¼ ë¡œë“œí•˜ì—¬,
    í˜„ì¬ processor(ê²€ì¦ ID ìˆœì„œ)ì— ë§ê²Œ ì„ë² ë”© í–‰ë ¬ì„ ì¬ì¡°ë¦½í•©ë‹ˆë‹¤.
    """
    print(f"\nğŸ”„ [Alignment] Loading model weights and aligning IDs...")
    
    # 1. íŒŒì¼ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
    
    # í•™ìŠµ ë‹¹ì‹œ ID ë§µ ë¡œë“œ (ì´ê²Œ ì—†ìœ¼ë©´ ë³µì› ë¶ˆê°€ëŠ¥)
    saved_maps = torch.load(maps_path, map_location='cpu')
    train_user2id = saved_maps['user2id']
    train_item2id = saved_maps['item2id']
    
    # 2. User Embedding ì •ë ¬
    # í•™ìŠµëœ ì›ë³¸ ê°€ì¤‘ì¹˜
    raw_user_emb = state_dict['embedding_user.weight'] 
    # ìƒˆë¡œ ë§Œë“¤ ë¹ˆ í–‰ë ¬ (í˜„ì¬ Processor ê¸°ì¤€ í¬ê¸°)
    aligned_user_emb = torch.zeros(len(processor.user_ids) + 1, raw_user_emb.shape[1])
    
    u_hit = 0
    # í˜„ì¬ Processorì˜ ID ìˆœì„œëŒ€ë¡œ ìˆœíšŒí•˜ë©° í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì˜´
    for i, uid_str in enumerate(processor.user_ids):
        target_idx = i + 1 # 1-based index
        if uid_str in train_user2id:
            src_idx = train_user2id[uid_str]
            if src_idx < len(raw_user_emb):
                aligned_user_emb[target_idx] = raw_user_emb[src_idx]
                u_hit += 1
    
    model.embedding_user = nn.Embedding.from_pretrained(aligned_user_emb, freeze=True, padding_idx=0)
    print(f"   âœ… Users Aligned: {u_hit} / {len(processor.user_ids)} (Coverage: {u_hit/len(processor.user_ids):.2%})")

    # 3. Item Embedding ì •ë ¬
    raw_item_emb = state_dict['embedding_item.weight']
    aligned_item_emb = torch.zeros(len(processor.item_ids) + 1, raw_item_emb.shape[1])
    
    i_hit = 0
    for i, iid_str in enumerate(processor.item_ids):
        target_idx = i + 1
        if iid_str in train_item2id:
            src_idx = train_item2id[iid_str]
            if src_idx < len(raw_item_emb):
                aligned_item_emb[target_idx] = raw_item_emb[src_idx]
                i_hit += 1
                
    model.embedding_item = nn.Embedding.from_pretrained(aligned_item_emb, freeze=True, padding_idx=0)
    print(f"   âœ… Items Aligned: {i_hit} / {len(processor.item_ids)} (Coverage: {i_hit/len(processor.item_ids):.2%})")
    
    return model.to(device)

# =========================================================
# 3. ì •ë‹µ ë°ì´í„° ì „ì²˜ë¦¬ (String -> Integer Set)
# =========================================================
def prepare_ground_truth(target_df_path, processor):
    """
    í‰ê°€ ì†ë„ë¥¼ ìœ„í•´ String ID ì •ë‹µì§€ë¥¼ Integer Index ì§‘í•©ìœ¼ë¡œ ë¯¸ë¦¬ ë³€í™˜í•©ë‹ˆë‹¤.
    Return: {user_idx: {item_idx1, item_idx2, ...}}
    """
    print("\nâš¡ Preparing Ground Truth Data...")
    df = pd.read_parquet(target_df_path) # [customer_id, target_ids]
    
    ground_truth = {}
    
    # DataFrame ìˆœíšŒ
    for _, row in tqdm(df.iterrows(), total=len(df), desc="   -> Indexing Targets"):
        u_str = row['customer_id']
        t_list = row['target_ids']
        
        # Userê°€ Processorì— ì—†ìœ¼ë©´ í‰ê°€ ë¶ˆê°€ (Skip)
        if u_str not in processor.user2id:
            continue
            
        u_idx = processor.user2id[u_str]
        
        # Target Itemë“¤ë„ Integer IDë¡œ ë³€í™˜
        item_indices = set()
        for i_str in t_list:
            if i_str in processor.item2id:
                item_indices.add(processor.item2id[i_str])
        
        if item_indices: # ì •ë‹µì´ í•˜ë‚˜ë¼ë„ ìˆëŠ” ê²½ìš°ë§Œ
            ground_truth[u_idx] = item_indices
            
    print(f"   âœ… Ready to evaluate {len(ground_truth)} users.")
    return ground_truth

# =========================================================
# 4. í‰ê°€ ë£¨í”„ (Clean Logic)
# =========================================================
def evaluate_recall(model, ground_truth_dict, device, k_list=[20, 100], batch_size=4096):
    """
    [ìˆ˜ì •ë¨] Cosine Similarity ëŒ€ì‹  Dot Product ì‚¬ìš©
    """
    max_k = max(k_list)
    model.eval()
    
    eval_user_indices = list(ground_truth_dict.keys())
    
    # DataLoader
    loader = DataLoader(
        eval_user_indices, 
        batch_size=batch_size, 
        shuffle=False, 
        collate_fn=lambda x: torch.tensor(x, dtype=torch.long)
    )
    
    # 1. ì•„ì´í…œ ì„ë² ë”© ì¤€ë¹„ (Normalize ì œê±°! âŒ)
    with torch.no_grad():
        all_items = model.embedding_item.weight.data # (M, Dim)
        # all_items_norm = F.normalize(all_items, p=2, dim=1) <--- ì‚­ì œ
    
    hits = {k: 0 for k in k_list}
    total_users = 0
    
    print(f"\nğŸš€ Starting Recall@{k_list} Evaluation (Metric: Dot Product)...")
    
    with torch.no_grad():
        for batch_u_idx in tqdm(loader, desc="   -> Retrieving"):
            batch_u_idx = batch_u_idx.to(device)
            
            # 2. ìœ ì € ì„ë² ë”© ì¤€ë¹„ (Normalize ì œê±°! âŒ)
            user_emb = model.embedding_user(batch_u_idx)
            # user_norm = F.normalize(user_emb, p=2, dim=1) <--- ì‚­ì œ
            
            # 3. Score ê³„ì‚° (Pure Dot Product)
            # (Batch, Dim) @ (All_Items, Dim).T
            scores = torch.matmul(user_emb, all_items.T)
            
            # Padding ë§ˆìŠ¤í‚¹
            scores[:, 0] = -float('inf')
            
            # Top-K
            _, topk_indices = torch.topk(scores, k=max_k, dim=1)
            topk_cpu = topk_indices.cpu().numpy()
            batch_u_cpu = batch_u_idx.cpu().numpy()
            
            # Metric Check (ê¸°ì¡´ ë™ì¼)
            for i, u_idx in enumerate(batch_u_cpu):
                true_item_set = ground_truth_dict[u_idx]
                pred_list = topk_cpu[i]
                
                for k in k_list:
                    if not true_item_set.isdisjoint(pred_list[:k]):
                        hits[k] += 1
                        
            total_users += len(batch_u_cpu)

    # Report
    print(f"\n{'='*40}")
    print(f"ğŸ“Š LightGCL Final Report (Dot Product)")
    print(f"{'-'*40}")
    for k in sorted(k_list):
        recall = hits[k] / total_users
        print(f"Recall@{k:<3} | {recall:.4f}")
    print(f"{'='*40}\n")

# =========================================================
# 5. ì‹¤í–‰ë¶€ (Main)
# =========================================================
if __name__ == '__main__':
    # ì„¤ì • (ê²½ë¡œ ìˆ˜ì • í•„ìš”)
    BASE_DIR = r'D:\trainDataset\localprops'
    CACHE_DIR = os.path.join(BASE_DIR, 'cache')
    
    # 1. í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (Fine-tuning ì™„ë£Œëœ ëª¨ë¸)
    MODEL_PATH = os.path.join(CACHE_DIR, "lightgcl_best_finetuned_2401_ep3.pth") 
    MAPS_PATH = os.path.join(CACHE_DIR, "id_maps_train.pt") # í•™ìŠµ ì‹œ ì €ì¥í•œ ID ë§¤í•‘
    
    # 2. ê²€ì¦ìš© ë°ì´í„° ê²½ë¡œ (Parquet)
    TARGET_DF_PATH = os.path.join(BASE_DIR, "validation_targets.parquet") # [customer_id, target_ids]
    print("1ï¸âƒ£ Initializing Processors...")
    train_proc = FeatureProcessor(USER_FEAT_PATH_PQ, ITEM_FEAT_PATH_PQ, SEQ_DATA_PATH_PQ)
    valid_proc = FeatureProcessor(
        USER_VAL_FEAT_PATH,  # ê²€ì¦ ìœ ì € í”¼ì²˜
        ITEM_FEAT_PATH_PQ,   # ì•„ì´í…œ í”¼ì²˜ (ê³µìœ )
        SEQ_VAL_DATA_PATH,   # â­ í•µì‹¬: ê²€ì¦ìš© ì‹œí€€ìŠ¤ (Target ì œì™¸)
        scaler=train_proc.user_scaler # Scaler ê³µìœ 
    )
    
    # [ì¤‘ìš”] ID ë§¤í•‘ì„ Trainê³¼ ë™ì¼í•˜ê²Œ ê°•ì œ ì¼ì¹˜
    # (ìƒˆë¡œìš´ ì•„ì´í…œ/ìœ ì €ê°€ ìˆìœ¼ë©´ ë¬´ì‹œí•˜ê±°ë‚˜ ì²˜ë¦¬í•˜ê¸° ìœ„í•´)
    valid_proc.user2id = train_proc.user2id
    valid_proc.item2id = train_proc.item2id
    valid_proc.user_ids = train_proc.user_ids 
    valid_proc.item_ids = train_proc.item_ids
    num_users = len(train_proc.user_ids) + 1
    num_items = len(train_proc.item_ids) + 1
    

    print("1ï¸âƒ£ Initializing Processor...")
    # processor = FeatureProcessor(...) # ì‹¤ì œ ì½”ë“œì—ì„  ì´ê±¸ ì“°ì„¸ìš”


    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    model = LightGCL_InferenceWrapper(
        num_users=num_users,
        num_items=num_items
    )
    model = load_and_align_model(model, train_proc, MODEL_PATH, MAPS_PATH, device)
    
    
    
    train_df = pd.read_parquet(SEQ_DATA_PATH_PQ) 



    adj_tensor = build_sparse_graph(
        train_proc.user_ids, 
        train_proc.item_ids, 
        train_df,
        device
    )
    
    
    
    
    print("\nğŸ” [Check] Verifying Embedding Propagation...")

    # 1. ì „íŒŒ ì „ (Original Layer-0) ìƒíƒœ ì €ì¥
    # .clone()ì„ í•´ì•¼ ê°’ì´ ë³µì‚¬ë˜ì–´ ë”°ë¡œ ì €ì¥ë©ë‹ˆë‹¤.
    before_user_emb = model.embedding_user.weight.data.clone()
    before_mean = before_user_emb.mean().item()
    before_std = before_user_emb.std().item()

    print(f"   Original Weights | Mean: {before_mean:.6f} | Std: {before_std:.6f}")
    
    
    
    final_user_emb, final_item_emb = compute_final_embeddings(
    model, 
    adj_tensor, 
    n_layers=2 # í•™ìŠµ ë•Œ ì„¤ì •í•œ ë ˆì´ì–´ ìˆ˜ì™€ ë™ì¼í•´ì•¼ í•¨! (ë³´í†µ 2 or 3)
    )


    
    with torch.no_grad():
        model.embedding_user.weight.copy_(final_user_emb)
        model.embedding_item.weight.copy_(final_item_emb)

    print("âœ… Model updated with Propagated Embeddings.")
    # 4. ì „íŒŒ í›„ (Propagated Final) ìƒíƒœ í™•ì¸
    after_user_emb = model.embedding_user.weight.data
    after_mean = after_user_emb.mean().item()
    after_std = after_user_emb.std().item()

    print(f"   Propagated Weights | Mean: {after_mean:.6f} | Std: {after_std:.6f}")

    # 5. ê²°ê³¼ íŒì •
    if before_mean == after_mean:
        print("âŒ [FAIL] Embeddings did NOT change! (Something is wrong)")
        # ì›ì¸: compute_final_embeddings í•¨ìˆ˜ê°€ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í–ˆê±°ë‚˜, copy_ê°€ ì•ˆ ë¨¹í˜
    else:
        print("âœ… [SUCCESS] Embeddings successfully updated via Graph Propagation!")
        
        # ì–¼ë§ˆë‚˜ ë³€í–ˆëŠ”ì§€ ì°¨ì´ ê³„ì‚° (L2 Distance)
        diff = torch.norm(before_user_emb - after_user_emb).item()
        print(f"   -> Difference Magnitude: {diff:.4f}")
        
    
    ground_truth = prepare_ground_truth(TARGET_VAL_PATH, valid_proc)
    # 4. í‰ê°€
    evaluate_recall(model, ground_truth, device, k_list=[20, 100])