import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import numpy as np
import scipy.sparse as sp
import time
import os
import json
from tqdm import tqdm

# ==========================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ==========================================
def load_and_process_data(json_file_path, cache_dir):
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)

    cache_path = os.path.join(cache_dir, "processed_graph_train.pt")
    map_path = os.path.join(cache_dir, "id_maps_train.pt")

    if os.path.exists(cache_path) and os.path.exists(map_path):
        print(f"[Data] Cache Hit! Loading graph data from {cache_dir}...")
        data_cache = torch.load(cache_path)
        maps_cache = torch.load(map_path)
        return (data_cache['edge_index'], data_cache['num_users'], data_cache['num_items'], 
                maps_cache['user2id'], maps_cache['item2id'])

    print(f"[Data] Cache Miss! Processing {json_file_path}...")
    with open(json_file_path, 'r') as f: 
        raw_data = json.load(f)
    
    users = sorted(list(raw_data.keys()))
    user2id = {u: i for i, u in enumerate(users)}
    
    all_items = set()
    for item_list in raw_data.values():
        all_items.update(item_list)
    items = sorted(list(all_items))
    item2id = {i: idx for idx, i in enumerate(items)}
    
    src, dst = [], []
    for u, i_list in tqdm(raw_data.items(), desc="Building Graph"):
        if u not in user2id: continue
        uid = user2id[u]
        for i in i_list:
            if i in item2id:
                src.append(uid)
                dst.append(item2id[i])
                
    edge_index = torch.tensor([src, dst], dtype=torch.long)
    edge_index = torch.unique(edge_index, dim=1) 
    
    num_users, num_items = len(user2id), len(item2id)
    print(f" -> Users: {num_users}, Items: {num_items}, Edges: {edge_index.size(1)}")
    
    torch.save({'edge_index': edge_index, 'num_users': num_users, 'num_items': num_items}, cache_path)
    torch.save({'user2id': user2id, 'item2id': item2id}, map_path)
    
    return edge_index, num_users, num_items, user2id, item2id

# ==========================================
# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (Optimized)
# ==========================================
class TrainDataset(data.Dataset):
    def __init__(self, edge_index, num_users, num_items):
        self.num_users = num_users
        self.num_items = num_items
        
        # Tensorë¡œ ì €ì¥
        self.users = edge_index[0]
        self.items = edge_index[1]
        
        print(f"\n[Dataset] Preparing Negative Sampling Sets for {num_users} Users...")
        self.user_pos_set = [set() for _ in range(num_users)]
        
        # CPU ì—°ì‚° ê°€ì†ì„ ìœ„í•´ numpyë¡œ ë³€í™˜í•˜ì—¬ ìˆœíšŒ
        src = self.users.numpy()
        dst = self.items.numpy()
        
        for u, i in tqdm(zip(src, dst), total=len(src), desc="Indexing Interactions"):
            self.user_pos_set[u].add(i)
            
        print("[Dataset] Ready!")

    def __len__(self):
        return len(self.users)

    def __getitem__(self, idx):
        # [ìµœì í™”] torch.tensor() ë³€í™˜ì„ ì œê±°í•˜ê³  int/long íƒ€ì…ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
        # DataLoaderì˜ collate_fnì´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ì„ ë•Œ í•œ ë²ˆì— Tensorë¡œ ë³€í™˜í•˜ë¯€ë¡œ í›¨ì”¬ ë¹ ë¦„
        user = self.users[idx].item()
        pos_item = self.items[idx].item()
        
        neg_item = np.random.randint(0, self.num_items)
        while neg_item in self.user_pos_set[user]:
            neg_item = np.random.randint(0, self.num_items)
            
        return user, pos_item, neg_item

# ==========================================
# 3. ê·¸ë˜í”„ ë¹Œë”
# ==========================================
def build_graph(edge_index, num_users, num_items, device, q=5):
    print("Building Graph & Calculating SVD...")
    start = time.time()

    src = edge_index[0].numpy()
    dst = edge_index[1].numpy()
    
    user_nodes_R = src
    item_nodes_R = dst + num_users
    item_nodes_RT = dst + num_users
    user_nodes_RT = src
    
    rows = np.concatenate([user_nodes_R, item_nodes_RT])
    cols = np.concatenate([item_nodes_R, user_nodes_RT])
    data = np.ones(len(rows), dtype=np.float32)
    
    num_nodes = num_users + num_items
    adj_mat = sp.coo_matrix((data, (rows, cols)), shape=(num_nodes, num_nodes))
    
    rowsum = np.array(adj_mat.sum(axis=1)).flatten()
    d_inv = np.power(rowsum, -0.5)
    d_inv[np.isinf(d_inv)] = 0.
    d_mat = sp.diags(d_inv)
    
    norm_adj = d_mat.dot(adj_mat).dot(d_mat)
    norm_adj = norm_adj.tocoo() 

    indices = torch.from_numpy(np.vstack((norm_adj.row, norm_adj.col)).astype(np.int64))
    values = torch.from_numpy(norm_adj.data)
    shape = torch.Size(norm_adj.shape)
    
    adj_tensor = torch.sparse_coo_tensor(indices, values, shape).coalesce().to(device)
    U, S, V = torch.svd_lowrank(adj_tensor, q=q, niter=2)
    
    print(f"Graph Built & SVD Done ({time.time() - start:.2f}s)")
    return adj_tensor, U, S, V

# ==========================================
# 4. ëª¨ë¸
# ==========================================
class LightGCL(nn.Module):
    def __init__(self, num_users, num_items, config, adj_tensor, svd_components):
        super(LightGCL, self).__init__()
        self.num_users = num_users
        self.num_items = num_items
        self.emb_dim = config['emb_dim']
        self.n_layers = config['n_layers']
        self.temp = config['temp']
        self.lambda_ssl = config['lambda_ssl']
        
        self.adj = adj_tensor
        self.U, self.S, self.V = svd_components 
        
        self.embedding_user = nn.Embedding(num_users, self.emb_dim)
        self.embedding_item = nn.Embedding(num_items, self.emb_dim)
        
        nn.init.xavier_uniform_(self.embedding_user.weight)
        nn.init.xavier_uniform_(self.embedding_item.weight)

    def forward(self):
        all_emb = torch.cat([self.embedding_user.weight, self.embedding_item.weight])
        
        local_embs = [all_emb]
        x = all_emb
        for _ in range(self.n_layers):
            with torch.amp.autocast('cuda', enabled=False):
                x = x.float()
                x = torch.sparse.mm(self.adj, x)
            local_embs.append(x)
        local_final = torch.mean(torch.stack(local_embs, dim=1), dim=1)
        
        global_embs = [all_emb]
        x_g = all_emb
        for _ in range(self.n_layers):
            with torch.amp.autocast('cuda', enabled=False):
                x_g = x_g.float()
                temp = torch.matmul(self.V.t(), x_g)
                temp = temp * self.S.unsqueeze(1) 
                x_g = torch.matmul(self.U, temp)
            global_embs.append(x_g)
            
        global_final = torch.mean(torch.stack(global_embs, dim=1), dim=1)
        return local_final, global_final

    def calc_bpr_loss(self, local_emb, users, pos_items, neg_items):
        users_emb = local_emb[users]
        pos_emb = local_emb[pos_items]
        neg_emb = local_emb[neg_items]
        pos_scores = torch.sum(users_emb * pos_emb, dim=1)
        neg_scores = torch.sum(users_emb * neg_emb, dim=1)
        loss = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10))
        return loss

    def calc_ssl_loss(self, local_emb, global_emb, users, items):
        local_emb_norm = F.normalize(local_emb, dim=1)
        global_emb_norm = F.normalize(global_emb, dim=1)
        unique_users = torch.unique(users)
        unique_items = torch.unique(items)
        
        def robust_info_nce(view1, view2, indices):
            v1 = view1[indices]
            v2 = view2[indices]
            logits = torch.matmul(v1, v2.t()) / self.temp
            logits = torch.clamp(logits, max=100.0) 
            labels = torch.arange(logits.shape[0]).to(logits.device)
            return F.cross_entropy(logits, labels)

        user_ssl_loss = robust_info_nce(local_emb_norm, global_emb_norm, unique_users)
        item_ssl_loss = robust_info_nce(local_emb_norm, global_emb_norm, unique_items)
        return user_ssl_loss + item_ssl_loss

    def get_l2_reg(self, users, pos_items, neg_items):
        reg_loss = (1/2)*(self.embedding_user.weight[users].norm(2).pow(2) + 
                          self.embedding_item.weight[pos_items].norm(2).pow(2) +
                          self.embedding_item.weight[neg_items].norm(2).pow(2))
        return reg_loss

# ==========================================
# 5. í•™ìŠµ ë£¨í”„ (Refactored)
# ==========================================
def train(config, dataset, svd_components, edge_index_info):
    """
    ì´ì œ train í•¨ìˆ˜ëŠ” ì™¸ë¶€ì—ì„œ ë°ì´í„°ì™€ SVD ê²°ê³¼ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤. (ì´ì¤‘ ë¡œë”© ë°©ì§€)
    """
    edge_index, num_users, num_items = edge_index_info
    adj_tensor, U, S, V = svd_components
    
    # DataLoader ìƒì„±
    dataloader = data.DataLoader(
        dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        num_workers=0, 
        pin_memory=True
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = LightGCL(num_users, num_items, config, adj_tensor, (U, S, V)).to(config['device'])
    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'])
    
    # AMP Scaler
    use_amp = (config['device'] == 'cuda')
    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)
    
    print(f"\nğŸš€ Start Training on {config['device']} | Batch Size: {config['batch_size']}")
    
    # SVD Stats ì¶œë ¥
    print("="*40)
    print(f"ğŸ“Š [SVD Stats] Top-{config['svd_q']} Singular Values: {S.cpu().numpy()}") 
    print("="*40 + "\n")
    best_loss = float('inf') # ìµœê³  ê¸°ë¡ ì €ì¥ìš©
    for epoch in range(config['epochs']):
        model.train()
        total_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{config['epochs']}")
        
        for batch_i, (users, pos_items, neg_items) in enumerate(pbar):
            users = users.to(config['device'])
            pos_items = pos_items.to(config['device'])
            neg_items = neg_items.to(config['device'])
            
            optimizer.zero_grad()
            
            with torch.amp.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):
                local_emb, global_emb = model()
                
                bpr_loss = model.calc_bpr_loss(local_emb, users, pos_items + num_users, neg_items + num_users)
                ssl_loss = model.calc_ssl_loss(local_emb, global_emb, users, pos_items + num_users)
                reg_loss = model.get_l2_reg(users, pos_items, neg_items)
                
                loss = bpr_loss + config['lambda_ssl'] * ssl_loss + config['lambda_reg'] * reg_loss
                
                loss_val = loss.item()
                bpr_val = bpr_loss.item()
                ssl_val = ssl_loss.item()
            
            # [Sanity Check] ì²« ì—í¬í¬, ì²« ë°°ì¹˜ì¼ ë•Œë§Œ ì¶œë ¥ (tqdm ê¹¨ì§ ë°©ì§€ ìœ„í•´ write ì‚¬ìš©)
            if epoch == 0 and batch_i == 0:
                tqdm.write("\n" + "="*50)
                tqdm.write(f"ğŸ” [Sanity Check] First Batch Loss: {loss_val:.4f}")
                tqdm.write(f"   Local Emb Mean: {local_emb.float().mean().item():.4f}")
                tqdm.write("="*50 + "\n")

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            total_loss += loss_val
            
            # [ìˆ˜ì •] ë§¤ ë°°ì¹˜ë§ˆë‹¤ ìƒíƒœë°” ê°±ì‹  (ë©ˆì¶¤ í™•ì¸ìš©)
            # 1.1ì´ˆë§ˆë‹¤ ê°±ì‹ ë˜ë©´ ì •ìƒì…ë‹ˆë‹¤.
            pbar.set_postfix({
                'Tot': f"{loss_val:.3f}", 
                'BPR': f"{bpr_val:.4f}", 
                'SSL': f"{ssl_val:.3f}"
            })

            # ìƒì„¸ ë¡œê·¸ (500 ë°°ì¹˜ë§ˆë‹¤)
            if batch_i % 100 == 0 and batch_i > 0:
                with torch.no_grad():
                    user_norm = model.embedding_user.weight.data.norm(2, dim=1).mean().item()
                    l_norm = local_emb.norm(p=2, dim=1).mean().item()
                    g_norm = global_emb.norm(p=2, dim=1).mean().item()
                    alignment = F.cosine_similarity(local_emb, global_emb, dim=1).mean().item()

                tqdm.write(
                    f"   [Step {batch_i}] BPR: {bpr_val:.4f} | SSL: {ssl_val:.3f} | "
                    f"Norm(U): {user_norm:.2f} | SVD(Align): {alignment:.3f}"
                )
        
        avg_loss = total_loss / len(dataloader)
        
        # -------------------------------------------------------
        # [ì¶”ê°€] ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¡œì§ (Best & Last)
        # -------------------------------------------------------
        checkpoint = {
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scaler_state_dict': scaler.state_dict(), # AMP ì‚¬ìš© ì‹œ í•„ìˆ˜
            'loss': avg_loss,
            'config': config # ë‚˜ì¤‘ì— ì„¤ì • í™•ì¸ìš©
        }
        
        # 1. ìµœì‹  ëª¨ë¸ ì €ì¥ (ë®ì–´ì“°ê¸°)
        torch.save(checkpoint, os.path.join(config['cache_dir'], "lightgcl_last_checkpoint.pth"))
        
        # 2. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ (Loss ê°±ì‹  ì‹œì—ë§Œ)
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(checkpoint, os.path.join(config['cache_dir'], "lightgcl_best_model.pth"))
            tqdm.write(f"ğŸ’¾ New Best Model Saved! (Loss: {best_loss:.4f})")
            
        print(f"Epoch {epoch+1} Done. Avg Loss: {avg_loss:.4f}")
        
    torch.save(model.state_dict(), "lightgcl_model.pth")
    print("Model Saved!")

def resume_training(new_config, checkpoint_path):
    print(f"\nâ™»ï¸ Resuming training from: {checkpoint_path}")
    
    # 1. ë°ì´í„° ë¡œë“œ (Mainì—ì„œ ì²˜ë¦¬ëœ ê²ƒê³¼ ë™ì¼)
    edge_index, num_users, num_items, _, _ = load_and_process_data(
        new_config['json_file_path'], new_config['cache_dir']
    )
    
    # 2. ê·¸ë˜í”„ êµ¬ì¶• (SVD) - êµ¬ì¡°ëŠ” ë³€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‹¤ì‹œ ê³„ì‚°
    adj_tensor, U, S, V = build_graph(
        edge_index, num_users, num_items, new_config['device'], q=new_config['svd_q']
    )
    
    # 3. ë°ì´í„°ì…‹ & ë¡œë” ì¤€ë¹„
    dataset = TrainDataset(edge_index, num_users, num_items)
    dataloader = data.DataLoader(
        dataset, 
        batch_size=new_config['batch_size'], 
        shuffle=True, 
        num_workers=0, 
        pin_memory=True
    )
    
    # 4. ëª¨ë¸ ì´ˆê¸°í™” & ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    model = LightGCL(num_users, num_items, new_config, adj_tensor, (U, S, V)).to(new_config['device'])
    
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—´ê¸°
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    print("âœ… Model weights loaded.")

    # 5. Optimizer ì„¤ì • (ì¤‘ìš”: ìƒˆë¡œìš´ LR ì ìš©)
    # ë°©ë²• A: ì•„ì˜ˆ ìƒˆë¡œìš´ Optimizerë¥¼ ë§Œë“¦ (ê°€ì¥ ê¹”ë”í•¨, ì¶”ì²œ)
    optimizer = torch.optim.Adam(model.parameters(), lr=new_config['lr'])
    print(f"âœ… Optimizer reset with NEW LR: {new_config['lr']}")

    # (ì„ íƒ) ë°©ë²• B: ì´ì „ Optimizer ìƒíƒœë¥¼ ë³µêµ¬í•˜ë˜ LRë§Œ ë°”ê¿€ ê²½ìš°
    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    # for param_group in optimizer.param_groups:
    #     param_group['lr'] = new_config['lr']

    # 6. AMP Scaler ë³µêµ¬
    use_amp = (new_config['device'] == 'cuda')
    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)
    if 'scaler_state_dict' in checkpoint:
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
    start_epoch = checkpoint['epoch']
    print(f"ğŸš€ Resuming from Epoch {start_epoch + 1}...")

    # 7. í•™ìŠµ ë£¨í”„ (ê¸°ì¡´ train í•¨ìˆ˜ì™€ ë™ì¼)
    # ëª©í‘œ ì—í¬í¬ë§Œí¼ ì¶”ê°€ë¡œ ë” ëŒë¦¬ê±°ë‚˜, ì „ì²´ ì—í¬í¬ë¥¼ ì±„ìš¸ ë•Œê¹Œì§€ ëŒë¦¼
    # ì—¬ê¸°ì„œëŠ” 'ì¶”ê°€ë¡œ new_config['epochs'] ë§Œí¼ ë”' ëŒë¦¬ëŠ” ê²ƒìœ¼ë¡œ ì„¤ì •
    total_epochs = start_epoch + new_config['epochs']
    
    best_loss = checkpoint['loss'] # ì´ì „ ê¸°ë¡ë¶€í„° ì‹œì‘

    for epoch in range(start_epoch, total_epochs):
        model.train()
        total_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{total_epochs}")
        
        for batch_i, (users, pos_items, neg_items) in enumerate(pbar):
            users = users.to(new_config['device'])
            pos_items = pos_items.to(new_config['device'])
            neg_items = neg_items.to(new_config['device'])
            
            optimizer.zero_grad()
            
            with torch.amp.autocast(device_type='cuda', dtype=torch.float16, enabled=use_amp):
                local_emb, global_emb = model()
                
                bpr_loss = model.calc_bpr_loss(local_emb, users, pos_items + num_users, neg_items + num_users)
                ssl_loss = model.calc_ssl_loss(local_emb, global_emb, users, pos_items + num_users)
                reg_loss = model.get_l2_reg(users, pos_items, neg_items)
                
                loss = bpr_loss + new_config['lambda_ssl'] * ssl_loss + new_config['lambda_reg'] * reg_loss
                
                loss_val = loss.item()
                bpr_val = bpr_loss.item()
                ssl_val = ssl_loss.item()

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            total_loss += loss_val
            
            pbar.set_postfix({
                'Tot': f"{loss_val:.3f}", 
                'BPR': f"{bpr_val:.4f}", 
                'SSL': f"{ssl_val:.3f}"
            })

            if batch_i % 100 == 0 and batch_i > 0:
                with torch.no_grad():
                    user_norm = model.embedding_user.weight.data.norm(2, dim=1).mean().item()
                    alignment = F.cosine_similarity(local_emb, global_emb, dim=1).mean().item()
                tqdm.write(f"   [Step {batch_i}] BPR: {bpr_val:.4f} | SSL: {ssl_val:.3f} | Norm(U): {user_norm:.2f} | SVD(Align): {alignment:.3f}")

        avg_loss = total_loss / len(dataloader)
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        new_checkpoint = {
            'epoch': epoch + 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scaler_state_dict': scaler.state_dict(),
            'loss': avg_loss,
            'config': new_config
        }
        
        torch.save(new_checkpoint, os.path.join(new_config['cache_dir'], "lightgcl_last_checkpoint.pth"))
        
        if avg_loss < best_loss:
            best_loss = avg_loss
            torch.save(new_checkpoint, os.path.join(new_config['cache_dir'], "best_model.pth"))
            tqdm.write(f"ğŸ’¾ New Best Model Saved! (Loss: {best_loss:.4f})")
            
        print(f"Epoch {epoch+1} Done. Avg Loss: {avg_loss:.4f}")
        
        
        
from torch.optim.lr_scheduler import CosineAnnealingLR     
        
        
        
def train_fine_tuning(new_config, checkpoint_path):
    print(f"\nğŸ”¥ Starting Fine-tuning with Scheduler & Relaxed Reg...")
    
    # 1. ë°ì´í„° ë° ëª¨ë¸ ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)
    edge_index, num_users, num_items, _, _ = load_and_process_data(
        new_config['json_file_path'], new_config['cache_dir']
    )
    adj_tensor, U, S, V = build_graph(
        edge_index, num_users, num_items, new_config['device'], q=new_config['svd_q']
    )
    dataset = TrainDataset(edge_index, num_users, num_items)
    dataloader = data.DataLoader(dataset, batch_size=new_config['batch_size'], shuffle=True, num_workers=0)
    
    model = LightGCL(num_users, num_items, new_config, adj_tensor, (U, S, V)).to(new_config['device'])
    
    # 2. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (Loss 0.40 ìƒíƒœì˜ ëª¨ë¸)
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    print("âœ… Weights Loaded.")

    # 3. Optimizer & Scheduler ì„¤ì • (í•µì‹¬!)
    # LRì„ ë‹¤ì‹œ 0.002(ì•½ê°„ ë†’ìŒ)ìœ¼ë¡œ ì‹œì‘í•´ì„œ íƒˆì¶œì„ ì‹œë„í•©ë‹ˆë‹¤.
    start_lr = 0.002 
    optimizer = torch.optim.Adam(model.parameters(), lr=start_lr)
    
    # CosineAnnealingLR: LRì„ ì½”ì‚¬ì¸ ê³¡ì„ ì²˜ëŸ¼ ë¶€ë“œëŸ½ê²Œ 0.00001ê¹Œì§€ ë–¨ì–´ëœ¨ë¦¼
    scheduler = CosineAnnealingLR(optimizer, T_max=len(dataloader) * new_config['epochs'], eta_min=1e-5)
    
    use_amp = (new_config['device'] == 'cuda')
    scaler = torch.amp.GradScaler('cuda', enabled=use_amp)
    if 'scaler_state_dict' in checkpoint:
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
    
  
    best_loss = checkpoint['loss']
    print(f"ğŸš€ Fine-tuning for {new_config['epochs']} epochs...")
    print(f"   Strategy: Reg 1e-4 -> {new_config['lambda_reg']} | LR Schedule: {start_lr} -> 1e-5")

    for epoch in range(new_config['epochs']):
        model.train()
        total_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{new_config['epochs']}")
        
        for batch_i, (users, pos_items, neg_items) in enumerate(pbar):
            users = users.to(new_config['device'])
            pos_items = pos_items.to(new_config['device'])
            neg_items = neg_items.to(new_config['device'])
            
            optimizer.zero_grad()
            
            with torch.amp.autocast(device_type='cuda', dtype=torch.float16, enabled=True):
                local_emb, global_emb = model()
                
                bpr_loss = model.calc_bpr_loss(local_emb, users, pos_items + num_users, neg_items + num_users)
                ssl_loss = model.calc_ssl_loss(local_emb, global_emb, users, pos_items + num_users)
                reg_loss = model.get_l2_reg(users, pos_items, neg_items)
                
                loss = bpr_loss + new_config['lambda_ssl'] * ssl_loss + new_config['lambda_reg'] * reg_loss
                loss_val = loss.item()
                bpr_val = bpr_loss.item()
                ssl_val = ssl_loss.item()
                
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            # [ì¶”ê°€] ë§¤ ë°°ì¹˜ë§ˆë‹¤ LRì„ ì•„ì£¼ ì¡°ê¸ˆì”© ê¹ìŠµë‹ˆë‹¤.
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
            
            total_loss += loss_val
            
            # ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§
            pbar.set_postfix({
                'Loss': f"{loss_val:.4f}", 
                'LR': f"{current_lr:.6f}",
                'BPR': f"{bpr_val:.4f}", 
                'SSL': f"{ssl_val:.3f}"
            })
            
            # Step ë‹¨ìœ„ Best ì €ì¥ (ì•ˆì „ì„ ìœ„í•´)
            if batch_i % 100 == 0 and batch_i > 0:
            
                with torch.no_grad():
                    user_norm = model.embedding_user.weight.data.norm(2, dim=1).mean().item()
                    alignment = F.cosine_similarity(local_emb, global_emb, dim=1).mean().item()
                tqdm.write(f"   [Step {batch_i}] BPR: {bpr_val:.4f} | SSL: {ssl_val:.3f} | Norm(U): {user_norm:.2f} | SVD(Align): {alignment:.3f}")
            
            if batch_i % 100 == 0 and loss_val < best_loss:
                best_loss = loss_val
                torch.save(model.state_dict(), os.path.join(new_config['cache_dir'], "lightgcl_best_finetuned.pth"))

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} Done. Avg Loss: {avg_loss:.4f}")
# ==========================================
# 6. Main Execution
# ==========================================
if __name__ == '__main__':
    CONFIG = {
        'emb_dim': 64,
        'n_layers': 2,
        'temp': 0.2,
        'lambda_ssl': 0.01,
        'lambda_reg': 1e-5,
        'svd_q': 5,
        'lr': 0.005,
        'batch_size': 8192, # 8192ë¡œ ëŠ˜ë ¤ë„ ë©ë‹ˆë‹¤
        'epochs': 20,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'json_file_path': r'D:\trainDataset\localprops\cache', # ì‹¤ì œ ê²½ë¡œ
        'cache_dir': r'D:\trainDataset\localprops\cache'
    }
    NEW_CONFIG = {
        'emb_dim': 64,
        'n_layers': 2,
        'temp': 0.2,
        'lambda_ssl': 0.01,
        'lambda_reg': 1e-4,
        'svd_q': 5,
        
        # ğŸ”¥ [í•µì‹¬] ì¤„ì–´ë“  LR ì ìš©
        'lr': 0.001, 
        
        'batch_size': 8192,
        'epochs': 5, # ì¶”ê°€ë¡œ 10 ì—í¬í¬ ë” í•™ìŠµ
        'device': 'cuda',
        'json_file_path': r'D:\trainDataset\localprops\cache',
        'cache_dir': r'D:\trainDataset\localprops\cache'
    }
    FINE_TUNE_CONFIG = {
        'emb_dim': 64,
        'n_layers': 2,
        'temp': 0.2,
        
        # ğŸ”¥ [í•µì‹¬ 1] ë°©í•´ê¾¼ ì œê±° (SSL ê±°ì˜ ë”)
        'lambda_ssl': 0.001, 
        
        # ğŸ”¥ [í•µì‹¬ 2] ì¡±ì‡„ í’€ê¸° (Regë¥¼ 1/10ë¡œ ì¤„ì„)
        'lambda_reg': 1e-5, 
        
        'svd_q': 5,
        'lr': 0.002, # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ê°’ (ë¬´ì‹œë¨, ì½”ë“œ ë‚´ë¶€ start_lr ë”°ë¦„)
        'batch_size': 8192,
        'epochs': 5, # 5 ì—í¬í¬ë©´ ì¶©ë¶„íˆ ìˆ˜ë ´í•©ë‹ˆë‹¤.
        'device': 'cuda',
        'json_file_path': r'D:\trainDataset\localprops\cache',
        'cache_dir': r'D:\trainDataset\localprops\cache'
    }

    checkpoint_path = os.path.join(FINE_TUNE_CONFIG['cache_dir'], "lightgcl_last_checkpoint.pth")   
    train_fine_tuning(FINE_TUNE_CONFIG, checkpoint_path)
    '''
    # 1. ë©”ì¸ì—ì„œ ë°ì´í„° ë¡œë“œ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    edge_index, num_users, num_items, _, _ = load_and_process_data(
        CONFIG['json_file_path'], CONFIG['cache_dir']
    )

    # 2. ë©”ì¸ì—ì„œ ë°ì´í„°ì…‹ ìƒì„±
    print("\n--- Initializing Dataset ---")
    dataset = TrainDataset(edge_index, num_users, num_items)
    
    # 3. ë©”ì¸ì—ì„œ ê·¸ë˜í”„ êµ¬ì¶• (SVD)
    adj_tensor, U, S, V = build_graph(
        edge_index, num_users, num_items, CONFIG['device'], q=CONFIG['svd_q']
    )

    # 4. ì¤€ë¹„ëœ ê°ì²´ë“¤ì„ train í•¨ìˆ˜ë¡œ ì „ë‹¬
    train(
        CONFIG, 
        dataset, 
        (adj_tensor, U, S, V), 
        (edge_index, num_users, num_items)
    )
    
    '''
'''
Epoch 1/20:   4%|â–‹                   | 51/1375 [00:34<14:39,  1.50it/s, Tot=0.779, BPR=0.6930, SSL=8.345] 






SVD(Align) 0.4 ~ 0.8 ì‚¬ì´ ìœ ì§€.
Norm(U) 1.0 ~ 5.0 ìˆ˜ì¤€ìœ¼ë¡œ ì»¤ì§‘ë‹ˆë‹¤

'''