import torch
import torch.nn as nn
import torch.nn.functional as F
import os
import gc
import json
from tqdm import tqdm
from torch.utils.data import TensorDataset, DataLoader
from torch.optim.lr_scheduler import OneCycleLR

# ==========================================
# 0. Data Processing Utilities
# ==========================================

def calculate_logq_from_edge_index(edge_index, num_items, cache_dir, device):
    """
    GNN í•™ìŠµ ë°ì´í„°(Edge Index)ì—ì„œ ì•„ì´í…œ ë¹ˆë„ë¥¼ ê³„ì‚°í•˜ì—¬ LogQ í…ì„œ ìƒì„±
    """
    cache_path = os.path.join(cache_dir, "item_logq_pop.pt")
    
    if os.path.exists(cache_path):
        print(f"[Cache Hit] Loading LogQ from {cache_path}")
        return torch.load(cache_path, map_location=device)
    
    print("âš¡ Calculating Item Popularity (LogQ) for GNN...")
    
    # 1. ì•„ì´í…œ ë¹ˆë„ ì¹´ìš´íŠ¸ (edge_index[1]ì´ ì•„ì´í…œ ì¸ë±ìŠ¤ë¼ê³  ê°€ì •)
    # GNNì—ì„œ ë³´í†µ item indexëŠ” 0ë¶€í„° ì‹œì‘í•˜ì§€ë§Œ, 
    # GraphDatasetì—ì„œ user ë‹¤ìŒì— itemì´ ì˜¤ë„ë¡ reindex ë˜ì—ˆë‹¤ë©´ ì£¼ì˜ í•„ìš”.
    # ì—¬ê¸°ì„œëŠ” load_and_process_dataê°€ ë°˜í™˜í•œ raw item id (0 ~ num_items-1) ê¸°ì¤€ì…ë‹ˆë‹¤.
    
    items = edge_index[1]
    # bincountëŠ” 1D í…ì„œì˜ ê° ê°’ì˜ ë¹ˆë„ë¥¼ ì…ˆ
    # minlengthë¥¼ num_itemsë¡œ ì„¤ì •í•˜ì—¬ ì•ˆ ë‚˜ì˜¨ ì•„ì´í…œë„ 0ìœ¼ë¡œ ì¡í˜
    item_counts = torch.bincount(items, minlength=num_items).float()
    
    # 2. í™•ë¥  ë³€í™˜ (Smoothing)
    total_count = item_counts.sum()
    probs = (item_counts + 1e-6) / total_count # Divide by zero ë°©ì§€
    
    # 3. Log ê³„ì‚°
    log_q = torch.log(probs)
    
    # 4. ì €ì¥ ë° ë°˜í™˜
    torch.save(log_q, cache_path)
    print(f"âœ… LogQ Calculated & Saved. Shape: {log_q.shape}")
    
    return log_q.to(device)
def load_and_process_data(json_file_path, cache_dir="cache"):
    """
    JSON ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  PyTorch Geometric í˜¸í™˜ Edge Indexë¡œ ë³€í™˜
    """
    if not os.path.exists(cache_dir):
        os.makedirs(cache_dir)
        
    cache_path = os.path.join(cache_dir, "processed_graph_train.pt")
    map_path = os.path.join(cache_dir, "id_maps_train.pt")

    if os.path.exists(cache_path) and os.path.exists(map_path):
        print(f"[Cache Hit] Loading graph data from {cache_path}...")
        data_dict = torch.load(cache_path)
        maps = torch.load(map_path)
        return (data_dict['edge_index'], data_dict['num_users'], data_dict['num_items'], 
                maps['user2id'], maps['item2id'])

    print(f"[Cache Miss] Processing Train Sequences from {json_file_path}...")
    
    with open(json_file_path, 'r', encoding='utf-8') as f:
        train_seq_data = json.load(f)
    
    print("Mapping IDs...")
    user_list = list(train_seq_data.keys())
    user2id = {u: i for i, u in enumerate(user_list)}
    
    all_items = set()
    for items in train_seq_data.values():
        all_items.update(items)
    
    item_list = list(all_items)
    item2id = {item: i for i, item in enumerate(item_list)}
    
    num_users = len(user2id)
    num_items = len(item2id)
    
    print(f" -> Num Users: {num_users}")
    print(f" -> Num Items: {num_items}")

    print("Building Edge Index...")
    src_nodes = []
    dst_nodes = []
    
    for u_str, i_list in tqdm(train_seq_data.items(), desc="Flattening Edges"):
        u_idx = user2id[u_str]
        for i_str in i_list:
            if i_str in item2id:
                i_idx = item2id[i_str]
                src_nodes.append(u_idx)
                dst_nodes.append(i_idx)
    
    src = torch.tensor(src_nodes, dtype=torch.long)
    dst = torch.tensor(dst_nodes, dtype=torch.long)
    
    edge_tensor = torch.stack([src, dst], dim=1)
    edge_tensor = torch.unique(edge_tensor, dim=0)
    edge_index = edge_tensor.t()
    
    print(f" -> Total Interactions (Edges): {edge_index.size(1)}")
    
    del train_seq_data, src_nodes, dst_nodes, src, dst, edge_tensor
    gc.collect()

    print("Saving to cache...")
    torch.save({'edge_index': edge_index, 'num_users': num_users, 'num_items': num_items}, cache_path)
    torch.save({'user2id': user2id, 'item2id': item2id}, map_path)

    return edge_index, num_users, num_items, user2id, item2id


class GraphDataset:
    def __init__(self, num_users, num_items, edge_index, device):
        self.num_users = num_users
        self.num_items = num_items
        self.device = device
        self.Graph = self._get_sparse_graph(edge_index.to(device))

    def _get_sparse_graph(self, edge_index_gpu): 
        print("Generating Sparse Graph Adjacency Matrix...")
        n_nodes = self.num_users + self.num_items
        
        users = edge_index_gpu[0]
        items = edge_index_gpu[1]
        items_offset = items + self.num_users
        
        row = torch.cat([users, items_offset])
        col = torch.cat([items_offset, users])
        
        indices = torch.stack([row, col], dim=0)
        values = torch.ones(indices.size(1), device=self.device)
        
        deg = torch.zeros(n_nodes, device=self.device)
        deg = deg.scatter_add(0, row, values)
        
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        
        row_idx = indices[0]
        col_idx = indices[1]
        
        norm_values = values * deg_inv_sqrt[row_idx] * deg_inv_sqrt[col_idx]
        
        norm_adj = torch.sparse_coo_tensor(indices, norm_values, size=(n_nodes, n_nodes))
        return norm_adj


# ==========================================
# 1. SimGCL Model
# ==========================================
class SimGCL(nn.Module):
    def __init__(self, dataset, embed_dim=128, n_layers=2, eps=0.1): 
        super(SimGCL, self).__init__()
        self.num_users = dataset.num_users
        self.num_items = dataset.num_items
        self.Graph = dataset.Graph
        self.n_layers = n_layers
        self.embed_dim = embed_dim
        self.eps = eps

        self.embedding_user = nn.Embedding(self.num_users, self.embed_dim)
        self.embedding_item = nn.Embedding(self.num_items, self.embed_dim)
        # Xavier Initialization
        nn.init.xavier_uniform_(self.embedding_user.weight)
        nn.init.xavier_uniform_(self.embedding_item.weight)

    def perturb_embedding(self, embeds):
        noise = torch.rand_like(embeds)
        noise = F.normalize(noise, dim=1)
        return embeds + self.eps * noise

    def forward(self, perturbed=False):
        ego_embeddings = torch.cat([self.embedding_user.weight, self.embedding_item.weight], dim=0)
        
        if perturbed:
            ego_embeddings = self.perturb_embedding(ego_embeddings)

        all_embeddings = [ego_embeddings]
        
        for k in range(self.n_layers):
            # Sparse MM requires FP32 usually, disable autocast for this op if needed
            with torch.amp.autocast(device_type='cuda', enabled=False):
                ego_embeddings = torch.sparse.mm(self.Graph, ego_embeddings.float())
            all_embeddings.append(ego_embeddings)
            
        final_embeddings = torch.stack(all_embeddings, dim=1).mean(dim=1)
        users_emb, items_emb = torch.split(final_embeddings, [self.num_users, self.num_items])
        
        return users_emb, items_emb


# ==========================================
# 2. SimGCLLoss (Loss Logic Encapsulated)
# ==========================================
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimGCLLoss(nn.Module):
    def __init__(self, lambda_val=0.2, init_temp=0.2, lambda_logq=0.0, log_q=None):
        super(SimGCLLoss, self).__init__()
        self.lambda_val = lambda_val
        self.lambda_logq = lambda_logq
        
        # Learnable Temperature
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / init_temp))
        
        # LogQ Tensor ë“±ë¡ (Bufferë¡œ ë“±ë¡í•˜ì—¬ ì—…ë°ì´íŠ¸ ë˜ì§€ ì•Šê²Œ í•¨)
        if log_q is not None:
            self.register_buffer('log_q', log_q)
        else:
            self.log_q = None

    def get_current_temp(self):
        return (1.0 / self.logit_scale.exp()).clamp(0.01, 1.0)

    def _bpr_loss_with_logq(self, users_emb, pos_items_emb, neg_items_emb, pos_idx, neg_idx):
        # 1. ë‚´ì  ì ìˆ˜ ê³„ì‚°
        pos_scores = torch.sum(users_emb * pos_items_emb, dim=1)
        neg_scores = torch.sum(users_emb * neg_items_emb, dim=1)
        
        # 2. LogQ Correction ì ìš© (Popularity Bias ì œê±°)
        if self.lambda_logq > 0.0 and self.log_q is not None:
            # í•´ë‹¹ ì•„ì´í…œë“¤ì˜ Log í™•ë¥  ê°€ì ¸ì˜¤ê¸°
            pos_pop = self.log_q[pos_idx]
            neg_pop = self.log_q[neg_idx]
            
            # ì ìˆ˜ ë³´ì •: Score_new = Score_old - lambda * log(P(i))
            # ì¸ê¸° ì•„ì´í…œì¼ìˆ˜ë¡ ì ìˆ˜ë¥¼ ê¹ìŒ -> Hard Negative íš¨ê³¼
            pos_scores = pos_scores - (self.lambda_logq * pos_pop)
            neg_scores = neg_scores - (self.lambda_logq * neg_pop)

        # 3. BPR Loss: -log(sigmoid(pos - neg)) = softplus(-(pos - neg))
        loss = F.softplus(-(pos_scores - neg_scores))
        return torch.mean(loss)

    def _info_nce_loss(self, view1, view2):
        curr_temp = self.get_current_temp()
        view1 = F.normalize(view1, dim=1)
        view2 = F.normalize(view2, dim=1)
        
        pos_score = torch.sum(view1 * view2, dim=1)
        pos_score = torch.exp(pos_score / curr_temp)
        
        ttl_score = torch.matmul(view1, view2.transpose(0, 1))
        ttl_score = torch.exp(ttl_score / curr_temp).sum(dim=1)
        
        return -torch.log(pos_score / ttl_score).mean()

    def forward(self, base_out, perturbed_out1=None, perturbed_out2=None, batch_data=None):
        users, pos_items, neg_items = batch_data
        u_emb, i_emb = base_out
        
        # 1. BPR Loss (LogQ ì ìš©)
        loss_bpr = self._bpr_loss_with_logq(
            u_emb[users], i_emb[pos_items], i_emb[neg_items], 
            pos_items, neg_items # ì¸ë±ìŠ¤ ì „ë‹¬ í•„ìš”
        )
        
        # 2. CL Loss
        loss_cl = 0.0
        if perturbed_out1 is not None:
            u_v1, i_v1 = perturbed_out1
            u_v2, i_v2 = perturbed_out2
            loss_cl = self._info_nce_loss(u_v1[users], u_v2[users]) + \
                      self._info_nce_loss(i_v1[pos_items], i_v2[pos_items])
            
        return loss_bpr + self.lambda_val * loss_cl
# ==========================================
# 3. GNN Trainer
# ==========================================
class GNNTrainer:
    def __init__(self, config, model, train_loader, log_q_tensor, device):
        self.config = config
        self.model = model
        self.train_loader = train_loader
        self.device = device
        
        # Loss ì´ˆê¸°í™” (LogQ ì „ë‹¬)
        self.criterion = SimGCLLoss(
            lambda_val=config['lambda'], 
            init_temp=0.2,
            lambda_logq=config.get('lambda_logq', 0.0), # Configì—ì„œ ë°›ê¸°
            log_q=log_q_tensor
        ).to(device)
        
        # Optimizer (Loss íŒŒë¼ë¯¸í„° í¬í•¨)
        all_params = list(model.parameters()) + list(self.criterion.parameters())
        self.optimizer = torch.optim.Adam(all_params, lr=config['lr'], weight_decay=config['weight_decay'])
        # ... (Scheduler ë“± ê¸°ì¡´ ë™ì¼) ...
        self.scaler = torch.amp.GradScaler('cuda')

    def _calc_batch_metrics(self, u_emb, i_emb, users, pos_items):
        """
        í˜„ì¬ ë°°ì¹˜ë¥¼ ì´ìš©í•˜ì—¬ Alignmentì™€ Uniformityë¥¼ ê·¼ì‚¬ ê³„ì‚° (Fast)
        """
        with torch.no_grad():
            # Normalize
            u_norm = F.normalize(u_emb[users], dim=1)
            i_norm = F.normalize(i_emb[pos_items], dim=1)
            
            # 1. Alignment: (u - i)^2
            align = (u_norm - i_norm).norm(p=2, dim=1).pow(2).mean().item()
            
            # 2. Uniformity: exp(-2 * dist^2)
            # ë°°ì¹˜ ë‚´ ì•„ì´í…œë“¤ë¼ë¦¬ì˜ ë¶„í¬ë§Œ í™•ì¸ (ì „ì²´ ê·¼ì‚¬)
            # pdist ê³„ì‚° ë¹„ìš©ì´ í¬ë¯€ë¡œ 2048ê°œê¹Œì§€ë§Œ ìƒ˜í”Œë§
            if len(i_norm) > 2048:
                idx = torch.randperm(len(i_norm))[:2048]
                i_sample = i_norm[idx]
            else:
                i_sample = i_norm
                
            dist = torch.cdist(i_sample, i_sample, p=2).pow(2)
            unif = torch.log(torch.exp(-2 * dist).mean()).item()
            
        return align, unif

    def train_epoch(self, epoch_idx):
        self.model.train()
        total_loss = 0
        
        # -------------------------------------------------------
        # â­ [ìµœì í™” 1] Gradient Accumulation ì„¤ì •
        # ë¬¼ë¦¬ ë°°ì¹˜ëŠ” 1024ì§€ë§Œ, ë…¼ë¦¬ ë°°ì¹˜ëŠ” 4096ìœ¼ë¡œ í•™ìŠµ íš¨ê³¼ë¥¼ ëƒ„
        # -------------------------------------------------------
        accumulation_steps = 4  # 1024 * 4 = 4096 (ê¶Œì¥)
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch_idx}")
        
        # Optimizer ì´ˆê¸°í™” (ë£¨í”„ ì‹œì‘ ì „)
        self.optimizer.zero_grad(set_to_none=True) # set_to_none=Trueê°€ ë” ë¹ ë¦„
        
        for batch_idx, (batch_users, batch_pos_items) in enumerate(pbar):
            batch_users = batch_users.to(self.device)
            batch_pos_items = batch_pos_items.to(self.device)
            
            # Negatives Sampling
            batch_neg_items = torch.randint(0, self.config['num_items'], (len(batch_users),), device=self.device)
            
            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
                # 1. Forward
                base_out = self.model(perturbed=False)
                
                # 2. CL Views (ë§¤ ìŠ¤í… ìˆ˜í–‰í•˜ë˜, ë©”ëª¨ë¦¬ ì•„ë¼ë ¤ë©´ ì—¬ê¸°ì„œë„ ì¡°ì ˆ ê°€ëŠ¥)
                pert_out1, pert_out2 = None, None
                if batch_idx % self.config['cl_interval'] == 0:
                    pert_out1 = self.model(perturbed=True)
                    pert_out2 = self.model(perturbed=True)
                
                # 3. Loss Calculation
                loss = self.criterion(
                    base_out, pert_out1, pert_out2, 
                    (batch_users, batch_pos_items, batch_neg_items)
                )
                
                # â­ Loss ì •ê·œí™” (Accumulationì„ ìœ„í•´ ë‚˜ëˆ ì¤Œ)
                loss = loss / accumulation_steps

            # 4. Backward (Gradient ëˆ„ì ë¨)
            self.scaler.scale(loss).backward()
            
            # 5. Step (accumulation_steps ë§ˆë‹¤ ì‹¤í–‰)
            if (batch_idx + 1) % accumulation_steps == 0:
                # Gradient Clipping (í•™ìŠµ ì•ˆì •ì„± í™•ë³´)
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0) # 5.0 -> 1.0ìœ¼ë¡œ ë” ë¹¡ë¹¡í•˜ê²Œ
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
                self.optimizer.zero_grad(set_to_none=True) # ë©”ëª¨ë¦¬ ìµœì í™”
                
                # SchedulerëŠ” Step ë‹¨ìœ„ ì—…ë°ì´íŠ¸ì¸ ê²½ìš° ì—¬ê¸°ì„œ í˜¸ì¶œ
                # self.scheduler.step() 

            total_loss += loss.item() * accumulation_steps # ë¡œê¹…ìš© ë³µì›
            
            if batch_idx % 100 == 0:
                # (ì§€í‘œ ëª¨ë‹ˆí„°ë§ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€)
                pass
        
        return total_loss / len(self.train_loader)
    def save_checkpoint(self, epoch_idx, avg_loss):
        checkpoint = {
            'epoch': epoch_idx,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': avg_loss,
            'config': self.config
        }
        save_path = os.path.join(self.checkpoint_dir, f"gnn_epoch_{epoch_idx}.pth")
        torch.save(checkpoint, save_path)
        print(f"âœ… Checkpoint saved: {save_path}")

    def run(self):
        print(f"\n[Training Start] Users: {self.config['num_users']}, Items: {self.config['num_items']}")
        
        for epoch in range(1, self.config['epochs'] + 1):
            avg_loss = self.train_epoch(epoch)
            print(f"Epoch {epoch} Done. Avg Loss: {avg_loss:.4f}")
            self.save_checkpoint(epoch, avg_loss)
            
        torch.save(self.model.state_dict(), self.final_save_path)
        print(f"ğŸ‰ Final Model saved to {self.final_save_path}")


# ==========================================
# 4. Main Execution Function
# ==========================================
def train_gnn_cl_user_noise():
    # -----------------------------------------------------------
    # 1. Configuration
    # -----------------------------------------------------------
    BASE_DIR = r"D:\trainDataset\localprops"
    
    config = {
        'json_file_path': os.path.join(BASE_DIR, "final_train_seq.json"),
        'cache_dir': os.path.join(BASE_DIR, "cache"),
        'checkpoint_dir': "./checkpoints",
        'batch_size': 1024,      # ë¬¼ë¦¬ì  í•œê³„ (ìœ ì§€)
        'accumulation_steps': 4, # â­ ì¶”ê°€ (ë…¼ë¦¬ì  ë°°ì¹˜ 4096 íš¨ê³¼)
        
        'epochs': 15,            # ì°¨ì›ì´ ì»¤ì ¸ì„œ ìˆ˜ë ´ì´ ëŠë¦´ ìˆ˜ ìˆìŒ
        'lr': 0.001,             # â­ 0.005 -> 0.001 (ì•ˆì •ì„± í™•ë³´)
        'weight_decay': 1e-5,    # â­ 1e-4 -> 1e-5 (ì œì•½ ì™„í™”)
        
        'embed_dim': 128,
        'n_layers': 2,           # 3ì¸µìœ¼ë¡œ ëŠ˜ë¦¬ë©´ ì˜¤ë²„ìŠ¤ë¬´ë”© ì˜¬ ìˆ˜ ìˆìœ¼ë‹ˆ 2ì¸µ ìœ ì§€
        'eps': 0.2,              # â­ ë…¸ì´ì¦ˆ 0.1 -> 0.2 (ë” ê°•í•œ ë³€í˜•ìœ¼ë¡œ ê°•ê±´ì„± í™•ë³´)
        
        'cl_interval': 1,        # ìœ ì§€
        'lambda': 0.5,           # â­ 0.2 -> 0.5 (CL ê°•í™”) ë°°ì¹˜ ì‘ì•„ì ¸ì„œ ë°€ì–´ë‚´ëŠ”ê±° ì¼ë‹¨ ì¢€ ì´ˆê¸°ì— ë¹¡
        
        # ì•„ê¹Œ êµ¬í˜„í•œ LogQ / Learnable Temp ì ìš© í•„ìˆ˜
        'lambda_logq': 0.2,     
        'init_temp': 0.1,        # â­ 0.2 -> 0.1 (ì¢€ ë” Sharpí•˜ê²Œ ì‹œì‘)
    }
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device selected: {device}")

    # -----------------------------------------------------------
    # 2. Data Loading
    # -----------------------------------------------------------
    edge_index, n_users, n_items, u_map, i_map = load_and_process_data(
        config['json_file_path'], 
        config['cache_dir']
    )
    
    config['num_users'] = n_users
    config['num_items'] = n_items
    
    gc.collect()
    torch.cuda.empty_cache()

    # -----------------------------------------------------------
    # 3. Dataset Setup
    # -----------------------------------------------------------
    graph_dataset = GraphDataset(n_users, n_items, edge_index, device)
    train_ds = TensorDataset(edge_index[0], edge_index[1])
    
    train_loader = DataLoader(
        train_ds, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        drop_last=True, 
        pin_memory=True
    )

    # -----------------------------------------------------------
    # 4. Model & Trainer Setup
    # -----------------------------------------------------------
    model = SimGCL(
        graph_dataset, 
        embed_dim=config['embed_dim'], 
        n_layers=config['n_layers'], 
        eps=config['eps']
    ).to(device)
    
        # --------------------------------------------------------
    # â­ [ì¶”ê°€] LogQ Tensor ê³„ì‚°
    # --------------------------------------------------------
    log_q_tensor = calculate_logq_from_edge_index(
        edge_index, n_items, config['cache_dir'], device
    )

    # ... (Dataset, Model ì´ˆê¸°í™”) ...

    # --------------------------------------------------------
    # â­ [ìˆ˜ì •] Trainerì— log_q ì „ë‹¬
    # --------------------------------------------------------
    trainer = GNNTrainer(config, model, train_loader, log_q_tensor, device)
    
    # -----------------------------------------------------------
    # 5. Run Training
    # -----------------------------------------------------------
    trainer.run()




import torch
import torch.nn.functional as F
import numpy as np

def calculate_alignment_uniformity(model, edge_index, batch_size=2048):
    """
    SimGCL ëª¨ë¸ì˜ ì„ë² ë”© í’ˆì§ˆ(Alignment & Uniformity)ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    """
    model.eval()
    
    # 1. ì„ë² ë”© ì¶”ì¶œ (Normalization í•„ìˆ˜)
    with torch.no_grad():
        u_emb, i_emb = model(perturbed=False)
        u_emb = F.normalize(u_emb, dim=1)
        i_emb = F.normalize(i_emb, dim=1)
    
    # ---------------------------------------------------------
    # 1. Alignment Loss (User - Positive Item ê±°ë¦¬)
    # : ìœ ì €ì™€ ê·¸ê°€ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œì€ ê°€ê¹Œì›Œì•¼ í•œë‹¤.
    # Formula: E[ || f(u) - f(i) ||^2 ]
    # ---------------------------------------------------------
    users = edge_index[0]
    items = edge_index[1]
    
    # ë©”ëª¨ë¦¬ ë¬¸ì œë¡œ ë°°ì¹˜ ë‹¨ìœ„ ê³„ì‚°
    total_align_loss = 0
    num_edges = len(users)
    
    for i in range(0, num_edges, batch_size):
        batch_u = users[i:i+batch_size]
        batch_i = items[i:i+batch_size]
        
        u_vecs = u_emb[batch_u]
        i_vecs = i_emb[batch_i]
        
        # ìœ í´ë¦¬ë“œ ê±°ë¦¬ ì œê³± (x-y)^2
        align_loss = (u_vecs - i_vecs).norm(p=2, dim=1).pow(2).mean()
        total_align_loss += align_loss.item() * len(batch_u)
        
    avg_align = total_align_loss / num_edges
    
    # ---------------------------------------------------------
    # 2. Uniformity Loss (All Items Distribution)
    # : ì•„ì´í…œë“¤ì€ ê³µê°„ìƒì— ê³ ë¥´ê²Œ í¼ì ¸ ìˆì–´ì•¼ í•œë‹¤. (ë¶•ê´´ ë°©ì§€)
    # Formula: log E[ exp( -2 * || f(i) - f(j) ||^2 ) ]
    # ì „ì²´ ìŒ(N*N)ì€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ê·¼ì‚¬
    # ---------------------------------------------------------
    num_samples = 5000 # ìƒ˜í”Œë§ ê°œìˆ˜
    perm = torch.randperm(len(i_emb))[:num_samples]
    sampled_items = i_emb[perm]
    
    # pdist: pairwise distance between all sampled items
    # (N, D) -> (N, N) distance matrix
    dist_matrix = torch.cdist(sampled_items, sampled_items, p=2).pow(2)
    
    # exp(-2 * dist) ê³„ì‚° í›„ í‰ê·  -> ë¡œê·¸
    # t=2 (Wang et al. ë…¼ë¬¸ í‘œì¤€)
    unif_loss = torch.log(torch.exp(-2 * dist_matrix).mean()).item()
    
    return avg_align, unif_loss

# ==========================================
# ì‚¬ìš© ì˜ˆì‹œ
# ==========================================
# í•™ìŠµì´ ëë‚œ ëª¨ë¸(model)ê³¼ edge_indexë¥¼ ë„£ì–´ì£¼ì„¸ìš”.
# align, unif = calculate_alignment_uniformity(model, edge_index.to(device))
# print(f"ğŸ“Š Alignment: {align:.4f} (Low is Good, < 0.5 is great)")
# print(f"ğŸ“Š Uniformity: {unif:.4f} (Low is Good, usually -1 ~ -3)")

# ==========================================
# 5. Resume Execution Function (ì¶”ê°€ë¨)
# ==========================================
def resume_gnn_cl_user_noise(checkpoint_filename):
    """
    ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì—ì„œ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ ë¡œë“œí•˜ì—¬ í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.
    Args:
        checkpoint_filename: ì˜ˆ) "gnn_epoch_5.pth"
    """
    # -----------------------------------------------------------
    # 1. Base Setup (ê²½ë¡œ ì„¤ì •)
    # -----------------------------------------------------------
    BASE_DIR = r"D:\trainDataset\localprops"
    checkpoint_dir = "./checkpoints"
    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ Error: Checkpoint file not found at {checkpoint_path}")
        return

    print(f"ğŸ”„ Loading checkpoint configuration from {checkpoint_path}...")
    checkpoint = torch.load(checkpoint_path)
    
    # ì €ì¥ëœ Config ë¶ˆëŸ¬ì˜¤ê¸° (ê²½ë¡œëŠ” í˜„ì¬ í™˜ê²½ì— ë§ê²Œ ì¬ì„¤ì •)
    config = checkpoint['config']
    config['json_file_path'] = os.path.join(BASE_DIR, "final_train_seq.json")
    config['cache_dir'] = os.path.join(BASE_DIR, "cache")
    config['checkpoint_dir'] = checkpoint_dir
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device selected: {device}")

    # -----------------------------------------------------------
    # 2. Data Loading (ëª¨ë¸ ì´ˆê¸°í™”ë¥¼ ìœ„í•´ í•„ìˆ˜)
    # -----------------------------------------------------------
    # ê·¸ë˜í”„ êµ¬ì¡°(Adjacency Matrix)ë¥¼ ë‹¤ì‹œ ë§Œë“¤ì–´ì•¼ ëª¨ë¸ì„ ì˜¬ë¦´ ìˆ˜ ìˆìŒ
    edge_index, n_users, n_items, u_map, i_map = load_and_process_data(
        config['json_file_path'], 
        config['cache_dir']
    )
    
    # Configì— User/Item ìˆ˜ ë™ê¸°í™”
    config['num_users'] = n_users
    config['num_items'] = n_items
    
    gc.collect()
    torch.cuda.empty_cache()

    # -----------------------------------------------------------
    # 3. Dataset & Loader Setup
    # -----------------------------------------------------------
    graph_dataset = GraphDataset(n_users, n_items, edge_index, device)
    train_ds = TensorDataset(edge_index[0], edge_index[1])
    
    train_loader = DataLoader(
        train_ds, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        drop_last=True, 
        pin_memory=True
    )

    # -----------------------------------------------------------
    # 4. Model & Trainer Initialization
    # -----------------------------------------------------------
    model = SimGCL(
        graph_dataset, 
        embed_dim=config['embed_dim'], 
        n_layers=config['n_layers'], 
        eps=config['eps']
    ).to(device)
    
    trainer = GNNTrainer(config, model, train_loader, device)

    # -----------------------------------------------------------
    # 5. Load State Dicts (í•µì‹¬: ìƒíƒœ ë³µì›)
    # -----------------------------------------------------------
    model.load_state_dict(checkpoint['model_state_dict'])
    trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # ì´ì „ í•™ìŠµ ì¢…ë£Œ ì§€ì  í™•ì¸
    start_epoch = checkpoint['epoch'] + 1
    prev_loss = checkpoint['loss']
    
    print(f"âœ… Successfully loaded checkpoint '{checkpoint_filename}'")
    print(f"   -> Resuming from Epoch {start_epoch} (Previous Loss: {prev_loss:.4f})")
    
    # -----------------------------------------------------------
    # 6. Run Remaining Epochs
    # -----------------------------------------------------------
    if start_epoch > config['epochs']:
        print("âš ï¸ Training already finished based on config epochs.")
    else:
        trainer.run(start_epoch=start_epoch)


if __name__ == "__main__":
    train_gnn_cl_user_noise()