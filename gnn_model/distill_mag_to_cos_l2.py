import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

class MagnitudeEncoder(nn.Module):
    def __init__(self, input_dim=64, output_dim=64, hidden_dim=128):
        """
        Args:
            input_dim: LightGCL ì„ë² ë”© ì°¨ì› (ì˜ˆ: 64)
            output_dim: ë³€í™˜í•  ì°¨ì› (FAISS ë“±ì— ë§ì¶¤, ë³´í†µ ê°™ê±°ë‚˜ í¼)
            hidden_dim: ì •ë³´ë¥¼ ì„ê¸° ìœ„í•´ ì ì‹œ ì°¨ì›ì„ ëŠ˜ë¦¼
        """
        super(MagnitudeEncoder, self).__init__()
        
        self.encoder = nn.Sequential(
            # 1. ì°¨ì› í™•ì¥ & ë¹„ì„ í˜•ì„± ì¶”ê°€
            # í¬ê¸°(Length) ì •ë³´ë¥¼ ì¢Œí‘œ(Coordinate) ì •ë³´ë¡œ 'ì ‘ì–´ì„œ' ë„£ê¸° ìœ„í•¨
            nn.Linear(input_dim, hidden_dim),
            nn.LeakyReLU(0.2), # ReLUë³´ë‹¤ ì •ë³´ ì†ì‹¤ì´ ì ì€ Leaky ê¶Œì¥
            nn.Dropout(0.1),
            
            # 2. ë‹¤ì‹œ ì›ë˜ ì°¨ì›(ë˜ëŠ” íƒ€ê²Ÿ ì°¨ì›)ìœ¼ë¡œ ì••ì¶•
            nn.Linear(hidden_dim, output_dim)
        )
        
        # [ì¤‘ìš”] Cosine Similarityì˜ í•œê³„(-1~1)ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ í•™ìŠµ ê°€ëŠ¥í•œ ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„°
        # Dot ProductëŠ” 10, 20ê¹Œì§€ ê°€ëŠ”ë° Cosineì€ 1ì´ ìµœëŒ€ë¼ì„œ,
        # ì´ logit_scaleì„ ê³±í•´ì„œ ë²”ìœ„ë¥¼ ë§ì¶°ì¤Œ.
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

    def forward(self, x):
        x = self.encoder(x)
        # 3. ë¬´ì¡°ê±´ í¬ê¸°ë¥¼ 1ë¡œ ê³ ì • (L2 Normalize)
        # ì´ì œ ëª¨ë“  ì •ë³´ëŠ” 'ë°©í–¥(Angle)'ì—ë§Œ ë‚¨ì•„ìˆìŒ
        return F.normalize(x, p=2, dim=1)
    
    
    
    
def train_projector(lightgcl_model, dataloader, device, epochs=5):
    """
    LightGCL(Teacher) -> Projector(Student) ì§€ì‹ ì¦ë¥˜ í•™ìŠµ
    """
    # 1. ëª¨ë¸ ì„¤ì •
    lightgcl_model.eval() # ì„ ìƒë‹˜ì€ ê³ ì • (í‰ê°€ ëª¨ë“œ)
    
    # Userìš©, Itemìš© Projectorë¥¼ ë”°ë¡œ ë§Œë“¤ê±°ë‚˜ í•˜ë‚˜ë¥¼ ê³µìœ í•´ë„ ë¨ (ì—¬ê¸°ì„  ê³µìœ )
    projector = MagnitudeEncoder(input_dim=64, output_dim=64).to(device)
    optimizer = torch.optim.Adam(projector.parameters(), lr=0.001)
    
    # Teacherì˜ User/Item ì„ë² ë”© ê°€ì ¸ì˜¤ê¸° (ê³ ì •ëœ í…ì„œ)
    # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ detach()
    src_user_emb = lightgcl_model.embedding_user.weight.detach()
    src_item_emb = lightgcl_model.embedding_item.weight.detach()

    print("ğŸš€ Start Projector Distillation...")
    
    for epoch in range(epochs):
        projector.train()
        total_loss = 0
        
        # tqdm ë“± ì‚¬ìš© ê°€ëŠ¥
        for batch_users, batch_pos_items, _ in dataloader:
            batch_users = batch_users.to(device)
            batch_pos_items = batch_pos_items.to(device)
            
            # -----------------------------------------------------------
            # A. Teacher (LightGCL) - ì •ë‹µì§€ ìƒì„±
            # -----------------------------------------------------------
            # ë‚´ì (Dot Product) ì‚¬ìš© -> í¬ê¸°(Magnitude) ì •ë³´ê°€ ì ìˆ˜ì— ë°˜ì˜ë¨
            # ê°’ì´ -inf ~ +inf ë²”ìœ„ (ì˜ˆ: 12.5)
            with torch.no_grad():
                u_tea = src_user_emb[batch_users]
                i_tea = src_item_emb[batch_pos_items]
                # (Batch,)
                scores_teacher = torch.sum(u_tea * i_tea, dim=1)

            # -----------------------------------------------------------
            # B. Student (Projector) - ë”°ë¼í•˜ê¸°
            # -----------------------------------------------------------
            # Projector í†µê³¼ -> í¬ê¸°ê°€ 1ë¡œ ë°”ë€œ (L2 Norm)
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë™ì¼í•´ì§
            u_stu = projector(u_tea) # ì…ë ¥ì€ ì›ë³¸ ë²¡í„°
            i_stu = projector(i_tea)
            
            # (Batch,) ê°’ì€ -1.0 ~ 1.0
            cosine_scores = torch.sum(u_stu * i_stu, dim=1)
            
            # [í•µì‹¬] ìŠ¤ì¼€ì¼ ë³´ì •
            # Cosine(-1~1)ì— í° ê°’ì„ ê³±í•´ì„œ Teacher(-10~10)ì™€ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¦
            scores_student = cosine_scores * projector.logit_scale.exp()
            
            # -----------------------------------------------------------
            # C. Loss ê³„ì‚° (Distillation)
            # -----------------------------------------------------------
            # ë‘ ì ìˆ˜ ë¶„í¬ì˜ ì°¨ì´ë¥¼ ì¤„ì„ (MSEê°€ ê°€ì¥ ì§ê´€ì ì´ê³  ë¹ ë¦„)
            loss = F.mse_loss(scores_student, scores_teacher)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
        print(f"Epoch {epoch+1}/{epochs} | Distillation Loss: {total_loss / len(dataloader):.4f}")

    return projector




'''

# 1. í•™ìŠµ ì™„ë£Œ í›„ ë³€í™˜
projector.eval()
with torch.no_grad():
    # LightGCLì˜ ì›ë³¸ ì„ë² ë”©(í¬ê¸° ì œê°ê°)ì„ ë„£ìŒ
    raw_user_emb = lightgcl_model.embedding_user.weight
    raw_item_emb = lightgcl_model.embedding_item.weight
    
    # Projector í†µê³¼ -> í¬ê¸°ê°€ 1ì´ë©´ì„œ ì¸ê¸°ë„ ì •ë³´ê°€ ê°ë„ì— ë°˜ì˜ëœ ë²¡í„° íƒ„ìƒ
    final_user_emb = projector(raw_user_emb).cpu().numpy()
    final_item_emb = projector(raw_item_emb).cpu().numpy()

# 2. ì´ì œ ì´ final_item_embëŠ” Norm=1 ì´ë¯€ë¡œ
#    ê¸°ì¡´ ì‹œìŠ¤í…œ(FAISS ë“±)ì— ë°”ë¡œ ë„£ì–´ë„ "ì¸ê¸°ë„"ê°€ ë°˜ì˜ëœ ì¶”ì²œì´ ë‚˜ì˜µë‹ˆë‹¤.
print(f"New Norm: {np.linalg.norm(final_user_emb, axis=1).mean():.4f}") # 1.0000 ì¶œë ¥
'''