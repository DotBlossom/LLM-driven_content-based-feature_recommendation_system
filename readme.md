# ğŸ›ï¸ LLM-Driven content-based Feature Recommendation System

## ğŸ“– Overview
2024-10 ~ 2024-12 project(Partial Impl) -> complete AI Impl ver(Logic ì „ì²´ ë°˜ì˜)
[prev(24.10) github link](https://github.com/DotBlossom/flask-AI-inference-bedrock-controller)

ì´ í”„ë¡œì íŠ¸ëŠ” **LLM(Large Language Model)ì„ í™œìš©í•œ ê³ í’ˆì§ˆ Feature Engineering**ê³¼ **Contrastive Learning(SimCSE)** ê¸°ë°˜ì˜ ì„ë² ë”© í•™ìŠµì„ ê²°í•©í•œ ì»¤ë¨¸ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ê¸°ì¡´ì˜ ë‹¨ìˆœ í˜‘ì—… í•„í„°ë§(CF)ì´ë‚˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì˜ í•œê³„ë¥¼ ë„˜ì–´, ìƒí’ˆì˜ **êµ¬ì¡°ì  ì†ì„±(Standard)** ê³¼ **ë¹„ì •í˜• ìƒì„¸ ì†ì„±(Reinforced)** ì„ **Cross-Attention** ìœ¼ë¡œ ìœµí•©í•˜ì—¬ ì •êµí•œ ë²¡í„° ê³µê°„ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **Two-Tower êµ¬ì¡°ì˜ Retrieval(í›„ë³´ ì¶”ì¶œ)** ê³¼ **DeepFM ê¸°ë°˜ì˜ Reranking(ì •ë°€ ì •ë ¬)** íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ìœ ì €ì˜ ì‹ ì²´ ì •ë³´ì™€ ë§¥ë½ê¹Œì§€ ê³ ë ¤í•œ ì´ˆê°œì¸í™” ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.


## Data Strategy & Processing Pipeline Overview

ë³¸ í”„ë¡œì íŠ¸ì˜ ë°ì´í„° ì•„í‚¤í…ì²˜ëŠ” **"LLMì„ í™œìš©í•œ í‘œí˜„ë ¥ ì¦ê°•(Representation Enrichment)"** ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

ì—¬ê¸°ì— **AI Hub íŒ¨ì…˜ ìƒí’ˆ ë°ì´í„°ì…‹**ê³¼ í˜¸í™˜ ê°€ëŠ¥í•œ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¥¼ ì ìš©í•˜ì—¬, í•™ìŠµ ë°ì´í„°ì™€ ì‹¤ì œ ì„œë¹„ìŠ¤ ë°ì´í„° ê°„ì˜ ì •í•©ì„±ì„ ë³´ì¥í•˜ê³  **Reranker(DeepFM, DLRM)** ë“± ê³ ë„í™”ëœ ëª¨ë“ˆë¡œ ì¦‰ì‹œ í™•ì¥ ê°€ëŠ¥í•œ **ë²”ìš©ì ì´ê³  ìœ ê¸°ì ì¸ ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ—ï¸ System Architecture

ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ ë°ì´í„° ì „ì²˜ë¦¬, í›„ë³´ ì¶”ì¶œ(Retrieval), ì •ë°€ ì •ë ¬(Reranking)ì˜ 3ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph Data Processing
    A[Raw Product Data] -->|LLM Inference| B(Structured JSON)
    B -->|Vocab Mapping| C{Dual Vocab System}
    end

    subgraph Stage 1: Representation
    C -->|STD + RE + Title| D[Item Tower <br> SimCSE Pre-training]
    D -->|Inference & Normalize| E[Vector DB <br> pgvector]
    end

    subgraph Stage 2: Retrieval
    F[User Logs / Profile] -->|Lookup & Fusion| G[User Tower]
    G -.->|ANN Search| E
    E -->|Top-100 Candidates| H[Candidate List]
    end

    subgraph Stage 3: Ranking
    H -->|Features + Interaction| I[Reranker <br> DeepFM]
    I -->|Top-k Reordering| J[Final Recommendation]
    end

```

## ğŸ”‘ Key Logics & Features

### 1. Hybrid Vocabulary System (Data Strategy)
LLMì´ ì¶”ì¶œí•œ í”¼ì²˜ë¥¼ ì„±ê²©ì— ë”°ë¼ ë‘ ê°€ì§€ Vocabìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ **ì—°ì‚° íš¨ìœ¨ì„±**ê³¼ **í‘œí˜„ë ¥**ì„ ë™ì‹œì— í™•ë³´í–ˆìŠµë‹ˆë‹¤.
* **STD (Standard) Vocab:** ì¹´í…Œê³ ë¦¬, ë¸Œëœë“œ, ì„±ë³„ ë“± ê³ ì •ëœ ë„ë©”ì¸ í”¼ì²˜. ë‹¨ì¼ í†µí•© ì„ë² ë”© í…Œì´ë¸”ì„ ê³µìœ í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
* **RE (Reinforced) Vocab:** ì†Œì¬, í•, ìŠ¤íƒ€ì¼, ìƒí’ˆëª…(Title) ë“± ê°€ë³€ì ì¸ ìƒì„¸ ì†ì„±. ë™ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•œ Vocab êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
* **ID Mapping:** ëª¨ë“  í…ìŠ¤íŠ¸ í”¼ì²˜ë¥¼ ê³ ìœ  IDë¡œ ë§¤í•‘í•˜ì—¬ Cross-Attention ì‹œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  **ì§êµì„±(Orthogonality)**ì„ í™•ë³´í•©ë‹ˆë‹¤.

### 2. Stage 1: Item Tower (Coarse-to-Fine Representation)
ìƒí’ˆì˜ ë³¸ì§ˆì ì¸ ì˜ë¯¸(Semantic)ë¥¼ ë²¡í„°í™”í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.
* **Architecture:** Transformer-based Encoder + MLP Projection Head
* **Cross-Attention Mechanism:**
    * **Query (Anchor):** STD í”¼ì²˜ (ì˜ˆ: "ë‚¨ì„± ìƒì˜") â†’ ë³€í•˜ì§€ ì•ŠëŠ” ê¸°ì¤€ì 
    * **Key/Value (Context):** RE í”¼ì²˜ + Title Tokens (ì˜ˆ: "ë¦°ë„¨", "ì˜¤ë²„í•", "ì—¬ë¦„ ì‹ ìƒ") â†’ ë³´ê°• ì •ë³´
    * *Effect:* í‘œì¤€ ì†ì„±ì´ ìƒì„¸ ì†ì„±ì„ ì°¸ì¡°í•˜ì—¬ ë²¡í„°ë¥¼ ê°•í™”(Reinforce)í•˜ëŠ” êµ¬ì¡°.
* **Training Objective (SimCSE):**
    * **Augmentation:** Feature Dropout & Token Maskingì„ í†µí•´ Positive Pair ìƒì„±.
    * **Loss:** `NTXentLoss` (InfoNCE) with In-batch Negatives.
    * **Normalization:** ì¶”ë¡  ì‹œ L2 Normalizationì„ ì ìš©í•˜ì—¬ Cosine Similarity ê²€ìƒ‰ì— ìµœì í™”.

### 3. Stage 2: User Tower (Multi-modal Retrieval)
ìœ ì €ì˜ í–‰ë™, í˜„ì¬ ì˜ë„(Context), ì‹ ì²´ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ì„ í˜¸ ì•„ì´í…œì„ íƒìƒ‰í•©ë‹ˆë‹¤.
* **3-Way Multi-modal Fusion:**
    1.  **Behavior:** ê³¼ê±° êµ¬ë§¤/í´ë¦­ ì´ë ¥ Sequence (Pre-trained Item Vector Lookup + Transformer).
    2.  **Context:** ì¥ë°”êµ¬ë‹ˆ ì»¨ì…‰ í…ìŠ¤íŠ¸ (Transformer Encoder).
    3.  **Profile:** í‚¤(Height), ëª¸ë¬´ê²Œ(Weight) ë“± ìˆ˜ì¹˜í˜• ë°ì´í„° (Linear Projection & Z-score Norm).
* **Strategy:** Item Towerì˜ ê°€ì¤‘ì¹˜ëŠ” **Freeze(ê³ ì •)**í•˜ê³ , ìœ ì € íƒ€ì›Œë§Œ í•™ìŠµí•˜ì—¬ ìœ ì € ë²¡í„°ë¥¼ ì•„ì´í…œ ë²¡í„° ê³µê°„ì— ì •ë ¬(Alignment)ì‹œí‚µë‹ˆë‹¤.

### 4. Stage 3: Reranker (Fine-grained Ranking)
Retrieval ë‹¨ê³„ì—ì„œ ì¶”ë ¤ì§„ í›„ë³´êµ°ì„ ì •ë°€í•˜ê²Œ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
* **Model:** **DeepFM (Deep Factorization Machine)**
* **Weight Transfer:**
    * SimCSEë¡œ í•™ìŠµëœ **STD/RE ì„ë² ë”© ê°€ì¤‘ì¹˜**ë¥¼ DeepFMì˜ Sparse Feature Embeddingìœ¼ë¡œ **ì´ì‹(Transfer)**í•˜ì—¬ ì´ˆê¸° í•™ìŠµ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **Feature Interaction:**
    * **Sparse Feat:** ì¹´í…Œê³ ë¦¬, ë¸Œëœë“œ, RE ì†ì„± (Shared Embeddings).
    * **Dense Feat:** ìœ ì € í‚¤, ëª¸ë¬´ê²Œ.
    * **Implicit Interaction:** DNNì„ í†µí•´ ê³ ì°¨ì› ìƒí˜¸ì‘ìš© ëª¨ë¸ë§.

---


---

## 2. Core Feature Strategy: STD-RE Symbiosis
ìš°ë¦¬ëŠ” ìƒí’ˆì„ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ë¡œ ë³´ì§€ ì•Šê³ , **ë™ì¼í•œ Feature Key**ë¥¼ ê³µìœ í•˜ì§€ë§Œ ê¹Šì´ê°€ ë‹¤ë¥¸ ë‘ ê°€ì§€ ì†ì„±ìœ¼ë¡œ ì´ì›í™”í•˜ì—¬ ê´€ë¦¬í•©ë‹ˆë‹¤.

### Unified Representation Mechanism
**STD(Standard)**ì™€ **RE(Reinforced)**ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ê°€ ì•„ë‹ˆë¼, ë™ì¼í•œ ë¿Œë¦¬(Key)ì—ì„œ ë‚˜ì™€ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ ì‘ìš©í•˜ëŠ” ê³µìƒ ê´€ê³„ì…ë‹ˆë‹¤.

| Feature Type | Role | Value Source | Example (Key: `Material`) |
| :--- | :--- | :--- | :--- |
| **STD (Base)** | **Structure** | Fixed Domain Vocab | `Wool` |
| **RE (Augmented)** | **Specification** | **LLM-Augmented** | `100% Cashmere`, `Soft Touch`, `Virgin Wool` |

* **Shared Key & Augmented Value:** REëŠ” STDì˜ ë°ì´í„° í˜•íƒœ(Form)ë¥¼ ë‹µìŠµí•˜ë˜, LLMì„ í†µí•´ **ìƒì„¸í•˜ê³  í’ë¶€í•œ í‘œí˜„(Specific Description)**ìœ¼ë¡œ ì¦ê°•ë©ë‹ˆë‹¤.
* **Effect:** Cross-Attention ìˆ˜í–‰ ì‹œ, Query(STD)ì™€ Key(RE)ê°€ **ë™ì¼í•œ ë¬¸ë§¥(Shared Key)** ìœ„ì—ì„œ ìƒí˜¸ì‘ìš©í•˜ë¯€ë¡œ, Attention Alignmentê°€ ë§¤ìš° ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.

```mermaid
graph TD
    subgraph Feature Definition Strategy
    Root[Feature Key: Material]
    
    Root -->|Static Mapping| STD[STD: Wool]
    Root -->|LLM Augmentation| RE[RE: 100% Cashmere / Soft Touch]
    
    STD -->|Query| Attention((Cross-Attention))
    RE -->|Context| Attention
    
    Attention -->|Result| Vector[Specific Vector: <br>High-end Soft Wool]
    end
    
    style Root fill:#f9f,stroke:#333,stroke-width:2px
    style Attention fill:#bbf,stroke:#333,stroke-width:2px
```
## 3. Processing Pipeline (Step-by-Step)
ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì€ **Raw Data ì…ë ¥**ë¶€í„° ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ **Tensor ë³€í™˜**ê¹Œì§€ 4ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.


```mermaid
graph TD
    A[Raw Product Data] -->|Step 1: Ingestion| B(LLM Inference Engine)
    B -->|Step 2: Disentanglement| C{Structured JSON}
    C -->|Step 3: Dual Vocab Mapping| D[STD ID Mapping] & E[RE ID Mapping]
    D & E -->|Step 4: Tensor Construction| F[Model Input Tensor]
```

### Step 1. LLM-Based Feature Enrichment
Raw Data(ì´ë¯¸ì§€, HTML ë“±)ë¥¼ LLMì— ì£¼ì…í•˜ì—¬ ì¶”ì²œ ëª¨ë¸ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ **ê³ ë°€ë„ í”¼ì²˜(High-Density Features)**ë¡œ ì •ì œí•©ë‹ˆë‹¤. ë‹¨ìˆœ í‚¤ì›Œë“œ ì¶”ì¶œì„ ë„˜ì–´ ìƒí’ˆì˜ **'ë¶„ìœ„ê¸°(Vibe)'**ë‚˜ **'ì ì¬ì  ì†ì„±(Implicit Attributes)'**ê¹Œì§€ ì¶”ë¡ í•˜ì—¬ JSONìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.


### ğŸ“‚ Data Structure Example
LLMì„ í†µí•´ êµ¬ì¡°í™”ëœ ìƒí’ˆ ë°ì´í„°(JSON) ì˜ˆì‹œì…ë‹ˆë‹¤. ë°ì´í„°ëŠ” í¬ê²Œ **ê³ ì •ëœ í‘œì¤€ ì†ì„±(`clothes`)**ê³¼ **LLMì´ ì¦ê°•í•œ ìƒì„¸ ì†ì„±(`reinforced_feature_value`)**ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

```json
{
  "data": {
    "clothes": {
      "__description__": "Standard (STD) Features - Fixed Domain Vocab",
      "category": ["01outer_01coat"],
      "season": ["winter"],
      "fiber_composition": ["Cotton", "Polyester"],
      "elasticity": ["none"],
      "transparency": ["none at all"],
      "isfleece": ["fleece_none"],
      "color": ["Beige"],
      "gender": ["both"],
      "category_specification": ["outer"],
      "top.length_type": ["long"],
      "top.sleeve_length_type": ["long sleeves"],
      "top.neck_color_design": ["tailored collar"],
      "top.sleeve_design": ["basic sleeve"]
    },
    "reinforced_feature_value": {
      "__description__": "Reinforced (RE) Features - LLM Augmented & Dynamic",
      "category": [""],
      "fiber_composition": ["Cotton blend"],
      "color": [""],
      "category_specification": ["trench coat"],
      "specification.metadata": ["long", "winter wear"]
    },
    "ì¼ë°˜ ì„œë¹„ìŠ¤ ì‚°ì¶œ ë°ì´í„°" : ["ê°€ê²©, categorical features .. "]
  }
}

```

### Step 2. Feature Disentanglement (STD vs. RE)
ì¶”ì¶œëœ í”¼ì²˜ë¥¼ **í‘œì¤€ ì†ì„±(Skeleton)**ê³¼ **ë³´ê°• ì†ì„±(Flesh)**ìœ¼ë¡œ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ Cross-Attentionì˜ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **STD (Standard):** ë³€í•˜ì§€ ì•ŠëŠ” ê³ ì •ëœ ê¸°ì¤€ (Query ì—­í•  / Anchor)
* **RE (Reinforced):** ìƒí’ˆì˜ ê³ ìœ ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ê°€ë³€ ì†ì„± (Context ì—­í•  / Detail)

### Step 3. Dual Vocabulary Mapping & Organic Expansion
ë¶„ë¦¬ëœ í”¼ì²˜ë“¤ì„ ëª¨ë¸ì´ ì—°ì‚° ê°€ëŠ¥í•œ **ì •ìˆ˜ ID(Integer IDs)**ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
* **Shared Embedding for STD:** ëª¨ë“  STD ì†ì„±(ì¹´í…Œê³ ë¦¬, ìƒ‰ìƒ ë“±)ì€ ë‹¨ì¼ í†µí•© Vocabì„ ê³µìœ í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **Dynamic Expansion for RE:** ì‹ ì¡°ì–´ë‚˜ íŠ¸ë Œë“œ ìš©ì–´(ì˜ˆ: *Gorpcore*)ê°€ ë“±ì¥í•˜ë©´ ì¦‰ì‹œ **RE Dynamic Vocab**ì— ë“±ë¡ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‹œìŠ¤í…œì€ ë³„ë„ì˜ ì½”ë“œ ìˆ˜ì • ì—†ì´ë„ **ë°ì´í„°ê°€ ìŒ“ì¼ìˆ˜ë¡ ìŠ¤ìŠ¤ë¡œ ì§„í™”(Organic Growth)**í•˜ëŠ” íŠ¹ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.

### Step 4. Tensor Construction & Augmentation
SimCSE í•™ìŠµì„ ìœ„í•´ ìµœì¢…ì ìœ¼ë¡œ í…ì„œë¥¼ ìƒì„±í•˜ê³  ì¦ê°•(Augmentation)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* **Feature Dropout:** JSON ë‚´ì˜ Key-Valueë¥¼ ëœë¤í•˜ê²Œ ì œê±°í•˜ì—¬ íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ ê³¼ì˜ì¡´ì„ ë°©ì§€í•©ë‹ˆë‹¤.
* **Token Masking:** Title ë‚´ì˜ ë‹¨ì–´ë¥¼ ëœë¤í•˜ê²Œ Maskingí•˜ì—¬ ì „ì²´ì ì¸ ë§¥ë½(Context) ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•©ë‹ˆë‹¤.

## 4. Key Characteristics & Impact

### â‘  Universal Compatibility (AI Hub í˜¸í™˜ì„±)
AI Hub íŒ¨ì…˜ ë°ì´í„°ì…‹ê³¼ í˜¸í™˜ ê°€ëŠ¥í•œ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¥¼ ì±„íƒí•˜ì—¬, í•™ìŠµ ë°ì´í„° í™•ë³´ì˜ ìš©ì´ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ ì •ê·œí™”ëœ í”¼ì²˜ êµ¬ì¡° ë•ë¶„ì— **DeepFM, DLRM** ê°™ì€ Reranker ë„ì… ì‹œ ë³µì¡í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì—†ì´ ì¦‰ì‹œ ì—°ë™ ê°€ëŠ¥í•œ ë†’ì€ **í™•ì¥ì„±(Scalability)**ì„ ê°€ì§‘ë‹ˆë‹¤.

### â‘¡ Noise Robustness (ë…¸ì´ì¦ˆ ê°•ê±´ì„±)
LLMì´ ë§ˆì¼€íŒ… ìš©ì–´(Noise)ë¥¼ ì œê±°í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ê¸° ë•Œë¬¸ì—, ì¶”ì²œ ëª¨ë¸ì€ **ìˆœë„ ë†’ì€ ì •ë³´(High-SNR)**ë§Œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” ë²¡í„° ê³µê°„ ë‚´ì—ì„œ ì•„ì´í…œ ê°„ì˜ ê±°ë¦¬ë¥¼ ë”ìš± ëª…í™•í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

### â‘¢ Zero-Shot & Cold-Start Adaptation
ìœ ì € í–‰ë™ ë¡œê·¸ê°€ ì—†ëŠ” ì‹ ìƒí’ˆ(Cold-Start)ì´ë¼ë„, LLMì´ ìƒì„±í•œ **í’ë¶€í•œ RE í”¼ì²˜(í…ìŠ¤íŠ¸ ì„¤ëª…)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸° ë²¡í„°ë¥¼ ìƒì„±í•˜ë¯€ë¡œ **ìœ ì‚¬í•œ ë¶„ìœ„ê¸°ì˜ ê¸°ì¡´ ìƒí’ˆ ê·¼ì²˜**ì— ì •í™•íˆ ë§¤í•‘ë©ë‹ˆë‹¤. (Reference: *UniSRec, KDD 2022*)

---

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ api_controller/      # FastAPI Serving Logic
â”œâ”€â”€ vocab.py             # ID Mapping & Dynamic Vocabulary Management
â”œâ”€â”€ model_simcse.py      # [Stage 1] Item Tower Definition (Encoder)
â”œâ”€â”€ model_reranker.py    # [Stage 3] DeepFM Reranker Definition (Build & Transfer)
â”œâ”€â”€ train_simcse.py      # SimCSE Training Pipeline
â”œâ”€â”€ train_reranker.py    # DeepFM Training Pipeline
â””â”€â”€ saved_models/        # Model Weights Directory
    â”œâ”€â”€ encoder_stage1.pth  # SimCSE Pre-trained Encoder
    â””â”€â”€ reranker_deepfm.pth # DeepFM Trained Weights

```

## ğŸ›  Tech Stack

| Category | Technology | Description |
| :--- | :--- | :--- |
| **Core Framework** | **PyTorch** | Deep Learning Model Training & Inference (v1.12.1) |
| **API Server** | **FastAPI** | High-performance Async API for Real-time Serving |
| **NLP & Tokenizer** | **HuggingFace Transformers** | `DistilBERT` for Title/Text Feature Encoding |
| **Ranking Model** | **DeepCTR-Torch** | Implementation of **DeepFM** for Fine-grained Reranking |
| **Loss Function** | **PyTorch Metric Learning** | `NTXentLoss` (InfoNCE) for Contrastive Learning |
| **Vector Database** | **PostgreSQL (pgvector)** | Vector Storage & Inner Product (IP) Similarity Search |
| **Optimization** | **AdamW, GELU** | Optimizer & Activation Function for Transformer |