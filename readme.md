# ğŸ›ï¸ LLM-Driven Hybrid Recommendation System

## ğŸ“– Overview
ì´ í”„ë¡œì íŠ¸ëŠ” **LLM(Large Language Model)ì„ í™œìš©í•œ ê³ í’ˆì§ˆ Feature Engineering**ê³¼ **Contrastive Learning(SimCSE)** ê¸°ë°˜ì˜ ì„ë² ë”© í•™ìŠµì„ ê²°í•©í•œ ì°¨ì„¸ëŒ€ ì»¤ë¨¸ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ê¸°ì¡´ì˜ ë‹¨ìˆœ í˜‘ì—… í•„í„°ë§(CF)ì´ë‚˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì˜ í•œê³„ë¥¼ ë„˜ì–´, ìƒí’ˆì˜ **êµ¬ì¡°ì  ì†ì„±(Standard)**ê³¼ **ë¹„ì •í˜• ìƒì„¸ ì†ì„±(Reinforced)**ì„ **Cross-Attention**ìœ¼ë¡œ ìœµí•©í•˜ì—¬ ì •êµí•œ ë²¡í„° ê³µê°„ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **Two-Tower êµ¬ì¡°ì˜ Retrieval(í›„ë³´ ì¶”ì¶œ)**ê³¼ **DeepFM ê¸°ë°˜ì˜ Reranking(ì •ë°€ ì •ë ¬)** íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ìœ ì €ì˜ ì‹ ì²´ ì •ë³´ì™€ ë§¥ë½ê¹Œì§€ ê³ ë ¤í•œ ì´ˆê°œì¸í™” ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.

---

## ğŸ—ï¸ System Architecture

ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ ë°ì´í„° ì „ì²˜ë¦¬, í›„ë³´ ì¶”ì¶œ(Retrieval), ì •ë°€ ì •ë ¬(Reranking)ì˜ 3ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph Data Processing
    A[Raw Product Data] -->|LLM Inference| B(Structured JSON)
    B -->|Vocab Mapping| C{Dual Vocab System}
    end

    subgraph Stage 1: Representation
    C -->|STD + RE + Title| D[Item Tower <br> SimCSE Pre-training]
    D -->|Inference & Normalize| E[Vector DB <br> pgvector]
    end

    subgraph Stage 2: Retrieval
    F[User Logs / Profile] -->|Lookup & Fusion| G[User Tower]
    G -.->|ANN Search| E
    E -->|Top-100 Candidates| H[Candidate List]
    end

    subgraph Stage 3: Ranking
    H -->|Features + Interaction| I[Reranker <br> DeepFM]
    I -->|Top-k Reordering| J[Final Recommendation]
    end

```

## ğŸ”‘ Key Logics & Features

### 1. Hybrid Vocabulary System (Data Strategy)
LLMì´ ì¶”ì¶œí•œ í”¼ì²˜ë¥¼ ì„±ê²©ì— ë”°ë¼ ë‘ ê°€ì§€ Vocabìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ **ì—°ì‚° íš¨ìœ¨ì„±**ê³¼ **í‘œí˜„ë ¥**ì„ ë™ì‹œì— í™•ë³´í–ˆìŠµë‹ˆë‹¤.
* **STD (Standard) Vocab:** ì¹´í…Œê³ ë¦¬, ë¸Œëœë“œ, ì„±ë³„ ë“± ê³ ì •ëœ ë„ë©”ì¸ í”¼ì²˜. ë‹¨ì¼ í†µí•© ì„ë² ë”© í…Œì´ë¸”ì„ ê³µìœ í•˜ì—¬ ë©”ëª¨ë¦¬ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
* **RE (Reinforced) Vocab:** ì†Œì¬, í•, ìŠ¤íƒ€ì¼, ìƒí’ˆëª…(Title) ë“± ê°€ë³€ì ì¸ ìƒì„¸ ì†ì„±. ë™ì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•œ Vocab êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
* **ID Mapping:** ëª¨ë“  í…ìŠ¤íŠ¸ í”¼ì²˜ë¥¼ ê³ ìœ  IDë¡œ ë§¤í•‘í•˜ì—¬ Cross-Attention ì‹œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•˜ê³  **ì§êµì„±(Orthogonality)**ì„ í™•ë³´í•©ë‹ˆë‹¤.

### 2. Stage 1: Item Tower (Coarse-to-Fine Representation)
ìƒí’ˆì˜ ë³¸ì§ˆì ì¸ ì˜ë¯¸(Semantic)ë¥¼ ë²¡í„°í™”í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.
* **Architecture:** Transformer-based Encoder + MLP Projection Head
* **Cross-Attention Mechanism:**
    * **Query (Anchor):** STD í”¼ì²˜ (ì˜ˆ: "ë‚¨ì„± ìƒì˜") â†’ ë³€í•˜ì§€ ì•ŠëŠ” ê¸°ì¤€ì 
    * **Key/Value (Context):** RE í”¼ì²˜ + Title Tokens (ì˜ˆ: "ë¦°ë„¨", "ì˜¤ë²„í•", "ì—¬ë¦„ ì‹ ìƒ") â†’ ë³´ê°• ì •ë³´
    * *Effect:* í‘œì¤€ ì†ì„±ì´ ìƒì„¸ ì†ì„±ì„ ì°¸ì¡°í•˜ì—¬ ë²¡í„°ë¥¼ ê°•í™”(Reinforce)í•˜ëŠ” êµ¬ì¡°.
* **Training Objective (SimCSE):**
    * **Augmentation:** Feature Dropout & Token Maskingì„ í†µí•´ Positive Pair ìƒì„±.
    * **Loss:** `NTXentLoss` (InfoNCE) with In-batch Negatives.
    * **Normalization:** ì¶”ë¡  ì‹œ L2 Normalizationì„ ì ìš©í•˜ì—¬ Cosine Similarity ê²€ìƒ‰ì— ìµœì í™”.

### 3. Stage 2: User Tower (Multi-modal Retrieval)
ìœ ì €ì˜ í–‰ë™, í˜„ì¬ ì˜ë„(Context), ì‹ ì²´ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ì„ í˜¸ ì•„ì´í…œì„ íƒìƒ‰í•©ë‹ˆë‹¤.
* **3-Way Multi-modal Fusion:**
    1.  **Behavior:** ê³¼ê±° êµ¬ë§¤/í´ë¦­ ì´ë ¥ Sequence (Pre-trained Item Vector Lookup + Transformer).
    2.  **Context:** ì¥ë°”êµ¬ë‹ˆ ì»¨ì…‰ í…ìŠ¤íŠ¸ (Transformer Encoder).
    3.  **Profile:** í‚¤(Height), ëª¸ë¬´ê²Œ(Weight) ë“± ìˆ˜ì¹˜í˜• ë°ì´í„° (Linear Projection & Z-score Norm).
* **Strategy:** Item Towerì˜ ê°€ì¤‘ì¹˜ëŠ” **Freeze(ê³ ì •)**í•˜ê³ , ìœ ì € íƒ€ì›Œë§Œ í•™ìŠµí•˜ì—¬ ìœ ì € ë²¡í„°ë¥¼ ì•„ì´í…œ ë²¡í„° ê³µê°„ì— ì •ë ¬(Alignment)ì‹œí‚µë‹ˆë‹¤.

### 4. Stage 3: Reranker (Fine-grained Ranking)
Retrieval ë‹¨ê³„ì—ì„œ ì¶”ë ¤ì§„ í›„ë³´êµ°ì„ ì •ë°€í•˜ê²Œ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
* **Model:** **DeepFM (Deep Factorization Machine)**
* **Weight Transfer:**
    * SimCSEë¡œ í•™ìŠµëœ **STD/RE ì„ë² ë”© ê°€ì¤‘ì¹˜**ë¥¼ DeepFMì˜ Sparse Feature Embeddingìœ¼ë¡œ **ì´ì‹(Transfer)**í•˜ì—¬ ì´ˆê¸° í•™ìŠµ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **Feature Interaction:**
    * **Sparse Feat:** ì¹´í…Œê³ ë¦¬, ë¸Œëœë“œ, RE ì†ì„± (Shared Embeddings).
    * **Dense Feat:** ìœ ì € í‚¤, ëª¸ë¬´ê²Œ.
    * **Implicit Interaction:** DNNì„ í†µí•´ ê³ ì°¨ì› ìƒí˜¸ì‘ìš© ëª¨ë¸ë§.

---

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ api_controller/      # FastAPI Serving Logic
â”œâ”€â”€ vocab.py             # ID Mapping & Dynamic Vocabulary Management
â”œâ”€â”€ model_simcse.py      # [Stage 1] Item Tower Definition (Encoder)
â”œâ”€â”€ model_reranker.py    # [Stage 3] DeepFM Reranker Definition (Build & Transfer)
â”œâ”€â”€ train_simcse.py      # SimCSE Training Pipeline
â”œâ”€â”€ train_reranker.py    # DeepFM Training Pipeline
â””â”€â”€ saved_models/        # Model Weights Directory
    â”œâ”€â”€ encoder_stage1.pth  # SimCSE Pre-trained Encoder
    â””â”€â”€ reranker_deepfm.pth # DeepFM Trained Weights

```

## ğŸ›  Tech Stack

| Category | Technology | Description |
| :--- | :--- | :--- |
| **Core Framework** | **PyTorch** | Deep Learning Model Training & Inference (v1.12.1) |
| **API Server** | **FastAPI** | High-performance Async API for Real-time Serving |
| **NLP & Tokenizer** | **HuggingFace Transformers** | `DistilBERT` for Title/Text Feature Encoding |
| **Ranking Model** | **DeepCTR-Torch** | Implementation of **DeepFM** for Fine-grained Reranking |
| **Loss Function** | **PyTorch Metric Learning** | `NTXentLoss` (InfoNCE) for Contrastive Learning |
| **Vector Database** | **PostgreSQL (pgvector)** | Vector Storage & Inner Product (IP) Similarity Search |
| **Optimization** | **AdamW, GELU** | Optimizer & Activation Function for Transformer |