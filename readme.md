# ğŸ›ï¸ LLM-Driven content-based Feature Recommendation System

## ğŸ“– Overview

- **Project Duration:** 2024-10 ~ 2024-12 (Partial Impl) â†’ **Complete AI Impl ver (Current, Logic ìƒì„¸ êµ¬í˜„)**
- **Previous Repo:** [github link](https://github.com/DotBlossom/flask-AI-inference-bedrock-controller)

ì´ í”„ë¡œì íŠ¸ëŠ” **LLM(Large Language Model)ì„ í™œìš©í•œ ê³ í’ˆì§ˆ Feature Engineering**ê³¼ **Contrastive Learning(SimCSE)** ê¸°ë°˜ì˜ ì„ë² ë”© í•™ìŠµì„ ê²°í•©í•œ ì°¨ì„¸ëŒ€ ì»¤ë¨¸ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

ê¸°ì¡´ì˜ ë‹¨ìˆœ í˜‘ì—… í•„í„°ë§(CF)ì´ë‚˜ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ì˜ í•œê³„ë¥¼ ë„˜ì–´, ìƒí’ˆì˜ **êµ¬ì¡°ì  ì†ì„±(Standard)** ê³¼ **ë¹„ì •í˜• ìƒì„¸ ì†ì„±(Reinforced)** ì„ **Hierarchical Residual Embedding(ê³„ì¸µì  ì”ì°¨ ì„ë² ë”©)** ìœ¼ë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.

ì´í›„, ì´ë“¤ì„ **ë‹¨ì¼ ì‹œí€€ìŠ¤(Single Sequence)ë¡œ ê²°í•©í•˜ì—¬ Unified Transformer ë‚´ì—ì„œ ì†ì„± ê°„ì˜ ìƒí˜¸ì‘ìš©(Self-Attention)ì„ í•™ìŠµí•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ Deep Residual MLP Layerë¡œ ì••ì¶•í•˜ì—¬** ì •êµí•œ ë²¡í„° ê³µê°„ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **Two-Tower êµ¬ì¡°ì˜ Retrieval(í›„ë³´ ì¶”ì¶œ)** ê³¼ **DeepFM ê¸°ë°˜ì˜ Reranking(ì •ë°€ ì •ë ¬)** íŒŒì´í”„ë¼ì¸ì„ í†µí•´ ìœ ì €ì˜ ì‹ ì²´ ì •ë³´ì™€ ë§¥ë½ê¹Œì§€ ê³ ë ¤í•œ ì´ˆê°œì¸í™” ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.


## Data Strategy & Processing Pipeline Overview

ë³¸ í”„ë¡œì íŠ¸ì˜ ë°ì´í„° ì•„í‚¤í…ì²˜ëŠ” **"LLMì„ í™œìš©í•œ í‘œí˜„ë ¥ ì¦ê°•(Representation Enrichment)"** ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

ì—¬ê¸°ì— **AI Hub íŒ¨ì…˜ ìƒí’ˆ ë°ì´í„°ì…‹**ê³¼ í˜¸í™˜ ê°€ëŠ¥í•œ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¥¼ ì ìš©í•˜ì—¬, í•™ìŠµ ë°ì´í„°ì™€ ì‹¤ì œ ì„œë¹„ìŠ¤ ë°ì´í„° ê°„ì˜ ì •í•©ì„±ì„ ë³´ì¥í•˜ê³  **Reranker(DeepFM, DLRM)** ë“± ê³ ë„í™”ëœ ëª¨ë“ˆë¡œ ì¦‰ì‹œ í™•ì¥ ê°€ëŠ¥í•œ **ë²”ìš©ì ì´ê³  ìœ ê¸°ì ì¸ ì‹œìŠ¤í…œ**ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ—ï¸ System Architecture

ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ ë°ì´í„° ì „ì²˜ë¦¬, í›„ë³´ ì¶”ì¶œ(Retrieval), ì •ë°€ ì •ë ¬(Reranking)ì˜ 3ë‹¨ê³„ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph Data Processing
    A[Raw Product Data] -->|LLM Inference| B(Structured JSON)
    B -->|Vocab Mapping| C{Dual Vocab System}
    end

    subgraph Stage 1: Representation
    C -->|STD + RE + Title| D[Item Tower <br> SimCSE Pre-training]
    D -->|Inference & Normalize| E[Vector DB <br> pgvector]
    end

    subgraph Stage 2: Retrieval
    F[User Logs / Profile] -->|Lookup & Fusion| G[User Tower]
    G -.->|ANN Search| E
    E -->|Top-100 Candidates| H[Candidate List]
    end

    subgraph Stage 3: Ranking
    H -->|Features + Interaction| I[Reranker <br> DeepFM]
    I -->|Top-k Reordering| J[Final Recommendation]
    end

```

## ğŸ”‘ Key Logics & Features

### 1. Hybrid Vocabulary System (Data Strategy)

ë°ì´í„°ì˜ ì„±ê²©(ì •í˜•/ë¹„ì •í˜•)ê³¼ ìš´ì˜ íš¨ìœ¨ì„±ì„ ê³ ë ¤í•˜ì—¬ ì´ì›í™”ëœ ì²˜ë¦¬ ì „ëµì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

* **STD (Standard) Vocab**
    * ì¹´í…Œê³ ë¦¬, ë¸Œëœë“œ, ì„±ë³„ ë“± ê³ ì •ëœ ë„ë©”ì¸ í”¼ì²˜.
    * ì‚¬ì „ ì •ì˜ëœ **Finite Lookup Table**ì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ì˜ë¯¸ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
* **RE (Reinforced) Vocab**
    * ì†Œì¬, í•, ìŠ¤íƒ€ì¼ ë“± ê°€ë³€ì ì´ê³  ë¬´í•œíˆ í™•ì¥ë˜ëŠ” ìƒì„¸ ì†ì„±.
    * ë³„ë„ì˜ ì‚¬ì „ ê´€ë¦¬ ì—†ì´ **Stateless Hashing (zlib)** ì„ í†µí•´ ê³ ì •ëœ ì°¨ì›ì˜ IDë¡œ ì¦‰ì‹œ ë³€í™˜í•˜ì—¬, ì„œë²„ í™•ì¥ì„±ê³¼ ìš´ì˜ ì•ˆì •ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **Field Embedding**
    * ê° ì†ì„±(Color, Season ë“±)ì˜ ì—­í• ì„ ì •ì˜í•˜ëŠ” ì„ë² ë”©ì„ ë„ì…í•˜ì—¬, STDì™€ REê°€ ë™ì¼í•œ ìœ„ìƒ ê³µê°„ì—ì„œ ìƒí˜¸ì‘ìš©í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

---

### 2. Stage 1: Item Tower (Coarse-to-Fine Representation)

ìƒí’ˆì˜ ë³¸ì§ˆì ì¸ ì˜ë¯¸(Semantic)ë¥¼ ë²¡í„°í™”í•˜ëŠ” ë‹¨ê³„ë¡œ, **TabTransformer** êµ¬ì¡°ë¥¼ ì‘ìš©í•˜ì—¬ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

* **Architecture**
    * **Unified Transformer Encoder** + **Deep Residual Head (MLP)**
* **Hierarchical Residual Input (Core Logic)**
    * **ë‹¨ì¼ ì‹œí€€ìŠ¤(Single Sequence)** ë‚´ì—ì„œ ìœ„ê³„ êµ¬ì¡°ë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ê°•ì œí•©ë‹ˆë‹¤.
    * **STD Token:** $Base\_Value + Field\_Key$
    * **RE Token:** $RE\_Delta + \text{StopGrad}(STD\_Base) + Field\_Key$
    * **Effect:** RE í”¼ì²˜ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì¡´ì¬í•˜ì§€ ì•Šê³  STDë¥¼ **ìƒì†(Inheritance)** ë°›ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°ì´í„° í¬ì†Œì„±(Sparsity) ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , ëª¨ë¸ì€ REë¥¼ "STDì™€ì˜ ë¯¸ì„¸í•œ ì°¨ì´(Delta)"ë¡œ ì¸ì‹í•˜ì—¬ í•™ìŠµ íš¨ìœ¨ì´ ê¸‰ì¦í•©ë‹ˆë‹¤.
* **Training Objective (SimCSE)**
    * **Augmentation:** ê³„ì¸µì  êµ¬ì¡°ë¥¼ í™œìš©í•œ **Complementary Masking** (STD ìœ„ì£¼ vs RE ìœ„ì£¼) ë° Feature Dropout.
    * **Loss:** `NTXentLoss` (InfoNCE). ëª¨ë¸ì€ ê°™ì€ ìƒí’ˆì˜ ë¼ˆëŒ€(STD)ì™€ ì‚´(RE)ì´ ë³¸ì§ˆì ìœ¼ë¡œ ê°™ìŒì„ í•™ìŠµí•©ë‹ˆë‹¤.

---

### 3. Stage 2: User Tower (Multi-modal Retrieval)
ìœ ì €ì˜ í–‰ë™, í˜„ì¬ ì˜ë„(Context), ì‹ ì²´ ì •ë³´ë¥¼ ê²°í•©í•˜ì—¬ ì„ í˜¸ ì•„ì´í…œì„ íƒìƒ‰í•©ë‹ˆë‹¤.
* **3-Way Multi-modal Fusion:**
    1.  **Behavior:** ê³¼ê±° êµ¬ë§¤/í´ë¦­ ì´ë ¥ Sequence (Pre-trained Item Vector Lookup + Transformer).
    2.  **Context:** ì¥ë°”êµ¬ë‹ˆ ì»¨ì…‰ í…ìŠ¤íŠ¸ (Transformer Encoder).
    3.  **Profile:** í‚¤(Height), ëª¸ë¬´ê²Œ(Weight) ë“± ìˆ˜ì¹˜í˜• ë°ì´í„° (Linear Projection & Z-score Norm).
* **Strategy:** Item Towerì˜ ê°€ì¤‘ì¹˜ëŠ” **Freeze(ê³ ì •)**í•˜ê³ , ìœ ì € íƒ€ì›Œë§Œ í•™ìŠµí•˜ì—¬ ìœ ì € ë²¡í„°ë¥¼ ì•„ì´í…œ ë²¡í„° ê³µê°„ì— ì •ë ¬(Alignment)ì‹œí‚µë‹ˆë‹¤.

### 4. Stage 3: Reranker (Fine-grained Ranking)
Retrieval ë‹¨ê³„ì—ì„œ ì¶”ë ¤ì§„ í›„ë³´êµ°ì„ ì •ë°€í•˜ê²Œ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
* **Model:** **DeepFM (Deep Factorization Machine)**
* **Weight Transfer:**
    * SimCSEë¡œ í•™ìŠµëœ **STD/RE ì„ë² ë”© ê°€ì¤‘ì¹˜**ë¥¼ DeepFMì˜ Sparse Feature Embeddingìœ¼ë¡œ **ì´ì‹(Transfer)**í•˜ì—¬ ì´ˆê¸° í•™ìŠµ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **Feature Interaction:**
    * **Sparse Feat:** ì¹´í…Œê³ ë¦¬, ë¸Œëœë“œ, RE ì†ì„± (Shared Embeddings).
    * **Dense Feat:** ìœ ì € í‚¤, ëª¸ë¬´ê²Œ.
    * **Implicit Interaction:** DNNì„ í†µí•´ ê³ ì°¨ì› ìƒí˜¸ì‘ìš© ëª¨ë¸ë§.

---


---

## 2. Core Feature Strategy: STD-RE Symbiosis
ìš°ë¦¬ëŠ” ìƒí’ˆì„ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ ë‚˜ì—´ë¡œ ë³´ì§€ ì•Šê³ , **ë™ì¼í•œ Feature Key**ë¥¼ ê³µìœ í•˜ëŠ” **ë¶€ëª¨(STD)ì™€ ìì‹(RE)**ì˜ ê´€ê³„ë¡œ ì •ì˜í•©ë‹ˆë‹¤.

### Unified Representation Mechanism (Delta Learning)

**STD(Standard)**ì™€ **RE(Reinforced)**ëŠ” ë³„ê°œì˜ ì •ë³´ê°€ ì•„ë‹™ë‹ˆë‹¤. REëŠ” STDë¼ëŠ” **ê¸°ì¤€ì (Anchor)** ìœ„ì—ì„œ ì–¼ë§ˆë‚˜ êµ¬ì²´í™”ë˜ì—ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **ë³€í˜•(Variation)** ì •ë³´ì…ë‹ˆë‹¤.

| Feature Type | Role | Value Source | Input Logic |
| :--- | :--- | :--- | :--- |
| **STD (Base)** | **Structure (ë¼ˆëŒ€)** | Fixed Domain Vocab | $Emb(STD) + Emb(Field)$ |
| **RE (Detail)** | **Specification (ì‚´)** | **LLM-Augmented & Hashed** | $Emb(RE_{hash}) + \text{StopGrad}(Emb(STD)) + Emb(Field)$ |

* **Inheritance & Delta**
    * RE í† í°ì€ ì…ë ¥ ë‹¨ê³„ì—ì„œ STD ë²¡í„°ë¥¼ ë”í•˜ê³  ì‹œì‘í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ˆê¸° í•™ìŠµ ì‹œì ë¶€í„° **"Cold Start"** ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.
    * í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ STDê°€ í‘œí˜„í•˜ì§€ ëª»í•˜ëŠ” **"ìƒì„¸í•œ ë‰˜ì•™ìŠ¤(Delta)"** ë§Œì„ ì§‘ì¤‘ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
* **Effect**
    * ëª¨ë“  í”¼ì²˜ê°€ **Unified Transformer** ë‚´ì—ì„œ í‰ë“±í•˜ê²Œ ìƒí˜¸ì‘ìš©(Self-Attention)í•˜ë˜, ì”ì°¨ ì—°ê²°ì„ í†µí•´ êµ¬ì¡°ì  ì—°ê´€ì„±ì´ ë¬¼ë¦¬ì ìœ¼ë¡œ ë³´ì¥ë©ë‹ˆë‹¤.
    * ì´ëŠ” Cross-Attention ë°©ì‹ë³´ë‹¤ í›¨ì”¬ ì§ê´€ì ì´ë©° ê°•ë ¥í•œ ë¬¸ë§¥ ê²°í•©ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤.

```mermaid
graph TD
    subgraph Feature Definition Strategy
    Root[Feature Key: Material]
    
    Root -->|Static Mapping| STD[STD: Wool]
    Root -->|LLM Augmentation| RE[RE: 100% Cashmere / Soft Touch]
    
    STD -->|Query| Attention((Cross-Attention))
    RE -->|Context| Attention
    
    Attention -->|Result| Vector[Specific Vector: <br>High-end Soft Wool]
    end
    
    style Root fill:#f9f,stroke:#333,stroke-width:2px
    style Attention fill:#bbf,stroke:#333,stroke-width:2px
```
## 3. Processing Pipeline (Step-by-Step)
ì „ì²´ ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì€ **Raw Data ì…ë ¥**ë¶€í„° ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ **Tensor ë³€í™˜**ê¹Œì§€ 4ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.


```mermaid
graph TD
    A[Raw Product Data] -->|Step 1: Ingestion| B(LLM Inference Engine)
    B -->|Step 2: Disentanglement| C{Structured JSON}
    C -->|Step 3: Dual Vocab Mapping| D[STD ID Mapping] & E[RE ID Mapping]
    D & E -->|Step 4: Tensor Construction| F[Model Input Tensor]
```

### Step 1. LLM-Based Feature Enrichment
Raw Data(ì´ë¯¸ì§€, HTML ë“±)ë¥¼ LLMì— ì£¼ì…í•˜ì—¬ ì¶”ì²œ ëª¨ë¸ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ **ê³ ë°€ë„ í”¼ì²˜(High-Density Features)**ë¡œ ì •ì œí•©ë‹ˆë‹¤. ë‹¨ìˆœ í‚¤ì›Œë“œ ì¶”ì¶œì„ ë„˜ì–´ ìƒí’ˆì˜ **'ë¶„ìœ„ê¸°(Vibe)'**ë‚˜ **'ì ì¬ì  ì†ì„±(Implicit Attributes)'**ê¹Œì§€ ì¶”ë¡ í•˜ì—¬ JSONìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.


### ğŸ“‚ Data Structure Example
LLMì„ í†µí•´ êµ¬ì¡°í™”ëœ ìƒí’ˆ ë°ì´í„°(JSON) ì˜ˆì‹œì…ë‹ˆë‹¤. ë°ì´í„°ëŠ” í¬ê²Œ **ê³ ì •ëœ í‘œì¤€ ì†ì„±(`clothes`)**ê³¼ **LLMì´ ì¦ê°•í•œ ìƒì„¸ ì†ì„±(`reinforced_feature_value`)**ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.

```json
{
  "data": {
    "clothes": {
      "__description__": "Standard (STD) Features - Fixed Domain Vocab",
      "category": ["01outer_01coat"],
      "season": ["winter"],
      "fiber_composition": ["Cotton", "Polyester"],
      "elasticity": ["none"],
      "transparency": ["none at all"],
      "isfleece": ["fleece_none"],
      "color": ["Beige"],
      "gender": ["both"],
      "category_specification": ["outer"],
      "top.length_type": ["long"],
      "top.sleeve_length_type": ["long sleeves"],
      "top.neck_color_design": ["tailored collar"],
      "top.sleeve_design": ["basic sleeve"]
    },
    "reinforced_feature_value": {
      "__description__": "Reinforced (RE) Features - LLM Augmented & Dynamic",
      "category": [""],
      "fiber_composition": ["Cotton blend"],
      "color": [""],
      "category_specification": ["trench coat"],
      "specification.metadata": ["long", "winter wear"]
    },
    "ì¼ë°˜ ì„œë¹„ìŠ¤ ì‚°ì¶œ ë°ì´í„°" : ["ê°€ê²©, categorical features .. "]
  }
}

```

### Step 2. Feature Disentanglement (STD vs. RE)
ì¶”ì¶œëœ í”¼ì²˜ë¥¼ **í‘œì¤€ ì†ì„±(Skeleton)**ê³¼ **ë³´ê°• ì†ì„±(Flesh)**ìœ¼ë¡œ ëª…í™•íˆ ë¶„ë¦¬í•˜ì—¬ Cross-Attentionì˜ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **STD (Standard):** ë³€í•˜ì§€ ì•ŠëŠ” ê³ ì •ëœ ê¸°ì¤€ (Query ì—­í•  / Anchor)
* **RE (Reinforced):** ìƒí’ˆì˜ ê³ ìœ ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ê°€ë³€ ì†ì„± (Context ì—­í•  / Detail)

### Step 3. Dual Vocabulary Mapping & Organic Expansion
ë¶„ë¦¬ëœ í”¼ì²˜ë“¤ì„ ëª¨ë¸ì´ ì—°ì‚° ê°€ëŠ¥í•œ **ì •ìˆ˜ ID(Integer IDs)**ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
* **Shared Embedding for STD:** ëª¨ë“  STD ì†ì„±(ì¹´í…Œê³ ë¦¬, ìƒ‰ìƒ ë“±)ì€ ë‹¨ì¼ í†µí•© Vocabì„ ê³µìœ í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
* **Dynamic Expansion for RE:** ì‹ ì¡°ì–´ë‚˜ íŠ¸ë Œë“œ ìš©ì–´(ì˜ˆ: *Gorpcore*)ê°€ ë“±ì¥í•˜ë©´ ì¦‰ì‹œ **RE Dynamic Vocab**ì— ë“±ë¡ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‹œìŠ¤í…œì€ ë³„ë„ì˜ ì½”ë“œ ìˆ˜ì • ì—†ì´ë„ **ë°ì´í„°ê°€ ìŒ“ì¼ìˆ˜ë¡ ìŠ¤ìŠ¤ë¡œ ì§„í™”(Organic Growth)**í•˜ëŠ” íŠ¹ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.

### Step 4. Tensor Construction & Augmentation
SimCSE í•™ìŠµì„ ìœ„í•´ ìµœì¢…ì ìœ¼ë¡œ í…ì„œë¥¼ ìƒì„±í•˜ê³  ì¦ê°•(Augmentation)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
* **Feature Dropout:** JSON ë‚´ì˜ Key-Valueë¥¼ ëœë¤í•˜ê²Œ ì œê±°í•˜ì—¬ íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ ê³¼ì˜ì¡´ì„ ë°©ì§€í•©ë‹ˆë‹¤.
* **Token Masking:** Title ë‚´ì˜ ë‹¨ì–´ë¥¼ ëœë¤í•˜ê²Œ Maskingí•˜ì—¬ ì „ì²´ì ì¸ ë§¥ë½(Context) ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•©ë‹ˆë‹¤.

## 4. Key Characteristics & Impact

### â‘  Universal Compatibility (AI Hub í˜¸í™˜ì„±)
AI Hub íŒ¨ì…˜ ë°ì´í„°ì…‹ê³¼ í˜¸í™˜ ê°€ëŠ¥í•œ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¥¼ ì±„íƒí•˜ì—¬, í•™ìŠµ ë°ì´í„° í™•ë³´ì˜ ìš©ì´ì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ ì •ê·œí™”ëœ í”¼ì²˜ êµ¬ì¡° ë•ë¶„ì— **DeepFM, DLRM** ê°™ì€ Reranker ë„ì… ì‹œ ë³µì¡í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì—†ì´ ì¦‰ì‹œ ì—°ë™ ê°€ëŠ¥í•œ ë†’ì€ **í™•ì¥ì„±(Scalability)**ì„ ê°€ì§‘ë‹ˆë‹¤.

### â‘¡ Noise Robustness (ë…¸ì´ì¦ˆ ê°•ê±´ì„±)
LLMì´ ë§ˆì¼€íŒ… ìš©ì–´(Noise)ë¥¼ ì œê±°í•˜ê³  êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•˜ê¸° ë•Œë¬¸ì—, ì¶”ì²œ ëª¨ë¸ì€ **ìˆœë„ ë†’ì€ ì •ë³´(High-SNR)**ë§Œ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” ë²¡í„° ê³µê°„ ë‚´ì—ì„œ ì•„ì´í…œ ê°„ì˜ ê±°ë¦¬ë¥¼ ë”ìš± ëª…í™•í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

### â‘¢ Zero-Shot & Cold-Start Adaptation
ìœ ì € í–‰ë™ ë¡œê·¸ê°€ ì—†ëŠ” ì‹ ìƒí’ˆ(Cold-Start)ì´ë¼ë„, LLMì´ ìƒì„±í•œ **í’ë¶€í•œ RE í”¼ì²˜(í…ìŠ¤íŠ¸ ì„¤ëª…)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ˆê¸° ë²¡í„°ë¥¼ ìƒì„±í•˜ë¯€ë¡œ **ìœ ì‚¬í•œ ë¶„ìœ„ê¸°ì˜ ê¸°ì¡´ ìƒí’ˆ ê·¼ì²˜**ì— ì •í™•íˆ ë§¤í•‘ë©ë‹ˆë‹¤. (Reference: *UniSRec, KDD 2022*)

---

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ api_controller/      # FastAPI Serving Logic
â”œâ”€â”€ vocab.py             # ID Mapping & Dynamic Vocabulary Management
â”œâ”€â”€ model_simcse.py      # [Stage 1] Item Tower Definition (Encoder)
â”œâ”€â”€ model_reranker.py    # [Stage 3] DeepFM Reranker Definition (Build & Transfer)
â”œâ”€â”€ train_simcse.py      # SimCSE Training Pipeline
â”œâ”€â”€ train_reranker.py    # DeepFM Training Pipeline
â””â”€â”€ saved_models/        # Model Weights Directory
    â”œâ”€â”€ encoder_stage1.pth  # SimCSE Pre-trained Encoder
    â””â”€â”€ reranker_deepfm.pth # DeepFM Trained Weights

```

## ğŸ›  Tech Stack

| Category | Technology | Description |
| :--- | :--- | :--- |
| **Core Framework** | **PyTorch** | Deep Learning Model Training & Inference (v1.12.1) |
| **API Server** | **FastAPI** | High-performance Async API for Real-time Serving |
| **NLP & Tokenizer** | **HuggingFace Transformers** | `DistilBERT` for Title/Text Feature Encoding |
| **Ranking Model** | **DeepCTR-Torch** | Implementation of **DeepFM** for Fine-grained Reranking |
| **Loss Function** | **PyTorch Metric Learning** | `NTXentLoss` (InfoNCE) for Contrastive Learning |
| **Vector Database** | **PostgreSQL (pgvector)** | Vector Storage & Inner Product (IP) Similarity Search |
| **Optimization** | **AdamW, GELU** | Optimizer & Activation Function for Transformer |